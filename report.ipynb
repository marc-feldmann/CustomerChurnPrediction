{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Keep in mind while writing up:*\n",
    "\n",
    "- *Be concise! Less is more - the fully story is in the source code for those interested.*\n",
    "- *Be deliberate about: What to highlight in which section (e.g., “this dataset was special due to its high number of variables”…)*\n",
    "- *Work with visuals and only exceptionally with code. Refer to GitHub, dump code there, the technical people will go there. And (hiring) managers will only read the write-up.*\n",
    "- *Optimize business value, not model performance! Time/Resource constraints, ….*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Report\n",
    "# **Preventing Customer Churn with Artificial Neural Networks**\n",
    "*Disclaimer: This mock project report serves educational purposes only. The data is public (https://www.kdd.org/kdd-cup/view/kdd-cup-2009/Data). The author has no relationship with mentioned parties.* \n",
    "***\n",
    "### **Executive Summary (max. 7 sentences)**\n",
    "Situation (1 sentence based on 1.)\n",
    "<br>\n",
    "Complication (1 sentence based on 1.)\n",
    "<br>\n",
    "Solution (1 sentence based on 2.)\n",
    "<br>\n",
    "Recommendations including Solutions' Business Value Add (1-3 sentences based on 3.)\n",
    "\"much buzz around ANN, let's test that here\"\n",
    "***\n",
    "### **Report Structure**\n",
    "[Include nice + simple process visualization!]\n",
    "1. Business Problem Statement\n",
    "2. Technical Solution\n",
    "<br>    *2.1. Technical Problem Statement*\n",
    "<br>    *2.2. Exploratory Data Analysis*\n",
    "<br>    *2.3. Data Preprocessing*\n",
    "<br>    *2.4. Model Selection (incl. Optimization)*\n",
    "<br>    *2.5. Final Model Evaluation*\n",
    "<br>    *2.6. Future Optimization Potentials*\n",
    "3. Business Recommendations\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Business Problem Statement**\n",
    "For French telecommunication provider Orange, customer retention is critical. This is because retaining customers is much cheaper than the alternative: losing a customer and their revenues *plus* having additional costs for acquiring a new customer. However, Orange lacks an automated, scalable, and data-driven method for predicting customer churn that would allow Orange to initiate retention measures before customers leave. Thus, Orange requested a proof-of-concept for a predictive model that can help identify customers who will likely churn. More specifically, given their widely reported success, Orange is interested in exploring the potential of \"deep learning\" models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Technical Solution**\n",
    "- for visualization inspirations see here: https://towardsdatascience.com/predict-customer-churn-the-right-way-using-pycaret-8ba6541608ac\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.1. Technical Solution: Technical Problem Statement*\n",
    "The business problem, as put by our client Orange, is \"to predict customer churn\". This problem requires translation into a better specified, technical problem to be solvable using mathematical-statistical methods.\n",
    "\n",
    "#### Problem understanding\n",
    "Hereto, it is first important to understand that the problem we solve is a binary classification problem: given the data available for any particular customer (e.g., age, gender, purchased services, average call duration), we want our model to assign this customer to one of the two classes \"churn\"/\"no churn\". Understanding that we solve a classification problem has important implications for two main elements of the technical problem statement:\n",
    "\n",
    "#### Model class choice\n",
    "In a typical data science project, we would train models from many different model classes (e.g., logistic regression classifiers, trees, support vector machines) and select the best performing models (or a combination of them in an *ensemble*) for deployment. In this project, however, the client Orange has specified upfront that they want a \"deep learning\" model, which in more precise technical terms is widely understood as an artificial neural network (ANN) with more than one hidden layer. Since we want the ANN's output to always be either \"churn\" or \"no churn\", its output layer must contain a single neuron with an activation function (e.g., ReLU, sigmoid) able to translate continuous into binary values (1/0).\n",
    "\n",
    "#### Evaluation metric choice\n",
    "An evaluation metric enables us to assess how \"good\" a developed model is and optimize it. The perhaps most intuitive metric for a classification model is the *accuracy* of its predictions. Accuracy tells us in which percentage of cases a classification model's predictions (\"churn\"/\"no churn\") are true (that is, correctly predict what customers will actually do). However, we can infer from the business context that the classes \"churn\"/\"no churn\" we are interested in are *imbalanced*: only a minority of all customers will churn in any given time period. We can thus expect many more customers to be in the \"no churn\" rather than the \"churn\" class. Accuracy will thus be a bad metric to optimize: the model could 'cheat' and predict \"no churn\" in 100% of the cases, and never detect a single churning customer, and still have awesome accuracy. In presence of class imbalance, a metric more adequate to optimize is the *F1-score*. A high F1-score indicates not only that the model is able to detect many of those customers who will indeed churn, but also that the model's \"churn\"-predictions are typically correct (two different things!).   \n",
    "\n",
    "\n",
    "#### > Technical Problem Statement\n",
    "Understanding the problem type, choosing adequate model class(es), and deciding on adequate evaluation metrics much helps us specify the technical problem we solve: *maximize the F1-score over the churn predictions of Orange's customers by implementing an artificial neural network with more than one hidden layer and an output layer containing a single neuron with a binary activation function*. \n",
    " \n",
    "\n",
    "Resources used:\n",
    "- Data: Orange has provided historical customer data (50,000 observations/customers; 230 features) for model optimization, selection, and evaluation. \n",
    "- Software: Python 3.8.5., main packages:\n",
    "    - Pandas, Numpy (for data wrangling)\n",
    "    - Keras/TensorFlow (for neural network modelling)\n",
    "    - Scikit-learn (for optimization/gridsearch automation)\n",
    "    - Matplotlib, Seaborn (for visualization)\n",
    "- Hardware: standard enduser office notebook (i7-8550U; 4 cores @1.80 GHz)\n",
    "\n",
    "Deliverable:\n",
    "-  source code in Python that can serve as proof of concept before deploying and putting into production (check vocabulary; for containerization also check this: https://github.com/Azure-Samples/MachineLearningSamples-TDSPUCIAdultIncome/blob/master/docs/deliverable_docs/ProjectReport.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.2. Technical Solution: Exploratory Data Analysis (EDA)*\n",
    "Now that we know precisely which technical problem we solve in this project, we familiarize ourselves a bit better with the historical customer data Orange has provided. This exploratory data analysis helps us identify how we need to preprocess the data so that the ANN will be able to best exploit it for churn prediction. This typically involves some basic overall checks (overall dataset structure, feature types, missing values), but also analyses more focused on our target variable (= what we want to predict), that is, the class label vector \"churn\"/\"no churn\".\n",
    "\n",
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_table('data/orange_small_train.data')\n",
    "data_labels = pd.read_table('data/orange_small_train_churn.labels', header=None, names=['Churn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>...</th>\n",
       "      <th>Var221</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var224</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "      <th>Var230</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1526.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>fXVEsaq</td>\n",
       "      <td>jySVZNlOJy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xb3V</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>525.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>2Kb5FSF</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fKCe</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5236.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Al6ZaUT</td>\n",
       "      <td>NKv4yOc</td>\n",
       "      <td>jySVZNlOJy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>Qu4f</td>\n",
       "      <td>02N6s8f</td>\n",
       "      <td>ib5G6X1eUxUn6</td>\n",
       "      <td>am7c</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>CE7uk3u</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>1J2cvxe</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>mj86</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 230 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Var1  Var2  Var3  Var4  Var5    Var6  Var7  Var8  Var9  Var10  ...  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN  1526.0   7.0   NaN   NaN    NaN  ...   \n",
       "1   NaN   NaN   NaN   NaN   NaN   525.0   0.0   NaN   NaN    NaN  ...   \n",
       "2   NaN   NaN   NaN   NaN   NaN  5236.0   7.0   NaN   NaN    NaN  ...   \n",
       "3   NaN   NaN   NaN   NaN   NaN     NaN   0.0   NaN   NaN    NaN  ...   \n",
       "4   NaN   NaN   NaN   NaN   NaN  1029.0   7.0   NaN   NaN    NaN  ...   \n",
       "\n",
       "    Var221   Var222      Var223  Var224  Var225  Var226   Var227  \\\n",
       "0     oslk  fXVEsaq  jySVZNlOJy     NaN     NaN    xb3V     RAYp   \n",
       "1     oslk  2Kb5FSF  LM8l689qOp     NaN     NaN    fKCe     RAYp   \n",
       "2  Al6ZaUT  NKv4yOc  jySVZNlOJy     NaN    kG3k    Qu4f  02N6s8f   \n",
       "3     oslk  CE7uk3u  LM8l689qOp     NaN     NaN    FSa2     RAYp   \n",
       "4     oslk  1J2cvxe  LM8l689qOp     NaN    kG3k    FSa2     RAYp   \n",
       "\n",
       "          Var228  Var229  Var230  \n",
       "0  F2FyR07IdsN7I     NaN     NaN  \n",
       "1  F2FyR07IdsN7I     NaN     NaN  \n",
       "2  ib5G6X1eUxUn6    am7c     NaN  \n",
       "3  F2FyR07IdsN7I     NaN     NaN  \n",
       "4  F2FyR07IdsN7I    mj86     NaN  \n",
       "\n",
       "[5 rows x 230 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Columns: 230 entries, Var1 to Var230\n",
      "dtypes: float64(191), int64(1), object(38)\n",
      "memory usage: 87.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check missing values (percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.697752347826087"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum().sum()/(data.shape[0]*data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 230 artists>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "temp = data.isna().sum()/(data.shape[0])\n",
    "plt.bar(range(len(temp)), sorted(temp), color='blue', alpha=0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Target variable analyses\n",
    "\n",
    "\n",
    "\n",
    "#### > Key insights from EDA:\n",
    "\n",
    "- explicit ANN data requirements; EDA (= departing from that + nice visualizations of basic dataset properties)\n",
    "- highlight peculiarity of dataset: MANY NaNs, will be main challenge to handle these in a way not compromising churn predictions\n",
    "- varibles are anonymized\n",
    "- missing values/sparsity\n",
    "- class balance\n",
    "-  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.3. Technical Solution: Data Preprocessing*\n",
    "(e.g., data cleaning (data types, missing value handling), feature engineering, feature selection, dimensionality reduction, variable encoding, normalization, resampling...)\n",
    "\n",
    "- proceed from EDA main findings\n",
    "- optimization: \"Optimization of only thought about in terms of tuning hyperparamters; but also preprocessing includes many steps that can be done in different ways - meaning also has potential to optimize: read following in conjunction with \"diary\" to see what I have optimzied here and with which success: <br>\n",
    " - (X) Dropping all columns which have more than 80% missing values\n",
    " - (X) Increase test set size to 20%\n",
    " - (X) Do not resample at all but work with class weights\n",
    " - (X) NaN handling: try in addition to creating binary indicator variables per column\n",
    " - (X) impute with column-wise median\n",
    " - (X) impute with column-wise mode (in most cases this equals a zero imputation)\n",
    " - (X) impute with column-wise minimum\n",
    " - (X) impute with column-wise maximum\n",
    " - (X) impute with column-wise NaN frequency\n",
    " - Feature Engineering:\n",
    "   - (X) DO NOT create binary indicator variables per column to indicate NaNs\n",
    "   - add column containing rows' NaN frequencies\n",
    " - Remove outliers (before norm/stand)\n",
    " - go from feature normalization to Standardization, mind sparsity\n",
    "   - (X) MinMax (wie in v1, nur jetzt über SKlearn Method statt manuell, daher zum Test reproduziert; diesen Schritt jetzt auch erst nach Splitting eingebaut)\n",
    "   - (X) RobustScaler\n",
    "   - (X) PowerTransformer\n",
    " - (X) Train and Reduce dimensionality (e.g., PCA); feature\n",
    " selection would have benefit to be explanable, however, features\n",
    " anonymized anyways\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### *2.4. Technical Solution: Model Selection (incl. Optimization)*\n",
    "(Model Choice, Training, Validation, Optimization)\n",
    "split dataset, choose/apply model, compile/train models, optimize model (gridsearch, cross-validation, stratified y/n, ...)\n",
    "\n",
    "pick best model that comes out of gridsearch and train on entire dataset (while during gridsearch has been trained only on fraction while validation data had been held out)\n",
    "\n",
    "- specify neural network as \"Feed-forward neural network\" - why not recurrent, LSTM that can loop/have memory -given goal in this project is to probe the hypoe around \"deep learning\", unnecessary complexity\n",
    "- in write-up: reflect on fact neural networks / deep learning seem to be overhyped\n",
    "- see e.g.: Peter Roßbach: \"Neural Networks vs. Random Forests – Does it always have to be Deep Learning?\n",
    "- - make that explicit point of the write-up! \"test\" that!\n",
    "- show here that I know how to work with learning curves\n",
    "\n",
    "when looking at results, come back to earlier point, explain via clas imbalance\n",
    "come back to earlier point: \n",
    "- make it one main technical point in the article that high accuracy can be misleading (when? why?) - have to also check other measures\n",
    "- - includein write-up my reflections for using precision/recall instead of AUC (argue by importance to detect minority class relative to importance of TPs and FPs) (expl in simple language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.5. Technical Solution: Final Model Evaluation*\n",
    "Model Evaluation on 'Unseen' Data (simulate by priorly held out 'Test Data')\n",
    "- Do the results make sense?\n",
    "\n",
    "- show/compare how accuracy can be misleading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.6. Technical Solution: Future Optimization Potentials*\n",
    "(hier sammeln alles ich zeitlich nicht geschafft hab, aber für wichtig halte - um Kritik zu preempten)\n",
    "\n",
    "Schema: Potential - Umsetzungsaufwand - erwarteter Umsetzungseffekt auf Business Metric\n",
    "\n",
    "- version 2: optimization potentials (versus v1) to explore ceteris paribus:\n",
    "- not explored/limitations: only individually optimized, due to constraints in processing power and time, optimization dependencies between variables neglected\n",
    "- only narrow ranges in gridsearch covered, so sound change that only found local optima per parameter\n",
    "- potential: NaN imputation with means on subsets of rows: one could search powerful clustering criteria first and than impute cluster means\n",
    "- also: was using smaller dataset, large dataset with many more variables may allow to increase a classifier's precision/recall\n",
    "- Make sure to also compare to others' results - I seem to be already working at the upper boundary of what's possible on this dataset with ANNs!\n",
    "- optimization potential: in practice, one would normally traing many different models and select/stack the best; show somehow that I'm aware of that\n",
    "- (optimization potential: add and compare AUC: simple logistic regression, random forest, 'flat' neural network, XGBoost)\n",
    "- optimization potential: put data into an AWS instance and run there\n",
    "- multicollinearity - check whether an issue - we want to have model as simple as possible!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Business Recommendations**\n",
    "\"What do the generated insights/model urge us/allow us to do different next Monday, and which value (business metric!) will that generate?\"\n",
    "\n",
    "direkt aus auftrag (1.) ableiten. incl.:\n",
    "- (after implementing comparative models:) \"turns out, deep learning (might) not be best for this kind of problem; best practice computer vision, very large datasets; here: tree model such as XGboost or simple logistic regression better \n",
    "\n",
    "Good Example: https://www.kaggle.com/code/hamzaben/employee-churn-model-w-strategic-retention-plan/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e858973d4b0767378b2a391e82dfc297a0e494a6ac1816312417e02d17e8ebb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
