{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Keep in mind while writing up:*\n",
    "\n",
    "- *Be concise! Less is more - the fully story is in the source code for those interested.*\n",
    "- *Be deliberate about: What to highlight in which section (e.g., “this dataset was special due to its high number of variables”…)*\n",
    "- *Work with visuals and only exceptionally with code. Refer to GitHub, dump code there, the technical people will go there. And (hiring) managers will only read the write-up.*\n",
    "- *Optimize business value, not model performance! Time/Resource constraints, ….*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Report\n",
    "# **Preventing Customer Churn with Feedforward Neural Networks**\n",
    "*Disclaimer: This mock project report serves educational purposes only. The company data is public (https://www.kdd.org/kdd-cup/view/kdd-cup-2009/Data). The author has no commercial relationship with mentioned parties.* \n",
    "***\n",
    "### **Executive Summary (max. 7 sentences)**\n",
    "Situation (1 sentence based on 1.)\n",
    "<br>\n",
    "Complication (1 sentence based on 1.)\n",
    "<br>\n",
    "Solution (1 sentence based on 2.)\n",
    "<br>\n",
    "Recommendations including Solutions' Business Value Add (1-3 sentences based on 3.)\n",
    "\"much buzz around ANN, let's test that here\"\n",
    "***\n",
    "### **Report Structure**\n",
    "[Include nice + simple process visualization!]\n",
    "1. Business Problem Statement\n",
    "2. Technical Solution\n",
    "<br>    *2.1. Technical Problem Statement*\n",
    "<br>    *2.2. Exploratory Data Analysis*\n",
    "<br>    *2.3. Data Preprocessing*\n",
    "<br>    *2.4. Feature Selection*\n",
    "<br>    *2.5. Model Selection (incl. Optimization)*\n",
    "<br>    *2.6. Final Model Evaluation*\n",
    "<br>    *2.7. Future Optimization Potentials*\n",
    "3. Business Recommendations\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Business Problem Statement**\n",
    "For firms like French telecommunication provider Orange, customer retention is critical. This is because retaining customers is much cheaper than the alternative: losing a customer and their revenues plus replacement costs. However, Orange lacks an automated, scalable, and data-driven method for predicting customer churn that would allow Orange to initiate retention measures before customers leave. That is, predicting customer churn currently more or less relies on sporadic guesses. Thus, Orange requested a proof-of-concept for a predictive model that can help identify customers who will likely churn so that retention measures can be initiated. Specifically, encouraged by the enthusiasm surrounding \"deep learning\", Orange wants the proof-of-concept to explore the potential of this model class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Technical Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.1. Technical Solution: Technical Problem Statement*\n",
    "The business problem, as put by Orange, is \"to predict customer churn\". This problem requires translation into a better specified, technical problem before it is solvable using mathematical-statistical methods. Technically put, the problem we solve is to\n",
    "\n",
    "*maximize the F1-score over the churn predictions of Orange's customers by implementing an artificial neural network with more than one hidden layer and an output layer containing a single neuron with an activation function*.\n",
    "\n",
    "Each component of this technical problem statement follows from considering the following three issues in light of the business problem we solve: \n",
    "\n",
    "#### Specifying the business problem\n",
    "It is first important to understand that predicting customer churn is, technically, a binary classification problem: given the data available for any particular customer (e.g., age, gender, purchased services, average call duration), we want our model to assign this customer to one of the two classes \"churn\"/\"no churn\". Understanding that we solve a classification problem has important implications for two main elements of the technical problem statement:\n",
    "\n",
    "#### Choosing an adequate model class\n",
    "In a typical data science project, we would train models from many different model classes (e.g., logistic regression classifiers, trees, support vector machines) and select the best performing models (or combine them in an *ensemble*) for deployment. In this project, however, the client Orange has specified upfront that they want a \"deep learning\" model, which in more precise technical terms is widely understood as an artificial neural network (ANN) with more than one hidden layer. Further, since we want the ANN's output to always be either \"churn\" or \"no churn\", its output layer must contain a single neuron with an activation function (e.g., ReLU, sigmoid) that translates continuous into binary values (1/0).\n",
    "\n",
    "#### Choosing adequate evaluation metrics\n",
    "An evaluation metric enables us to assess how \"good\" a developed model is and optimize it. The perhaps most intuitive metric for a classification model is the *accuracy* of its predictions. Accuracy tells us in which percentage of cases a classification model's predictions (\"churn\"/\"no churn\") are true (that is, correctly predict what customers will actually do). However, we can infer from the business context that the classes \"churn\"/\"no churn\" we are interested in are *imbalanced*: only a minority of all customers will churn in any given time period. We can thus expect many more customers to be in the \"no churn\" rather than the \"churn\" class. Accuracy will thus be a bad metric to optimize: the model could 'cheat' and simply predict \"no churn\" in 100% of the cases, and never detect a single churning customer, and still have awesome accuracy. In presence of class imbalance, a metric more adequate to optimize is the *F1-score*. A high F1-score indicates not only that the model is able to detect many of those customers who will indeed churn (high *recall*), but also that the model's \"churn\"-predictions are typically correct (high *precision*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following resources available to solve the problem:\n",
    "- Data: Orange has provided historical customer data (50,000 observations/customers; 230 features).\n",
    "- Software: Python 3.8.5., main packages:\n",
    "    - Pandas, Numpy (for data wrangling)\n",
    "    - Keras/TensorFlow (for neural network modelling)\n",
    "    - Scikit-learn (for feature selection and optimization/gridsearch automation)\n",
    "    - Matplotlib, Seaborn (for visualization)\n",
    "- Hardware: a standard office notebook with an i7-8550U (4 cores @1.80 GHz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.2. Technical Solution: Exploratory Data Analysis (EDA)*\n",
    "Now that we have specified the technical problem this project should solve, we first familiarize ourselves with the historical customer data Orange has provided. Exploratory data analysis helps us identify how we need to preprocess this data so that the ANN can better learn from it to predict churn. This typically involves some basic overall checks (overall dataset structure, feature types, missing values), but also analyses focused on our target variable, that is, the class label vector \"churn\"/\"no churn\" (= what we want to predict)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data\n",
    "We first load the data from a local drive. X is a matrix containing features and observations, y is a vector containing the class labels we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### some settings to increase reproducibility and report readability \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "seed(3992)\n",
    "tf.random.set_seed(3992)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"float_format\", \"{:f}\".format)\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "### Loading the data\n",
    "X = pd.read_table('data/orange_small_train.data')\n",
    "y = pd.read_table('data/orange_small_train_churn.labels', header = None,sep='\\t').loc[:, 0].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the overall dataset structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our very first analytical step is to take a broad look at the overall dataset structure, including the number of features (columns) and observations (rows = customers), feature names, features' data types, categorical features' cardinality, missing value formatting, and some basic descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>Var11</th>\n",
       "      <th>Var12</th>\n",
       "      <th>Var13</th>\n",
       "      <th>Var14</th>\n",
       "      <th>Var15</th>\n",
       "      <th>Var16</th>\n",
       "      <th>Var17</th>\n",
       "      <th>Var18</th>\n",
       "      <th>Var19</th>\n",
       "      <th>Var20</th>\n",
       "      <th>Var21</th>\n",
       "      <th>Var22</th>\n",
       "      <th>Var23</th>\n",
       "      <th>Var24</th>\n",
       "      <th>Var25</th>\n",
       "      <th>Var26</th>\n",
       "      <th>Var27</th>\n",
       "      <th>Var28</th>\n",
       "      <th>Var29</th>\n",
       "      <th>Var30</th>\n",
       "      <th>Var31</th>\n",
       "      <th>Var32</th>\n",
       "      <th>Var33</th>\n",
       "      <th>Var34</th>\n",
       "      <th>Var35</th>\n",
       "      <th>Var36</th>\n",
       "      <th>Var37</th>\n",
       "      <th>Var38</th>\n",
       "      <th>Var39</th>\n",
       "      <th>Var40</th>\n",
       "      <th>Var41</th>\n",
       "      <th>Var42</th>\n",
       "      <th>Var43</th>\n",
       "      <th>Var44</th>\n",
       "      <th>Var45</th>\n",
       "      <th>Var46</th>\n",
       "      <th>Var47</th>\n",
       "      <th>Var48</th>\n",
       "      <th>Var49</th>\n",
       "      <th>Var50</th>\n",
       "      <th>Var51</th>\n",
       "      <th>Var52</th>\n",
       "      <th>Var53</th>\n",
       "      <th>Var54</th>\n",
       "      <th>Var55</th>\n",
       "      <th>Var56</th>\n",
       "      <th>Var57</th>\n",
       "      <th>Var58</th>\n",
       "      <th>Var59</th>\n",
       "      <th>Var60</th>\n",
       "      <th>Var61</th>\n",
       "      <th>Var62</th>\n",
       "      <th>Var63</th>\n",
       "      <th>Var64</th>\n",
       "      <th>Var65</th>\n",
       "      <th>Var66</th>\n",
       "      <th>Var67</th>\n",
       "      <th>Var68</th>\n",
       "      <th>Var69</th>\n",
       "      <th>Var70</th>\n",
       "      <th>Var71</th>\n",
       "      <th>Var72</th>\n",
       "      <th>Var73</th>\n",
       "      <th>Var74</th>\n",
       "      <th>Var75</th>\n",
       "      <th>Var76</th>\n",
       "      <th>Var77</th>\n",
       "      <th>Var78</th>\n",
       "      <th>Var79</th>\n",
       "      <th>Var80</th>\n",
       "      <th>Var81</th>\n",
       "      <th>Var82</th>\n",
       "      <th>Var83</th>\n",
       "      <th>Var84</th>\n",
       "      <th>Var85</th>\n",
       "      <th>Var86</th>\n",
       "      <th>Var87</th>\n",
       "      <th>Var88</th>\n",
       "      <th>Var89</th>\n",
       "      <th>Var90</th>\n",
       "      <th>Var91</th>\n",
       "      <th>Var92</th>\n",
       "      <th>Var93</th>\n",
       "      <th>Var94</th>\n",
       "      <th>Var95</th>\n",
       "      <th>Var96</th>\n",
       "      <th>Var97</th>\n",
       "      <th>Var98</th>\n",
       "      <th>Var99</th>\n",
       "      <th>Var100</th>\n",
       "      <th>Var101</th>\n",
       "      <th>Var102</th>\n",
       "      <th>Var103</th>\n",
       "      <th>Var104</th>\n",
       "      <th>Var105</th>\n",
       "      <th>Var106</th>\n",
       "      <th>Var107</th>\n",
       "      <th>Var108</th>\n",
       "      <th>Var109</th>\n",
       "      <th>Var110</th>\n",
       "      <th>Var111</th>\n",
       "      <th>Var112</th>\n",
       "      <th>Var113</th>\n",
       "      <th>Var114</th>\n",
       "      <th>Var115</th>\n",
       "      <th>Var116</th>\n",
       "      <th>Var117</th>\n",
       "      <th>Var118</th>\n",
       "      <th>Var119</th>\n",
       "      <th>Var120</th>\n",
       "      <th>Var121</th>\n",
       "      <th>Var122</th>\n",
       "      <th>Var123</th>\n",
       "      <th>Var124</th>\n",
       "      <th>Var125</th>\n",
       "      <th>Var126</th>\n",
       "      <th>Var127</th>\n",
       "      <th>Var128</th>\n",
       "      <th>Var129</th>\n",
       "      <th>Var130</th>\n",
       "      <th>Var131</th>\n",
       "      <th>Var132</th>\n",
       "      <th>Var133</th>\n",
       "      <th>Var134</th>\n",
       "      <th>Var135</th>\n",
       "      <th>Var136</th>\n",
       "      <th>Var137</th>\n",
       "      <th>Var138</th>\n",
       "      <th>Var139</th>\n",
       "      <th>Var140</th>\n",
       "      <th>Var141</th>\n",
       "      <th>Var142</th>\n",
       "      <th>Var143</th>\n",
       "      <th>Var144</th>\n",
       "      <th>Var145</th>\n",
       "      <th>Var146</th>\n",
       "      <th>Var147</th>\n",
       "      <th>Var148</th>\n",
       "      <th>Var149</th>\n",
       "      <th>Var150</th>\n",
       "      <th>Var151</th>\n",
       "      <th>Var152</th>\n",
       "      <th>Var153</th>\n",
       "      <th>Var154</th>\n",
       "      <th>Var155</th>\n",
       "      <th>Var156</th>\n",
       "      <th>Var157</th>\n",
       "      <th>Var158</th>\n",
       "      <th>Var159</th>\n",
       "      <th>Var160</th>\n",
       "      <th>Var161</th>\n",
       "      <th>Var162</th>\n",
       "      <th>Var163</th>\n",
       "      <th>Var164</th>\n",
       "      <th>Var165</th>\n",
       "      <th>Var166</th>\n",
       "      <th>Var167</th>\n",
       "      <th>Var168</th>\n",
       "      <th>Var169</th>\n",
       "      <th>Var170</th>\n",
       "      <th>Var171</th>\n",
       "      <th>Var172</th>\n",
       "      <th>Var173</th>\n",
       "      <th>Var174</th>\n",
       "      <th>Var175</th>\n",
       "      <th>Var176</th>\n",
       "      <th>Var177</th>\n",
       "      <th>Var178</th>\n",
       "      <th>Var179</th>\n",
       "      <th>Var180</th>\n",
       "      <th>Var181</th>\n",
       "      <th>Var182</th>\n",
       "      <th>Var183</th>\n",
       "      <th>Var184</th>\n",
       "      <th>Var185</th>\n",
       "      <th>Var186</th>\n",
       "      <th>Var187</th>\n",
       "      <th>Var188</th>\n",
       "      <th>Var189</th>\n",
       "      <th>Var190</th>\n",
       "      <th>Var191</th>\n",
       "      <th>Var192</th>\n",
       "      <th>Var193</th>\n",
       "      <th>Var194</th>\n",
       "      <th>Var195</th>\n",
       "      <th>Var196</th>\n",
       "      <th>Var197</th>\n",
       "      <th>Var198</th>\n",
       "      <th>Var199</th>\n",
       "      <th>Var200</th>\n",
       "      <th>Var201</th>\n",
       "      <th>Var202</th>\n",
       "      <th>Var203</th>\n",
       "      <th>Var204</th>\n",
       "      <th>Var205</th>\n",
       "      <th>Var206</th>\n",
       "      <th>Var207</th>\n",
       "      <th>Var208</th>\n",
       "      <th>Var209</th>\n",
       "      <th>Var210</th>\n",
       "      <th>Var211</th>\n",
       "      <th>Var212</th>\n",
       "      <th>Var213</th>\n",
       "      <th>Var214</th>\n",
       "      <th>Var215</th>\n",
       "      <th>Var216</th>\n",
       "      <th>Var217</th>\n",
       "      <th>Var218</th>\n",
       "      <th>Var219</th>\n",
       "      <th>Var220</th>\n",
       "      <th>Var221</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var224</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "      <th>Var230</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1526.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>580.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166.560000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3570.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.076907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1350864.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7333.110000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>117625.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1175.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1212385.000000</td>\n",
       "      <td>69134.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>397579.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1812252.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38418.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bZkvyxLkBI</td>\n",
       "      <td>RO12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>taul</td>\n",
       "      <td>1K8T</td>\n",
       "      <td>lK27</td>\n",
       "      <td>ka_ns41</td>\n",
       "      <td>nQUveAzAF7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dXGu</td>\n",
       "      <td>9_Y1</td>\n",
       "      <td>FbIm</td>\n",
       "      <td>VpdQ</td>\n",
       "      <td>haYg</td>\n",
       "      <td>me75fM6ugJ</td>\n",
       "      <td>kIsH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>L84s</td>\n",
       "      <td>XfqtO3UdzaXh_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XTbPUYD</td>\n",
       "      <td>sH5Z</td>\n",
       "      <td>cJvF</td>\n",
       "      <td>FzaX</td>\n",
       "      <td>1YVfGrO</td>\n",
       "      <td>oslk</td>\n",
       "      <td>fXVEsaq</td>\n",
       "      <td>jySVZNlOJy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xb3V</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>353.520000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4764966.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.408032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2872928.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>151098.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58158.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>-356411.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>590.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4136430.000000</td>\n",
       "      <td>357038.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>278334.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10439160.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>238572.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CEat0G8rTN</td>\n",
       "      <td>RO12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>taul</td>\n",
       "      <td>1K8T</td>\n",
       "      <td>2Ix5</td>\n",
       "      <td>qEdASpP</td>\n",
       "      <td>y2LIM01bE1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lg1t</td>\n",
       "      <td>9_Y1</td>\n",
       "      <td>k13i</td>\n",
       "      <td>sJzTlal</td>\n",
       "      <td>zm5i</td>\n",
       "      <td>me75fM6ugJ</td>\n",
       "      <td>kIsH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>L84s</td>\n",
       "      <td>NhsEn4L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kZJyVg2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FzaX</td>\n",
       "      <td>0AJo2f2</td>\n",
       "      <td>oslk</td>\n",
       "      <td>2Kb5FSF</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fKCe</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5236.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1515.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>816.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>220.080000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5883894.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.599658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130</td>\n",
       "      <td>518.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1675776.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16211.580000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>405104.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3230.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5967.000000</td>\n",
       "      <td>-28.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3478905.000000</td>\n",
       "      <td>248932.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320565.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9826360.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>434946.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eOQt0GoOh3</td>\n",
       "      <td>AERks4l</td>\n",
       "      <td>SEuy</td>\n",
       "      <td>taul</td>\n",
       "      <td>1K8T</td>\n",
       "      <td>ffXs</td>\n",
       "      <td>NldASpP</td>\n",
       "      <td>y4g9XoZ</td>\n",
       "      <td>vynJTq9</td>\n",
       "      <td>smXZ</td>\n",
       "      <td>4bTR</td>\n",
       "      <td>9_Y1</td>\n",
       "      <td>MGOA</td>\n",
       "      <td>VpdQ</td>\n",
       "      <td>haYg</td>\n",
       "      <td>DHn_WUyBhW_whjA88g9bvA64_</td>\n",
       "      <td>kIsH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>L84s</td>\n",
       "      <td>UbxQ8lZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTGHfSv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pMWAe2U</td>\n",
       "      <td>bHR7</td>\n",
       "      <td>UYBR</td>\n",
       "      <td>FzaX</td>\n",
       "      <td>JFM1BiF</td>\n",
       "      <td>Al6ZaUT</td>\n",
       "      <td>NKv4yOc</td>\n",
       "      <td>jySVZNlOJy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>Qu4f</td>\n",
       "      <td>02N6s8f</td>\n",
       "      <td>ib5G6X1eUxUn6</td>\n",
       "      <td>am7c</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.080000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.988250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-275703.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-14.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jg69tYsGvO</td>\n",
       "      <td>RO12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>taul</td>\n",
       "      <td>1K8T</td>\n",
       "      <td>ssAy</td>\n",
       "      <td>_ybO0dd</td>\n",
       "      <td>4hMlgkf58mhwh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W8mQ</td>\n",
       "      <td>9_Y1</td>\n",
       "      <td>YULl</td>\n",
       "      <td>VpdQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>me75fM6ugJ</td>\n",
       "      <td>kIsH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>Mtgm</td>\n",
       "      <td>NhsEn4L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kq0dQfu</td>\n",
       "      <td>eKej</td>\n",
       "      <td>UYBR</td>\n",
       "      <td>FzaX</td>\n",
       "      <td>L91KIiz</td>\n",
       "      <td>oslk</td>\n",
       "      <td>CE7uk3u</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3216.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.552446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>82</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>784448.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37423.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89754.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>10714.840000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15111.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>150650.000000</td>\n",
       "      <td>66046.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3255.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>267162.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>644836.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IXSgUHShse</td>\n",
       "      <td>RO12</td>\n",
       "      <td>SEuy</td>\n",
       "      <td>taul</td>\n",
       "      <td>1K8T</td>\n",
       "      <td>uNkU</td>\n",
       "      <td>EKR938I</td>\n",
       "      <td>ThrHXVS</td>\n",
       "      <td>0v21jmy</td>\n",
       "      <td>smXZ</td>\n",
       "      <td>xklU</td>\n",
       "      <td>9_Y1</td>\n",
       "      <td>RVjC</td>\n",
       "      <td>sJzTlal</td>\n",
       "      <td>6JmL</td>\n",
       "      <td>me75fM6ugJ</td>\n",
       "      <td>kIsH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>L84s</td>\n",
       "      <td>XfqtO3UdzaXh_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SJs3duv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11p4mKe</td>\n",
       "      <td>H3p7</td>\n",
       "      <td>UYBR</td>\n",
       "      <td>FzaX</td>\n",
       "      <td>OrnLfvc</td>\n",
       "      <td>oslk</td>\n",
       "      <td>1J2cvxe</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>mj86</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Var1  Var2  Var3  Var4  Var5        Var6     Var7  Var8  Var9  Var10  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN 1526.000000 7.000000   NaN   NaN    NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN  525.000000 0.000000   NaN   NaN    NaN   \n",
       "2   NaN   NaN   NaN   NaN   NaN 5236.000000 7.000000   NaN   NaN    NaN   \n",
       "3   NaN   NaN   NaN   NaN   NaN         NaN 0.000000   NaN   NaN    NaN   \n",
       "4   NaN   NaN   NaN   NaN   NaN 1029.000000 7.000000   NaN   NaN    NaN   \n",
       "\n",
       "   Var11  Var12       Var13  Var14  Var15  Var16  Var17  Var18  Var19  Var20  \\\n",
       "0    NaN    NaN  184.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1    NaN    NaN    0.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2    NaN    NaN  904.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "3    NaN    NaN    0.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "4    NaN    NaN 3216.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "        Var21       Var22  Var23     Var24      Var25  Var26  Var27  \\\n",
       "0  464.000000  580.000000    NaN 14.000000 128.000000    NaN    NaN   \n",
       "1  168.000000  210.000000    NaN  2.000000  24.000000    NaN    NaN   \n",
       "2 1212.000000 1515.000000    NaN 26.000000 816.000000    NaN    NaN   \n",
       "3         NaN    0.000000    NaN       NaN   0.000000    NaN    NaN   \n",
       "4   64.000000   80.000000    NaN  4.000000  64.000000    NaN    NaN   \n",
       "\n",
       "       Var28  Var29  Var30  Var31  Var32  Var33  Var34    Var35  Var36  Var37  \\\n",
       "0 166.560000    NaN    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "1 353.520000    NaN    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "2 220.080000    NaN    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "3  22.080000    NaN    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "4 200.000000    NaN    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "\n",
       "           Var38  Var39  Var40  Var41  Var42  Var43    Var44  Var45  Var46  \\\n",
       "0    3570.000000    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "1 4764966.000000    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "2 5883894.000000    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "3       0.000000    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "4       0.000000    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "\n",
       "   Var47  Var48  Var49  Var50  Var51  Var52  Var53  Var54  Var55  Var56  \\\n",
       "0    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "3    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "4    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "     Var57  Var58  Var59  Var60  Var61  Var62  Var63  Var64     Var65  Var66  \\\n",
       "0 4.076907    NaN    NaN    NaN    NaN    NaN    NaN    NaN  9.000000    NaN   \n",
       "1 5.408032    NaN    NaN    NaN    NaN    NaN    NaN    NaN  9.000000    NaN   \n",
       "2 6.599658    NaN    NaN    NaN    NaN    NaN    NaN    NaN  9.000000    NaN   \n",
       "3 1.988250    NaN    NaN    NaN    NaN    NaN    NaN    NaN  9.000000    NaN   \n",
       "4 4.552446    NaN    NaN    NaN    NaN    NaN    NaN    NaN 18.000000    NaN   \n",
       "\n",
       "   Var67  Var68  Var69  Var70  Var71    Var72  Var73      Var74  Var75  \\\n",
       "0    NaN    NaN    NaN    NaN    NaN      NaN     36  35.000000    NaN   \n",
       "1    NaN    NaN    NaN    NaN    NaN 3.000000     26   0.000000    NaN   \n",
       "2    NaN    NaN    NaN    NaN    NaN      NaN    130 518.000000    NaN   \n",
       "3    NaN    NaN    NaN    NaN    NaN      NaN     12   0.000000    NaN   \n",
       "4    NaN    NaN    NaN    NaN    NaN 3.000000     82 224.000000    NaN   \n",
       "\n",
       "           Var76  Var77    Var78  Var79  Var80         Var81  Var82     Var83  \\\n",
       "0 1350864.000000    NaN 0.000000    NaN    NaN   7333.110000    NaN  5.000000   \n",
       "1 2872928.000000    NaN 3.000000    NaN    NaN 151098.900000    NaN 25.000000   \n",
       "2 1675776.000000    NaN 0.000000    NaN    NaN  16211.580000    NaN 40.000000   \n",
       "3       0.000000    NaN 0.000000    NaN    NaN           NaN    NaN  0.000000   \n",
       "4  784448.000000    NaN 0.000000    NaN    NaN  37423.500000    NaN  0.000000   \n",
       "\n",
       "   Var84     Var85  Var86  Var87  Var88  Var89  Var90  Var91  Var92  Var93  \\\n",
       "0    NaN 12.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1    NaN  2.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2    NaN 58.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "3    NaN  0.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "4    NaN  0.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "         Var94  Var95  Var96  Var97  Var98  Var99  Var100  Var101  Var102  \\\n",
       "0          NaN    NaN    NaN    NaN    NaN    NaN     NaN     NaN     NaN   \n",
       "1 58158.000000    NaN    NaN    NaN    NaN    NaN     NaN     NaN     NaN   \n",
       "2          NaN    NaN    NaN    NaN    NaN    NaN     NaN     NaN     NaN   \n",
       "3          NaN    NaN    NaN    NaN    NaN    NaN     NaN     NaN     NaN   \n",
       "4 89754.000000    NaN    NaN    NaN    NaN    NaN     NaN     NaN     NaN   \n",
       "\n",
       "   Var103  Var104  Var105  Var106  Var107  Var108     Var109  Var110  Var111  \\\n",
       "0     NaN     NaN     NaN     NaN     NaN     NaN 104.000000     NaN     NaN   \n",
       "1     NaN     NaN     NaN     NaN     NaN     NaN  40.000000     NaN     NaN   \n",
       "2     NaN     NaN     NaN     NaN     NaN     NaN 312.000000     NaN     NaN   \n",
       "3     NaN     NaN     NaN     NaN     NaN     NaN        NaN     NaN     NaN   \n",
       "4     NaN     NaN     NaN     NaN     NaN     NaN  32.000000     NaN     NaN   \n",
       "\n",
       "      Var112         Var113  Var114  Var115  Var116  Var117  Var118  \\\n",
       "0 168.000000  117625.600000     NaN     NaN     NaN     NaN     NaN   \n",
       "1  40.000000 -356411.600000     NaN     NaN     NaN     NaN     NaN   \n",
       "2 336.000000  405104.000000     NaN     NaN     NaN     NaN     NaN   \n",
       "3   0.000000 -275703.600000     NaN     NaN     NaN     NaN     NaN   \n",
       "4  56.000000   10714.840000     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "       Var119  Var120  Var121  Var122     Var123  Var124       Var125  \\\n",
       "0 1175.000000     NaN     NaN     NaN   6.000000     NaN   720.000000   \n",
       "1  590.000000     NaN     NaN     NaN  72.000000     NaN     0.000000   \n",
       "2 3230.000000     NaN     NaN     NaN 114.000000     NaN  5967.000000   \n",
       "3         NaN     NaN     NaN     NaN   0.000000     NaN     0.000000   \n",
       "4  215.000000     NaN     NaN     NaN   0.000000     NaN 15111.000000   \n",
       "\n",
       "      Var126  Var127  Var128  Var129  Var130  Var131   Var132         Var133  \\\n",
       "0   8.000000     NaN     NaN     NaN     NaN     NaN 0.000000 1212385.000000   \n",
       "1        NaN     NaN     NaN     NaN     NaN     NaN 8.000000 4136430.000000   \n",
       "2 -28.000000     NaN     NaN     NaN     NaN     NaN 0.000000 3478905.000000   \n",
       "3 -14.000000     NaN     NaN     NaN     NaN     NaN 0.000000       0.000000   \n",
       "4  58.000000     NaN     NaN     NaN     NaN     NaN 0.000000  150650.000000   \n",
       "\n",
       "         Var134  Var135  Var136  Var137  Var138  Var139      Var140  Var141  \\\n",
       "0  69134.000000     NaN     NaN     NaN     NaN     NaN  185.000000     NaN   \n",
       "1 357038.000000     NaN     NaN     NaN     NaN     NaN    0.000000     NaN   \n",
       "2 248932.000000     NaN     NaN     NaN     NaN     NaN  800.000000     NaN   \n",
       "3      0.000000     NaN     NaN     NaN     NaN     NaN    0.000000     NaN   \n",
       "4  66046.000000     NaN     NaN     NaN     NaN     NaN 3255.000000     NaN   \n",
       "\n",
       "   Var142   Var143    Var144  Var145  Var146  Var147  Var148        Var149  \\\n",
       "0     NaN 0.000000  9.000000     NaN     NaN     NaN     NaN 397579.000000   \n",
       "1     NaN 0.000000  9.000000     NaN     NaN     NaN     NaN 278334.000000   \n",
       "2     NaN 0.000000 36.000000     NaN     NaN     NaN     NaN 320565.000000   \n",
       "3     NaN 0.000000       NaN     NaN     NaN     NaN     NaN           NaN   \n",
       "4     NaN 0.000000  9.000000     NaN     NaN     NaN     NaN 267162.000000   \n",
       "\n",
       "   Var150  Var151  Var152          Var153  Var154  Var155  Var156  Var157  \\\n",
       "0     NaN     NaN     NaN  1812252.000000     NaN     NaN     NaN     NaN   \n",
       "1     NaN     NaN     NaN 10439160.000000     NaN     NaN     NaN     NaN   \n",
       "2     NaN     NaN     NaN  9826360.000000     NaN     NaN     NaN     NaN   \n",
       "3     NaN     NaN     NaN        0.000000     NaN     NaN     NaN     NaN   \n",
       "4     NaN     NaN     NaN   644836.000000     NaN     NaN     NaN     NaN   \n",
       "\n",
       "   Var158  Var159     Var160  Var161  Var162        Var163  Var164  Var165  \\\n",
       "0     NaN     NaN 142.000000     NaN     NaN  38418.000000     NaN     NaN   \n",
       "1     NaN     NaN  32.000000     NaN     NaN 238572.000000     NaN     NaN   \n",
       "2     NaN     NaN 206.000000     NaN     NaN 434946.000000     NaN     NaN   \n",
       "3     NaN     NaN   0.000000     NaN     NaN      0.000000     NaN     NaN   \n",
       "4     NaN     NaN   2.000000     NaN     NaN      0.000000     NaN     NaN   \n",
       "\n",
       "   Var166  Var167  Var168  Var169  Var170  Var171  Var172   Var173  Var174  \\\n",
       "0     NaN     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN   \n",
       "1     NaN     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN   \n",
       "2     NaN     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN   \n",
       "3     NaN     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN   \n",
       "4     NaN     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN   \n",
       "\n",
       "   Var175  Var176  Var177  Var178  Var179  Var180   Var181  Var182  Var183  \\\n",
       "0     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN     NaN   \n",
       "1     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN     NaN   \n",
       "2     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN     NaN   \n",
       "3     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN     NaN   \n",
       "4     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN     NaN   \n",
       "\n",
       "   Var184  Var185  Var186  Var187  Var188     Var189  Var190 Var191  \\\n",
       "0     NaN     NaN     NaN     NaN     NaN 462.000000     NaN    NaN   \n",
       "1     NaN     NaN     NaN     NaN     NaN        NaN     NaN    NaN   \n",
       "2     NaN     NaN     NaN     NaN     NaN        NaN     NaN    NaN   \n",
       "3     NaN     NaN     NaN     NaN     NaN        NaN     NaN    NaN   \n",
       "4     NaN     NaN     NaN     NaN     NaN        NaN     NaN    NaN   \n",
       "\n",
       "       Var192   Var193 Var194 Var195 Var196 Var197   Var198         Var199  \\\n",
       "0  bZkvyxLkBI     RO12    NaN   taul   1K8T   lK27  ka_ns41     nQUveAzAF7   \n",
       "1  CEat0G8rTN     RO12    NaN   taul   1K8T   2Ix5  qEdASpP     y2LIM01bE1   \n",
       "2  eOQt0GoOh3  AERks4l   SEuy   taul   1K8T   ffXs  NldASpP        y4g9XoZ   \n",
       "3  jg69tYsGvO     RO12    NaN   taul   1K8T   ssAy  _ybO0dd  4hMlgkf58mhwh   \n",
       "4  IXSgUHShse     RO12   SEuy   taul   1K8T   uNkU  EKR938I        ThrHXVS   \n",
       "\n",
       "    Var200 Var201 Var202 Var203 Var204   Var205 Var206  \\\n",
       "0      NaN    NaN   dXGu   9_Y1   FbIm     VpdQ   haYg   \n",
       "1      NaN    NaN   lg1t   9_Y1   k13i  sJzTlal   zm5i   \n",
       "2  vynJTq9   smXZ   4bTR   9_Y1   MGOA     VpdQ   haYg   \n",
       "3      NaN    NaN   W8mQ   9_Y1   YULl     VpdQ    NaN   \n",
       "4  0v21jmy   smXZ   xklU   9_Y1   RVjC  sJzTlal   6JmL   \n",
       "\n",
       "                      Var207 Var208  Var209 Var210 Var211         Var212  \\\n",
       "0                 me75fM6ugJ   kIsH     NaN   uKAI   L84s  XfqtO3UdzaXh_   \n",
       "1                 me75fM6ugJ   kIsH     NaN   uKAI   L84s        NhsEn4L   \n",
       "2  DHn_WUyBhW_whjA88g9bvA64_   kIsH     NaN   uKAI   L84s        UbxQ8lZ   \n",
       "3                 me75fM6ugJ   kIsH     NaN   uKAI   Mtgm        NhsEn4L   \n",
       "4                 me75fM6ugJ   kIsH     NaN   uKAI   L84s  XfqtO3UdzaXh_   \n",
       "\n",
       "  Var213   Var214 Var215   Var216 Var217 Var218 Var219   Var220   Var221  \\\n",
       "0    NaN      NaN    NaN  XTbPUYD   sH5Z   cJvF   FzaX  1YVfGrO     oslk   \n",
       "1    NaN      NaN    NaN  kZJyVg2    NaN    NaN   FzaX  0AJo2f2     oslk   \n",
       "2    NaN  TTGHfSv    NaN  pMWAe2U   bHR7   UYBR   FzaX  JFM1BiF  Al6ZaUT   \n",
       "3    NaN      NaN    NaN  kq0dQfu   eKej   UYBR   FzaX  L91KIiz     oslk   \n",
       "4    NaN  SJs3duv    NaN  11p4mKe   H3p7   UYBR   FzaX  OrnLfvc     oslk   \n",
       "\n",
       "    Var222      Var223 Var224 Var225 Var226   Var227         Var228 Var229  \\\n",
       "0  fXVEsaq  jySVZNlOJy    NaN    NaN   xb3V     RAYp  F2FyR07IdsN7I    NaN   \n",
       "1  2Kb5FSF  LM8l689qOp    NaN    NaN   fKCe     RAYp  F2FyR07IdsN7I    NaN   \n",
       "2  NKv4yOc  jySVZNlOJy    NaN   kG3k   Qu4f  02N6s8f  ib5G6X1eUxUn6   am7c   \n",
       "3  CE7uk3u  LM8l689qOp    NaN    NaN   FSa2     RAYp  F2FyR07IdsN7I    NaN   \n",
       "4  1J2cvxe  LM8l689qOp    NaN   kG3k   FSa2     RAYp  F2FyR07IdsN7I   mj86   \n",
       "\n",
       "   Var230  \n",
       "0     NaN  \n",
       "1     NaN  \n",
       "2     NaN  \n",
       "3     NaN  \n",
       "4     NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (Temp/ipykernel_2692/1072454990.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\MARC~1.FEL\\AppData\\Local\\Temp/ipykernel_2692/1072454990.py\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    plt.pyplot.xticks(np.arange(0, len(temp['categorical features'])+1, 20)\u001b[0m\n\u001b[1;37m                                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "features_cat = list(X.select_dtypes(include=['object']).columns)\n",
    "temp = X[features_cat]\n",
    "temp = temp.nunique().sort_values(ascending=False).reset_index()\n",
    "temp.columns = ['categorical features', 'number of different categories']\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "ax = sns.barplot(x='categorical features', y='number of different categories', data=temp, color='black', alpha=0.7)\n",
    "print(\"Categorical features' mean cardinality:\", round(temp['number of different categories'].sum()/temp.shape[0],0))\n",
    "plt.pyplot.xticks(np.arange(0, len(temp['categorical features'])+1, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we noted that the data contains missing values (\"NaN\"), we want to know precisely which percentage of values is missing in the data we have been given, and also how these missing values are distributed across features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(X.isna().sum().sum()/(X.shape[0]*X.shape[1]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 230 artists>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp = X.isna().sum()/(X.shape[0])\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.bar(range(len(temp)), sorted(temp), color='black', alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking class balance \"churn\"/\"no churn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having acquired an overall impression of the data, we now take a more focused look on our target variable 'Churn', that is, the column containing the class labels our model will need to predict.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts().plot.bar()\n",
    "plt.ylabel('value')\n",
    "plt.title('churn value for each class')\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### > EDA key insights:\n",
    "To summarize, our brief exploratory analysis helped us gain some important insights about the features, observations, and target variable in our data. Going forward, we will need to keep these insights at the back of our mind as they will instruct us how to properly preprocess the data so that our predictive model can learn well from it. Key insight we have gleaned are:\n",
    "\n",
    "- Features: \n",
    "\t- Most or all features' scales differ.\n",
    "\t- Orange has anonymized data before providing it (likely to protect customers' privacy).\n",
    "\t- The 230 total features include 38 categorical and 192 numerical features.\n",
    "\t- High cardinality is a big issue in this dataset (on average, 1882 unique values per categorical feature)\n",
    "- Observations:\n",
    "\t- We have been given data on 50.000 customers and their churn behaviour that we can use for optimizing and evaluating our model.\n",
    "\t- Missing values are another big issue in this dataset (around 70%).\n",
    "- Target variable:\n",
    "\t- As expected, the two classes in our target variable 'Churn' are heavily imbalanced (around 1 churning customer for 12 non-churning customers).\n",
    "\t- The class labels \"churn\"/\"no churn\" are represented by the numerical values 1/-1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.3. Technical Solution: Data Preprocessing*\n",
    "Before the data can be used for effective model training, it needs to be preprocessed. This is because the ANN model we implement requires data to be in a particular format to effectively learn from the data how to make good predictions. In addition, since preprocessing steps often have interdependencies, being deliberate about their order is important. Otherwise we risk messing up the data used for model training, compromising prediction quality. \n",
    "\n",
    "Main preprocessing steps that the key insights gleaned from EDA suggest are the following:\n",
    "- remove observations and features with all values missing\n",
    "- replace infrequent categories with a single 'catch-all' category\n",
    "- create binary indicator columns for missing values\n",
    "- impute missing values\n",
    "- encode (= 'make numerical') categorical features\n",
    "- normalize the features' scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove observations and features with all values missing\n",
    "We first remove any potential observations and features which might not hold any values. These will only push computation times and risk complicating our model without adding anything to its explanatory power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dropna(axis=0, how='all', inplace=True)\n",
    "X.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see comparing to above, no observations, but 18 features have been removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace infrequent categories with a single catch-all category\n",
    "EDA has shown that a big issue in the data is the high mean cardinality of categorical features. If there are many different categories, it will be hard for our model to recognize patterns that the model can exploit for predicting customer churn. Thus, to help our model more easily infer useful explanatory patterns, we replace all categories whose frequency of occurrence lies below a cetrain treshold with a single category \"RARE_VALUE\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cat = list(X.select_dtypes(include=['object']).columns) # update this since we have dropped some features\n",
    "for feat in features_cat:\n",
    "    X.loc[X[feat].value_counts(dropna=False)[X[feat]].values < X.shape[0] * 0.02, feat] = 'RARE_VALUE' # replace all categories which occurr in less than 2% of observatiosn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated by the following fraction of cells in which categories have been replaced, this preprocessing step will have a significant impact on our later model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X == 'RARE_VALUE'].count().sum()/(X.shape[0]*X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting aside some test data\n",
    "Before we proceed with the next preprocessing steps, we put aside some test data. The idea behing putting aside test data is essentially to pretend that we would possess some of the new/unseen data that will actually be only coming in after our model will have been deployed (= in actual use, or 'production'). This trick allows us to get an idea about how good our model would perform 'out in the wild'.\n",
    "<br>\n",
    "<br>\n",
    "However, the need to put aside test data also introduces additional complexity into preprocessing: For preprocessing steps whose parameters depend on the data they are applied to (e.g., scaling, encoding), we have to act like we would not have the test data available. This means we need to first preprocess the training data, and then the test data separately in the exact same way. Not doing this and having the test data still 'in' during data-dependent preprocessing steps would allow our model to learn from information that the model could, conceptually speaking, impossibly have access to. This is called 'peeking' or 'data leakage'. Data leakage risks leading our evaluation metrics to tell us that we have built a good model, but then after deployment find out that we have actually built a bad model - that is, when time and money have already been wasted.\n",
    "<br>\n",
    "<br>\n",
    "There is yet another issue we need to account for when putting aside test data: the class imbalance in our data that our EDA has confirmed. Usually, we would simply randomly split test and training data. However, since we need the training and test data to reflect the dataset's class distribution (\"churn\"/\"no churn\"), we perform a *stratified* split, that is, a split replicating the dataset's class distribution to both the created training and test subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=3992)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding missing value indicator features\n",
    "We now return once more to the issue of missing values. Since the number of missing values in the data is so high, we want to think some more about how we might exploit that. While a missing value represents an absence of information, the very fact per se that a value is missing might indicate something (e.g., a customer not booking a particular service) and possess predictive power. Thus, for each feature with missing values, we create an additional feature which indicates via the values 1 or 0 for each observation the presence or absence of a missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clm in X_train:\n",
    "    if X_train[clm].isna().sum() > 0:\n",
    "        X_train.insert(X_train.shape[1], f\"{clm}_NaNInd\", 0)\n",
    "        X_train[f\"{clm}_NaNInd\"] = np.where(pd.isnull(X_train[clm]), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, 185 binary variables have beend added to our training data, resulting in 397 variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing missing values\n",
    "To add at least some meaningful information to the data where values are missing, we impute missing values in numerical features with column-wise means, and missing values in categorical features with the category 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "features_num_train = list(X_train.select_dtypes(include=['float']).columns)\n",
    "imputer_nums = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_train[features_num_train] = imputer_nums.fit_transform(X_train[features_num_train])\n",
    "\n",
    "features_cat_train = list(X_train.select_dtypes(include=['object']).columns)\n",
    "imputer_cats = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='unknown')\n",
    "X_train[features_cat_train] = imputer_cats.fit_transform(X_train[features_cat_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our data now does not contain any missing values anymore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(X_train.isna().sum().sum()/(X_train.shape[0]*X.shape[1]), 3) #returns the percentage of missing values in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding categorical features\n",
    "We have also learned during EDA that there are quite some categorical features. Our ANN model (like many other model classes), however, can only handle numerical inputs. We thus *one-hot encode* (OHE) the categorical features. This means that for each unique value in each categorical feature, we create an additional feature which indicates via the values 1 or 0 for each observation the presence or absence of that unqiue value. We drop the original features and only keep the one-hot encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "enc = make_column_transformer((OneHotEncoder(max_categories=20, handle_unknown='ignore'), features_cat_train), remainder='passthrough')\n",
    "transformed = enc.fit_transform(X_train)\n",
    "enc_df = pd.DataFrame(transformed, columns=enc.get_feature_names())\n",
    "cols = enc_df.columns.tolist()\n",
    "cols = cols[168:] + cols[:168]\n",
    "X_train = enc_df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, OHE has added 130 dummy variables to our training data, resulting in 527 variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rescale features\n",
    "Finally, we rescale the features. We have seen in EDA that the features were measured using different scales. To rule out that this could affect our model, we remove from all features (except the indicator columns we added) the dataset mean and scale the feature to unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train[features_num_train] = scaler.fit_transform(X_train[features_num_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that scaling has happened properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming class labels\n",
    "We finally bring the class labels into a more suitable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.replace(-1, 0, inplace=True)\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set preprocessing\n",
    "We now apply the same preprocessing steps, in the same order, with the same parameters, to the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert NaN indicator columns\n",
    "for clm in X_test:\n",
    "    if X_test[clm].isna().sum() > 0:\n",
    "        X_test.insert(X_test.shape[1], f\"{clm}_NaNInd\", 0)\n",
    "        X_test[f\"{clm}_NaNInd\"] = np.where(pd.isnull(X_test[clm]), 1, 0)\n",
    "\n",
    "# missing value imputation: apply imputers that have been fit to training data\n",
    "X_test[features_num_train] = imputer_nums.transform(X_test[features_num_train])\n",
    "X_test[features_cat_train] = imputer_cats.transform(X_test[features_cat_train])\n",
    "X_test.isna().sum().sum()\n",
    "\n",
    "# Encode categorical features\n",
    "transformed = enc.transform(X_test)\n",
    "enc_df = pd.DataFrame(transformed, columns=enc.get_feature_names())\n",
    "cols = enc_df.columns.tolist()\n",
    "cols = cols[168:] + cols[:168]\n",
    "X_test = enc_df[cols]\n",
    "\n",
    "# apply scaler that has been fit to training data\n",
    "X_test[features_num_train] = scaler.transform(X_test[features_num_train])\n",
    "\n",
    "# transform target variable\n",
    "y_test.replace(-1, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We complete preprocessing by ensuring that the preprocessed training and test datasets have the same 527 features, in the same order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of identical columns:', sum(X_train.columns == X_test.columns)/len(X_train.columns)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### *2.5. Technical Solution: Feature Selection*\n",
    "\n",
    "While we have now preprocessed our data, the number of features is very high. In other words, our data is high-dimensional. This is undesirable for a variety of reasons, among the most important ones computational overhead and increased overfitting risk (see *curse of dimensionality*). Generally speaking, we want predictive models to use all the features with the most predictive power, but also as few features as possible. Thus, before we move on to building our ANN model, we train a random forest classifier model from which we can extract those features which have helped the forest model to best split observations into churning and non-churning customers.\n",
    "<br>\n",
    "<br>\n",
    "We first split our training data into a training and a cross-validation dataset for feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fs, X_cv_fs, y_train_fs, y_cv_fs = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2, random_state=3992)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that we do not select features based on an extremely bad random forest model, we optimize the model a little. Specifically, we grow multiple forest models with different numbers of trees, and with different maximum depths: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_classifier = RandomForestClassifier()\n",
    "parameters = {'max_depth':np.arange(3,10),'n_estimators':list(range(25,251,25))}\n",
    "random_grid = GridSearchCV(random_classifier, parameters, cv=3)\n",
    "random_grid.fit(X_cv_fs, y_cv_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing all forest models that we have grown, it turns out that the following parameters create the best random forest classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal Forest Hyperparams:\", random_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile a random forest classifier with the identified optimum parameters and train it on the training data for feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=25,\n",
    "    max_depth=3,\n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None,\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced',\n",
    "    random_state=3992,\n",
    "    verbose=0,\n",
    "    warm_start=False)\n",
    "\n",
    "rf_model.fit(X=X_train_fs, y=y_train_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract and visualize from this optimized forest model the features that most helped the model tell apart churning and non-churning customers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(rf_model.feature_importances_, index = X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "# feature importance plot\n",
    "std = np.std([rf_model.feature_importances_[:101] for rf_model in rf_model.estimators_], axis=0)\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "feature_importances[:101].plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean Decrease in Impurity (MDI)\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, only a very small number of the more than 500 features is actually helpful for predicting customer churn. Using only these features will save A LOT of computation time during ANN training, and also greatly reduce ANN complexity, without losing prediction quality. More simply put, feature selection has helped us make better predictions MUCH less costly. We thus throw all features but the most important ones from the training and test data that we use for ANN training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_imp_feats = feature_importances[:26].index.to_list()\n",
    "X_train = X_train[most_imp_feats]\n",
    "X_test = X_test[most_imp_feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### *2.4. Technical Solution: Model Selection (incl. Optimization)*\n",
    "\n",
    "Now that we have preprocessed our data and identified the most important features, we use the data for building and optimizing an artificial neural network classification model. Here, we will benefit from having spelled out earlier the technical problem we solve (*maximize the F1-score over the churn predictions of Orange's customers by implementing an artificial neural network with more than one hidden layer and an output layer containing a single neuron with an activation function*). This statement will guide us in the following steps: defining an optimization strategy, defining the ANN's architecture, optimization, and final model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model optimization strategy\n",
    "The reason for defining a model optimization strategy is simple: resources are limited, but the number of possible models is infinite (literally). This is due to the typically high number of optimizable parameters, and the infinite ranges of possible values for several of these parameters. This implies that simply jumping into optimization will likely make one end up endlessly tune everything and anything. Generally speaking, while model optimization strategies should be tailored to data science projects case-by-case, formulating them always requires a thorough understanding of the project's stakeholder expectations, the problem to be solved, the model classes used, and the resources available.\n",
    "<br>\n",
    "<br>\n",
    "For the prediction of churn among Orange's customers, we will focus our optimization strategy on four basic elements:\n",
    "- (1) the evaluation metric\n",
    "- (2) the optimization metric\n",
    "- (3) the optimized hyperparameters, and\n",
    "- (4) the optimization procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) _Evaluation metric_: F1-score.\n",
    "<br>\n",
    "We use this metric to identify which model among all trained models best solves our problem. We have explained our choice of the F1-score in 2.1. We also pull some further metrics from Keras' metrics library to enable a more comprehensive assessment of the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "scoring = {\n",
    "    \"F1\": sklearn.metrics.make_scorer(sklearn.metrics.f1_score),\n",
    "    'ROC_AUC': sklearn.metrics.make_scorer(sklearn.metrics.roc_auc_score),\n",
    "    \"Accuracy\": sklearn.metrics.make_scorer(sklearn.metrics.accuracy_score),\n",
    "    \"Recall\": sklearn.metrics.make_scorer(sklearn.metrics.recall_score),\n",
    "    \"Precision\": sklearn.metrics.make_scorer(sklearn.metrics.precision_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) _Optimization metric_: Binary cross-entropy loss.\n",
    "<br>\n",
    "We use this metric to allow the model to \"learn\", that is, adjust its coefficients ('weights') during training. Binary cross-entropy is a default optimization metric for binary classification problems, and there is no obvious reason to deviate. (Implementation: see step \"Define artificial neural network architecture\".)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) _Optimized Parameters_:\n",
    "<br>\n",
    "We will optimize the following parameters. These are also called \"hyper\"parameters to distinguish them from model-specific parameters such as coefficients. To be able to optimite these parameters, we will have to include them in the ANN's architecture (see step \"Define artificial neural network architecture\").\n",
    "\n",
    "| param name | explanation |\n",
    "| --- | --- |\n",
    "| batch_size | Controls how many observations are propagated through the network before coefficients are updated |\n",
    "| deep | Controls whether network is 'shallow' (1 hidden layer) or 'deep' (> 1 hidden layer) |\n",
    "| neurons | Controls number of nodes on network layers |\n",
    "| learning_rate | Controls how strongly coefficients are updated  |\n",
    "| dropout_rate | Controls fraction of layer inputs which will be randomly ignored in updating |\n",
    "| kernel_initializer | Controls the distribution from which the ANN's initial random coefficients are drawn |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) _Optimization procedure_: Staged grid search with class-weighted, k-fold cross-validation.\n",
    "<br>\n",
    "- \"Staged grid search\": Grid search means we define some values for each parameter we want to optimize, and exhaustively search through the resulting parameter \"grid\". For each grid node, a model using this grid node's parameter value combination will be trained on k training subsets (using the optimization metric) and evaluated on k validation subsets of the training data (using the evaluation metric). We run two grid search, optimizing the ANN architecture in the first and the hyperparameters in the second.\n",
    "- \"Class weighted\": To account for class imbalance, weights will be assigned to classes during training to penalize misclassification of the two different classes to different degrees (we unsuccessfully tried SMOTETomek resampling as an alternative).\n",
    "- \"cross-validation\": The purpose of validation sets is to have an indication how well a trained model would generalize to unseen data after deployment (similar to the train/test-split logic described above).\n",
    "- \"k-fold\": The k folds allow to compute the evaluation metric as a mean, and thus make model evaluation more robust against bias resulting from random validation set sampling. (Implementation: see step \"Optimization\".)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define artificial neural network architecture\n",
    "For the \"deep learning\" proof-of-concept requested by Orange, we choose a simple \"feedforward\" (as opposed to, e.g., recurrent or convolutional) neural net architecture. In essence, feedforward here means that the outputs of neurons on one network layer are sent (\"fed forward\") only to neurons of subsequent layers (instead of the same or previous layers). Further, to probe into the potential of \"deep\" versus \"shallow\" learning, we variabilize the number of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adam\n",
    "from keras.constraints import maxnorm\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import GaussianNoise\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import keras.metrics\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def create_model(learning_rate=0.001, dropout_rate=0.0, deep='n', neurons=X_train.shape[1], kernel_initializer='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    if deep == \"y\":\n",
    "        model.add(Dense(round(neurons**(1/1.2), 0), activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(round(neurons**(1/1.5), 0), activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.AUC(name='ROC_AUC'),\n",
    "            \"accuracy\",\n",
    "            keras.metrics.Precision(name=\"precision\"),\n",
    "            keras.metrics.Recall(name=\"recall\")\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=2) #this wrapper allows us to feed the Keras model into Sklearn's GridSearchCV class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "We now turn to the actual model optimization. Hereto, we first compile the optimization procedure we had defined as part of our optimization strategy. (The following code will run only grid search 2 and here use the optimal parameters that have been found during grid search 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "grid_name = \"arch_grid_\"\n",
    "\n",
    "if grid_name == \"arch_grid_\": #we first tune the ANN architecture...\n",
    "    param_grid = dict(\n",
    "        epochs=[5],\n",
    "        batch_size=[128],\n",
    "        deep=['n', 'y'],\n",
    "        neurons=[\n",
    "            round(X_train.shape[1]**(1/1.5), 0),\n",
    "            round(X_train.shape[1]/2, 0),\n",
    "            X_train.shape[1],\n",
    "            round(X_train.shape[1]*2, 0),\n",
    "            round(X_train.shape[1]**(1.5), 0),\n",
    "            round(X_train.shape[1]**(1.6), 0),\n",
    "            round(X_train.shape[1]**(1.7), 0),\n",
    "            round(X_train.shape[1]**(1.8), 0),\n",
    "            round(X_train.shape[1]**(1.9), 0),\n",
    "            round(X_train.shape[1]**(2), 0),\n",
    "            round(X_train.shape[1]**(2.1), 0),\n",
    "            round(X_train.shape[1]**(2.2), 0)]\n",
    "    )\n",
    "elif grid_name == \"hyparam_grid_\": #...and then the hyperparameters\n",
    "    param_grid = dict(\n",
    "        epochs=[5],\n",
    "        batch_size=[64, 128, 256, 512, 1024],\n",
    "        deep=['y'],\n",
    "        neurons=[round(X_train.shape[1]**(2.2), 0)],\n",
    "        learning_rate=[0.0001, 0.001, 0.01],\n",
    "        dropout_rate=[0.0, 0.45, 0.9],\n",
    "        kernel_initializer=['glorot_uniform', 'he_uniform', 'he_normal']\n",
    "    )\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=2) #this wrapper allows us to feed the Keras model into Sklearn's GridSearchCV class\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    verbose=3,\n",
    "    refit='F1',\n",
    "    n_jobs=2,\n",
    "    scoring=scoring,\n",
    "    return_train_score=True,\n",
    "    cv=StratifiedKFold(n_splits=3, shuffle=True),\n",
    ")\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train.values),\n",
    "    y=y_train.values.reshape(-1),\n",
    ")\n",
    "class_weights = dict(zip(np.unique(y_train.values), class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All now left to do in optimization is fit the grid search model to our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection\n",
    "As a basis for model selection, we store the grid search's results to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_result.cv_results_[\"params\"])\n",
    "results[\"means_val_F1\"] = grid_result.cv_results_[\"mean_test_F1\"]\n",
    "results['means_val_ROC_AUC'] = grid_result.cv_results_['mean_test_ROC_AUC']\n",
    "results[\"means_val_Accuracy\"] = grid_result.cv_results_[\"mean_test_Accuracy\"]\n",
    "results[\"means_val_Recall\"] = grid_result.cv_results_[\"mean_test_Recall\"]\n",
    "results[\"means_val_Precision\"] = grid_result.cv_results_[\"mean_test_Precision\"]\n",
    "results[\"means_train_F1\"] = grid_result.cv_results_[\"mean_train_F1\"]\n",
    "results['means_train_ROC_AUC'] = grid_result.cv_results_['mean_train_ROC_AUC']\n",
    "results[\"means_train_Accuracy\"] = grid_result.cv_results_[\"mean_train_Accuracy\"]\n",
    "results[\"means_train_Recall\"] = grid_result.cv_results_[\"mean_train_Recall\"]\n",
    "results[\"means_train_Precision\"] = grid_result.cv_results_[\"mean_train_Precision\"]\n",
    "\n",
    "from datetime import datetime\n",
    "import openpyxl\n",
    "path = \"C:\\\\Users\\\\marc.feldmann\\\\Documents\\\\data_science_local\\\\CustomerChurnPrediction\\\\results\\\\hyparam_opt\\\\\"\n",
    "filename = (path + \"FNN_clf_GSresults_\" + grid_name + datetime.now().strftime(\"%d_%m_%Y__%H_%M_%S\") + \".xlsx\")\n",
    "results.to_excel(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tabular form, the grid search results look like the following. Each row represents one of the models trained during grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.read_excel(\"C:\\\\Users\\\\marc.feldmann\\\\Documents\\\\data_science_local\\\\CustomerChurnPrediction\\\\results\\\\hyparam_opt\\\\FNN_clf_GSresults_param_bundle3_grid_09_08_2022__21_38_02.xlsx\")\n",
    "results.sort_values('means_val_Recall', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(add later here: visualization of some interesting grid search results, including deep learning y/n)\n",
    "nod to: model diagnosis: learning curves (train [how well model learns], val [how well model generalizes to new data]) > show that I can interpret and draw conclusions from that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final model training\n",
    "Wenow train the final model, using the identified optimal parameters (i.e., those that optimize F1 on the validation data). We do this time on the entire training data instead of subsets such as in CV to leverage all available data for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimum params:\n",
    "epochs=5\n",
    "batch_size=1024\n",
    "neurons=round(X_train.shape[1]**(1.9), 0)\n",
    "learning_rate=0.001\n",
    "dropout_rate=0.0\n",
    "kernel_initializer='glorot_uniform'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(neurons, activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(round(neurons**(1/1.2), 0), activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(round(neurons**(1/1.5), 0), activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        keras.metrics.AUC(name='ROC_AUC'),\n",
    "        \"accuracy\",\n",
    "        keras.metrics.Precision(name=\"precision\"),\n",
    "        keras.metrics.Recall(name=\"recall\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=2,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.5. Technical Solution: Final Model Evaluation*\n",
    "Model Evaluation on 'Unseen' Data (simulate by priorly held out 'Test Data')\n",
    "- Do the results make sense?\n",
    "\n",
    "\n",
    "Result interpretation\n",
    "- in write-up: reflect on fact neural networks / deep learning seem to be overhyped\n",
    "- see e.g.: Peter Roßbach: \"Neural Networks vs. Random Forests – Does it always have to be Deep Learning?\n",
    "- - make that explicit point of the write-up! \"test\" that!\n",
    "- show here that I know how to work with learning curves\n",
    "\n",
    "when looking at results, come back to earlier point, explain via clas imbalance\n",
    "come back to earlier point: \n",
    "- make it one main technical point in the article that high accuracy can be misleading (when? why?) - have to also check other measures\n",
    "- - includein write-up my reflections for using precision/recall instead of AUC (argue by importance to detect minority class relative to importance of TPs and FPs) (expl in simple language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['churn'])\n",
    "print(\"ROC-AUC score is {}\".format(sklearn.metrics.roc_auc_score(y_test, y_pred)))\n",
    "\n",
    "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(y_test, y_pred)\n",
    "auc = sklearn.metrics.auc(recall, precision)\n",
    "from matplotlib import pyplot\n",
    "# no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "plt.plot(0.0778, 0.0823, marker=\"o\", markersize=20, markeredgecolor=\"red\", markerfacecolor=\"green\", label='Churn Informed Guessing')\n",
    "# plt.plot([0.4286, 0.4286], [0.375, 0.375], linestyle='--', label='Churn Random Guessing')\n",
    "plt.plot(recall, precision, marker='.', label='ANN Churn Predictor')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.6. Technical Solution: Future Optimization Potentials*\n",
    "(hier sammeln alles ich zeitlich nicht geschafft hab, aber für wichtig halte - um Kritik zu preempten)\n",
    "\n",
    "Schema: Potential - Umsetzungsaufwand - erwarteter Umsetzungseffekt auf Business Metric\n",
    "\n",
    "- version 2: optimization potentials (versus v1) to explore ceteris paribus:\n",
    "- perform infrequent category replacement for test and train sets separately \n",
    "- not explored/limitations: only individually optimized, due to constraints in processing power and time, optimization dependencies between variables neglected\n",
    "- only narrow ranges in gridsearch covered, so sound change that only found local optima per parameter\n",
    "- potential: NaN imputation with means on subsets of rows: one could search powerful clustering criteria first and than impute cluster means\n",
    "- also: was using smaller dataset, large dataset with many more variables may allow to increase a classifier's precision/recall\n",
    "- Make sure to also compare to others' results - I seem to be already working at the upper boundary of what's possible on this dataset with ANNs!\n",
    "- optimization potential: in practice, one would normally traing many different models and select/stack the best; show somehow that I'm aware of that\n",
    "- (optimization potential: add and compare AUC: simple logistic regression, random forest, 'flat' neural network, XGBoost)\n",
    "- optimization potential: put data into an AWS instance and run there\n",
    "- multicollinearity - check whether an issue - we want to have model as simple as possible! will decrease risk of overfitting that ANNs are especially prone to\n",
    "- outliers: Scaler Min Max or Robust made no big difference (see model_comparision Excel), suggesting it is not problematic that we have not removed outliers in data preprocessing; still might contain some potential to increase model performance\n",
    "- feature selection: have touched (selectKBest), but not exhausted feature selection \n",
    "- feature engineering: dimensionality reduction to reduce dimensionality, create new and more 'powerful' features; kurz auf curse of dimensionality eingehen und auf ANNs overviffting tendency; feature selection would have benefit to be explanable however, features anonymized anyways\n",
    "- optimization: \"Optimization of only thought about in terms of tuning hyperparamters; but also preprocessing includes many steps that can be done in different ways - meaning also has potential to optimize: read following in conjunction with \"diary\" to see what I have optimzied here and with which success: <br>\n",
    "- code cleaning: to increase code reusability and readability, repetitive parts could be wrapped into functions and moved into separate script (e.g., preprocessing steps applied both to training and test data)\n",
    "- clarify optimization approach: first optimized preprocessing (experiemented eg. with X instead of Y) - resulted in above described procedure; now: hyperparameter tuning:\n",
    "- optimize network architecture, e.g. LSTM, see \"neural network zoo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Business Recommendations**\n",
    "\"What do the generated insights/model urge us/allow us to do different next Monday, and which value (business metric!) will that generate?\"\n",
    "\n",
    "direkt aus auftrag (1.) ableiten. incl.:\n",
    "- (after implementing comparative models:) \"turns out, deep learning (might) not be best for this kind of problem; best practice computer vision, very large datasets; here: tree model such as XGboost or simple logistic regression better \n",
    "- based on a feature importance chart for final ANN, identify potential churn drivers: discuss how can be made visible and influenced by which staff groups XY (account managers? service managers?); measure, enable and encourage these staff groups to act on identified drivers  \n",
    "\n",
    "Good Example: https://www.kaggle.com/code/hamzaben/employee-churn-model-w-strategic-retention-plan/notebook"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e858973d4b0767378b2a391e82dfc297a0e494a6ac1816312417e02d17e8ebb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
