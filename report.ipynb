{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Keep in mind while writing up:*\n",
    "\n",
    "- *Be concise! Less is more - the fully story is in the source code for those interested.*\n",
    "- *Be deliberate about: What to highlight in which section (e.g., “this dataset was special due to its high number of variables”…)*\n",
    "- *Work with visuals and only exceptionally with code. Refer to GitHub, dump code there, the technical people will go there. And (hiring) managers will only read the write-up.*\n",
    "- *Optimize business value, not model performance! Time/Resource constraints, ….*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Report\n",
    "# **Preventing Customer Churn with Feedforward Neural Networks**\n",
    "*Disclaimer: This mock project report serves educational purposes only. The company data is public (https://www.kdd.org/kdd-cup/view/kdd-cup-2009/Data). The author has no commercial relationship with mentioned parties.* \n",
    "***\n",
    "### **Executive Summary (max. 7 sentences)**\n",
    "Situation (1 sentence based on 1.)\n",
    "<br>\n",
    "Complication (1 sentence based on 1.)\n",
    "<br>\n",
    "Solution (1 sentence based on 2.)\n",
    "<br>\n",
    "Recommendations including Solutions' Business Value Add (1-3 sentences based on 3.)\n",
    "\"much buzz around ANN, let's test that here\"\n",
    "***\n",
    "### **Report Structure**\n",
    "[Include nice + simple process visualization!]\n",
    "1. Business Problem Statement\n",
    "2. Technical Solution\n",
    "<br>    *2.1. Technical Problem Statement*\n",
    "<br>    *2.2. Exploratory Data Analysis*\n",
    "<br>    *2.3. Data Preprocessing*\n",
    "<br>    *2.4. Feature Selection*\n",
    "<br>    *2.5. Model Selection (incl. Optimization)*\n",
    "<br>    *2.6. Final Model Evaluation*\n",
    "<br>    *2.7. Future Optimization Potentials*\n",
    "3. Business Recommendations\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Business Problem Statement**\n",
    "For firms like French telecommunication provider Orange, customer retention is critical. This is because retaining customers is much cheaper than the alternative: losing a customer and their revenues plus replacement costs. However, Orange lacks an automated, scalable, and data-driven method for predicting customer churn that would allow Orange to initiate retention measures before customers leave. That is, predicting customer churn currently more or less relies on sporadic guesses. Thus, Orange requested a proof-of-concept for a predictive model that can help identify customers who will likely churn so that retention measures can be initiated. Specifically, encouraged by the enthusiasm surrounding \"deep learning\", Orange wants the proof-of-concept to explore the potential of this model class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Technical Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.1. Technical Solution: Technical Problem Statement*\n",
    "The business problem, as put by Orange, is \"to predict customer churn\". This problem requires translation into a better specified, technical problem before it is solvable using mathematical-statistical methods. Technically put, the problem we solve is to\n",
    "\n",
    "*maximize the F1-score over the churn predictions of Orange's customers by implementing an artificial neural network with more than one hidden layer and an output layer containing a single neuron with an activation function*.\n",
    "\n",
    "Each component of this technical problem statement follows from considering the following three issues in light of the business problem we solve: \n",
    "\n",
    "#### Specifying the business problem\n",
    "It is first important to understand that predicting customer churn is, technically, a binary classification problem: given the data available for any particular customer (e.g., age, gender, purchased services, average call duration), we want our model to assign this customer to one of the two classes \"churn\"/\"no churn\". Understanding that we solve a classification problem has important implications for two main elements of the technical problem statement:\n",
    "\n",
    "#### Choosing an adequate model class\n",
    "In a typical data science project, we would train models from many different model classes (e.g., logistic regression classifiers, trees, support vector machines) and select the best performing models (or combine them in an *ensemble*) for deployment. In this project, however, the client Orange has specified upfront that they want a \"deep learning\" model, which in more precise technical terms is widely understood as an artificial neural network (ANN) with more than one hidden layer. Further, since we want the ANN's output to always be either \"churn\" or \"no churn\", its output layer must contain a single neuron with an activation function (e.g., ReLU, sigmoid) that translates continuous into binary values (1/0).\n",
    "\n",
    "#### Choosing adequate evaluation metrics\n",
    "An evaluation metric enables us to assess how \"good\" a developed model is and optimize it. The perhaps most intuitive metric for a classification model is the *accuracy* of its predictions. Accuracy tells us in which percentage of cases a classification model's predictions (\"churn\"/\"no churn\") are true (that is, correctly predict what customers will actually do). However, we can infer from the business context that the classes \"churn\"/\"no churn\" we are interested in are *imbalanced*: only a minority of all customers will churn in any given time period. We can thus expect many more customers to be in the \"no churn\" rather than the \"churn\" class. Accuracy will thus be a bad metric to optimize: the model could 'cheat' and simply predict \"no churn\" in 100% of the cases, and never detect a single churning customer, and still have awesome accuracy. In presence of class imbalance, a metric more adequate to optimize is the *F1-score*. A high F1-score indicates not only that the model is able to detect many of those customers who will indeed churn (high *recall*), but also that the model's \"churn\"-predictions are typically correct (high *precision*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following resources available to solve the problem:\n",
    "- Data: Orange has provided historical customer data (50,000 observations/customers; 230 features).\n",
    "- Software: Python 3.8.5., main packages:\n",
    "    - Pandas, Numpy (for data wrangling)\n",
    "    - Keras/TensorFlow (for neural network modelling)\n",
    "    - Scikit-learn (for feature selection and optimization/gridsearch automation)\n",
    "    - Matplotlib, Seaborn (for visualization)\n",
    "- Hardware: a standard office notebook with an i7-8550U (4 cores @1.80 GHz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.2. Technical Solution: Exploratory Data Analysis (EDA)*\n",
    "Now that we have specified the technical problem this project should solve, we first familiarize ourselves with the historical customer data Orange has provided. Exploratory data analysis helps us identify how we need to preprocess this data so that the ANN can better learn from it to predict churn. This typically involves some basic overall checks (overall dataset structure, feature types, missing values), but also analyses focused on our target variable, that is, the class label vector \"churn\"/\"no churn\" (= what we want to predict)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data\n",
    "We first load the data from a local drive. X is a matrix containing features and observations, y is a vector containing the class labels we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### some settings to increase reproducibility and report readability \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "seed(3992)\n",
    "tf.random.set_seed(3992)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"float_format\", \"{:f}\".format)\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "### Loading the data\n",
    "X = pd.read_table('data/orange_small_train.data')\n",
    "y = pd.read_table('data/orange_small_train_churn.labels', header = None,sep='\\t').loc[:, 0].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the overall dataset structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our very first analytical step is to take a broad look at the overall dataset structure, including the number of features (columns) and observations (rows = customers), feature names, features' data types, categorical features' cardinality, missing value formatting, and some basic descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cat = list(X.select_dtypes(include=['object']).columns)\n",
    "temp = X[features_cat]\n",
    "temp = temp.nunique().sort_values(ascending=False).reset_index()\n",
    "temp.columns = ['cat_feature', 'cardinality']\n",
    "import seaborn as sns\n",
    "ax = sns.barplot(x='cat_feature', y='cardinality', data=temp)\n",
    "print(\"Categorical features' mean cardinality:\", round(temp['cardinality'].sum()/temp.shape[0],0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we noted that the data contains missing values (\"NaN\"), we want to know precisely which percentage of values is missing in the data we have been given, and also how these missing values are distributed across features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(X.isna().sum().sum()/(X.shape[0]*X.shape[1]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "temp = X.isna().sum()/(X.shape[0])\n",
    "plt.bar(range(len(temp)), sorted(temp), color='blue', alpha=0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking class balance \"churn\"/\"no churn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having acquired an overall impression of the data, we now take a more focused look on our target variable 'Churn', that is, the column containing the class labels our model will need to predict.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts().plot.bar()\n",
    "plt.ylabel('value')\n",
    "plt.title('churn value for each class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### > EDA key insights:\n",
    "To summarize, our brief exploratory analysis helped us gain some important insights about the features, observations, and target variable in our data. Going forward, we will need to keep these insights at the back of our mind as they will instruct us how to properly preprocess the data so that our predictive model can learn well from it. Key insight we have gleaned are:\n",
    "\n",
    "- Features: \n",
    "\t- Most or all features' scales differ.\n",
    "\t- Orange has anonymized data before providing it (likely to protect customers' privacy).\n",
    "\t- The 230 total features include 38 categorical and 192 numerical features.\n",
    "\t- High cardinality is a big issue in this dataset (on average, 1882 unique values per categorical feature)\n",
    "- Observations:\n",
    "\t- We have been given data on 50.000 customers and their churn behaviour that we can use for optimizing and evaluating our model.\n",
    "\t- Missing values are another big issue in this dataset (around 70%).\n",
    "- Target variable:\n",
    "\t- As expected, the two classes in our target variable 'Churn' are heavily imbalanced (around 1 churning customer for 12 non-churning customers).\n",
    "\t- The class labels \"churn\"/\"no churn\" are represented by the numerical values 1/-1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.3. Technical Solution: Data Preprocessing*\n",
    "Before the data can be used for effective model training, it needs to be preprocessed. This is because the ANN model we implement requires data to be in a particular format to effectively learn from the data how to make good predictions. In addition, since preprocessing steps often have interdependencies, being deliberate about their order is important. Otherwise we risk messing up the data used for model training, compromising prediction quality. \n",
    "\n",
    "Main preprocessing steps that the key insights gleaned from EDA suggest are the following:\n",
    "- remove observations and features with all values missing\n",
    "- replace infrequent categories with a single 'catch-all' category\n",
    "- create binary indicator columns for missing values\n",
    "- impute missing values\n",
    "- encode (= 'make numerical') categorical features\n",
    "- normalize the features' scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove observations and features with all values missing\n",
    "We first remove any potential observations and features which might not hold any values. These will only push computation times and risk complicating our model without adding anything to its explanatory power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dropna(axis=0, how='all', inplace=True)\n",
    "X.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see comparing to above, no observations, but 18 features have been removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace infrequent categories with a single catch-all category\n",
    "EDA has shown that a big issue in the data is the high mean cardinality of categorical features. If there are many different categories, it will be hard for our model to recognize patterns that the model can exploit for predicting customer churn. Thus, to help our model more easily infer useful explanatory patterns, we replace all categories whose frequency of occurrence lies below a cetrain treshold with a single category \"RARE_VALUE\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cat = list(X.select_dtypes(include=['object']).columns) # update this since we have dropped some features\n",
    "for feat in features_cat:\n",
    "    X.loc[X[feat].value_counts(dropna=False)[X[feat]].values < X.shape[0] * 0.02, feat] = 'RARE_VALUE' # replace all categories which occurr in less than 2% of observatiosn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated by the following fraction of cells in which categories have been replaced, this preprocessing step will have a significant impact on our later model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X == 'RARE_VALUE'].count().sum()/(X.shape[0]*X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting aside some test data\n",
    "Before we proceed with the next preprocessing steps, we put aside some test data. The idea behing putting aside test data is essentially to pretend that we would possess some of the new/unseen data that will actually be only coming in after our model will have been deployed (= in actual use, or 'production'). This trick allows us to get an idea about how good our model would perform 'out in the wild'.\n",
    "<br>\n",
    "<br>\n",
    "However, the need to put aside test data also introduces additional complexity into preprocessing: For preprocessing steps whose parameters depend on the data they are applied to (e.g., scaling, encoding), we have to act like we would not have the test data available. This means we need to first preprocess the training data, and then the test data separately in the exact same way. Not doing this and having the test data still 'in' during data-dependent preprocessing steps would allow our model to learn from information that the model could, conceptually speaking, impossibly have access to. This is called 'peeking' or 'data leakage'. Data leakage risks leading our evaluation metrics to tell us that we have built a good model, but then after deployment find out that we have actually built a bad model - that is, when time and money have already been wasted.\n",
    "<br>\n",
    "<br>\n",
    "There is yet another issue we need to account for when putting aside test data: the class imbalance in our data that our EDA has confirmed. Usually, we would simply randomly split test and training data. However, since we need the training and test data to reflect the dataset's class distribution (\"churn\"/\"no churn\"), we perform a *stratified* split, that is, a split replicating the dataset's class distribution to both the created training and test subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=3992)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding missing value indicator features\n",
    "We now return once more to the issue of missing values. Since the number of missing values in the data is so high, we want to think some more about how we might exploit that. While a missing value represents an absence of information, the very fact per se that a value is missing might indicate something (e.g., a customer not booking a particular service) and possess predictive power. Thus, for each feature with missing values, we create an additional feature which indicates via the values 1 or 0 for each observation the presence or absence of a missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for clm in X_train:\n",
    "    if X_train[clm].isna().sum() > 0:\n",
    "        X_train.insert(X_train.shape[1], f\"{clm}_NaNInd\", 0)\n",
    "        X_train[f\"{clm}_NaNInd\"] = np.where(pd.isnull(X_train[clm]), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, 185 binary variables have beend added to our training data, resulting in 397 variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing missing values\n",
    "To add at least some meaningful information to the data where values are missing, we impute missing values in numerical features with column-wise means, and missing values in categorical features with the category 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "features_num_train = list(X_train.select_dtypes(include=['float']).columns)\n",
    "imputer_nums = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_train[features_num_train] = imputer_nums.fit_transform(X_train[features_num_train])\n",
    "\n",
    "features_cat_train = list(X_train.select_dtypes(include=['object']).columns)\n",
    "imputer_cats = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='unknown')\n",
    "X_train[features_cat_train] = imputer_cats.fit_transform(X_train[features_cat_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our data now does not contain any missing values anymore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(X_train.isna().sum().sum()/(X_train.shape[0]*X.shape[1]), 3) #returns the percentage of missing values in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding categorical features\n",
    "We have also learned during EDA that there are quite some categorical features. Our ANN model (like many other model classes), however, can only handle numerical inputs. We thus *one-hot encode* (OHE) the categorical features. This means that for each unique value in each categorical feature, we create an additional feature which indicates via the values 1 or 0 for each observation the presence or absence of that unqiue value. We drop the original features and only keep the one-hot encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "enc = make_column_transformer((OneHotEncoder(max_categories=20, handle_unknown='ignore'), features_cat_train), remainder='passthrough')\n",
    "transformed = enc.fit_transform(X_train)\n",
    "enc_df = pd.DataFrame(transformed, columns=enc.get_feature_names())\n",
    "cols = enc_df.columns.tolist()\n",
    "cols = cols[168:] + cols[:168]\n",
    "X_train = enc_df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, OHE has added 130 dummy variables to our training data, resulting in 527 variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rescale features\n",
    "Finally, we rescale the features. We have seen in EDA that the features were measured using different scales. To rule out that this could affect our model, we remove from all features (except the indicator columns we added) the dataset mean and scale the feature to unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train[features_num_train] = scaler.fit_transform(X_train[features_num_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that scaling has happened properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming class labels\n",
    "We finally bring the class labels into a more suitable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.replace(-1, 0, inplace=True)\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set preprocessing\n",
    "We now apply the same preprocessing steps, in the same order, with the same parameters, to the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert NaN indicator columns\n",
    "for clm in X_test:\n",
    "    if X_test[clm].isna().sum() > 0:\n",
    "        X_test.insert(X_test.shape[1], f\"{clm}_NaNInd\", 0)\n",
    "        X_test[f\"{clm}_NaNInd\"] = np.where(pd.isnull(X_test[clm]), 1, 0)\n",
    "\n",
    "# missing value imputation: apply imputers that have been fit to training data\n",
    "X_test[features_num_train] = imputer_nums.transform(X_test[features_num_train])\n",
    "X_test[features_cat_train] = imputer_cats.transform(X_test[features_cat_train])\n",
    "X_test.isna().sum().sum()\n",
    "\n",
    "# Encode categorical features\n",
    "transformed = enc.transform(X_test)\n",
    "enc_df = pd.DataFrame(transformed, columns=enc.get_feature_names())\n",
    "cols = enc_df.columns.tolist()\n",
    "cols = cols[168:] + cols[:168]\n",
    "X_test = enc_df[cols]\n",
    "\n",
    "# apply scaler that has been fit to training data\n",
    "X_test[features_num_train] = scaler.transform(X_test[features_num_train])\n",
    "\n",
    "# transform target variable\n",
    "y_test.replace(-1, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We complete preprocessing by ensuring that the preprocessed training and test datasets have the same 527 features, in the same order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of identical columns:', sum(X_train.columns == X_test.columns)/len(X_train.columns)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### *2.5. Technical Solution: Feature Selection*\n",
    "\n",
    "While we have now preprocessed our data, the number of features is very high. In other words, our data is high-dimensional. This is undesirable for a variety of reasons, among the most important ones computational overhead and increased overfitting risk (see *curse of dimensionality*). Generally speaking, we want predictive models to use all the features with the most predictive power, but also as few features as possible. Thus, before we move on to building our ANN model, we train a random forest classifier model from which we can extract those features which have helped the forest model to best split observations into churning and non-churning customers.\n",
    "<br>\n",
    "<br>\n",
    "We first split our training data into a training and a cross-validation dataset for feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fs, X_cv_fs, y_train_fs, y_cv_fs = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2, random_state=3992)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that we do not select features based on an extremely bad random forest model, we optimize the model a little. Specifically, we grow multiple forest models with different numbers of trees, and with different maximum depths: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_classifier = RandomForestClassifier()\n",
    "parameters = {'max_depth':np.arange(3,10),'n_estimators':list(range(25,251,25))}\n",
    "random_grid = GridSearchCV(random_classifier, parameters, cv=3)\n",
    "random_grid.fit(X_cv_fs, y_cv_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing all forest models that we have grown, it turns out that the following parameters create the best random forest classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal Forest Hyperparams:\", random_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile a random forest classifier with the identified optimum parameters and train it on the training data for feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=25,\n",
    "    max_depth=3,\n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None,\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced',\n",
    "    random_state=3992,\n",
    "    verbose=0,\n",
    "    warm_start=False)\n",
    "\n",
    "rf_model.fit(X=X_train_fs, y=y_train_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract and visualize from this optimized forest model the features that most helped the model tell apart churning and non-churning customers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(rf_model.feature_importances_, index = X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "# feature importance plot\n",
    "std = np.std([rf_model.feature_importances_[:101] for rf_model in rf_model.estimators_], axis=0)\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "feature_importances[:101].plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean Decrease in Impurity (MDI)\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, only a very small number of the more than 500 features is actually helpful for predicting customer churn. Using only these features will save A LOT of computation time during ANN training, and also greatly reduce ANN complexity, without losing prediction quality. More simply put, feature selection has helped us make better predictions MUCH less costly. We thus throw all features but the most important ones from the training and test data that we use for ANN training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_imp_feats = feature_importances[:26].index.to_list()\n",
    "X_train = X_train[most_imp_feats]\n",
    "X_test = X_test[most_imp_feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### *2.4. Technical Solution: Model Selection (incl. Optimization)*\n",
    "\n",
    "Now that we have preprocessed our data and identified the most important features, we use the data for building and optimizing an artificial neural network classification model. Here, we will benefit from having spelled out earlier the technical problem we solve (*maximize the F1-score over the churn predictions of Orange's customers by implementing an artificial neural network with more than one hidden layer and an output layer containing a single neuron with an activation function*). This statement will guide us in the following steps: defining an optimization strategy, defining the ANN's architecture, optimization, and final model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model optimization strategy\n",
    "The reason for defining a model optimization strategy is simple: resources are limited, but the number of possible models is infinite (literally). This is due to the typically high number of optimizable parameters, and the infinite ranges of possible values for several of these parameters. This implies that simply jumping into optimization will likely make one end up endlessly tune everything and anything. Generally speaking, while model optimization strategies should be tailored to data science projects case-by-case, formulating them always requires a thorough understanding of the project's stakeholder expectations, the problem to be solved, the model classes used, and the resources available.\n",
    "<br>\n",
    "<br>\n",
    "For the prediction of churn among Orange's customers, we will focus our optimization strategy on four basic elements:\n",
    "- (1) the evaluation metric\n",
    "- (2) the optimization metric\n",
    "- (3) the optimized hyperparameters, and\n",
    "- (4) the optimization procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) _Evaluation metric_: F1-score.\n",
    "<br>\n",
    "We use this metric to identify which model among all trained models best solves our problem. We have explained our choice of the F1-score in 2.1. We also pull some further metrics from Keras' metrics library to enable a more comprehensive assessment of the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "scoring = {\n",
    "    \"F1\": sklearn.metrics.make_scorer(sklearn.metrics.f1_score),\n",
    "    'ROC_AUC': sklearn.metrics.make_scorer(sklearn.metrics.roc_auc_score),\n",
    "    \"Accuracy\": sklearn.metrics.make_scorer(sklearn.metrics.accuracy_score),\n",
    "    \"Recall\": sklearn.metrics.make_scorer(sklearn.metrics.recall_score),\n",
    "    \"Precision\": sklearn.metrics.make_scorer(sklearn.metrics.precision_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) _Optimization metric_: Binary cross-entropy loss.\n",
    "<br>\n",
    "We use this metric to allow the model to \"learn\", that is, adjust its coefficients ('weights') during training. Binary cross-entropy is a default optimization metric for binary classification problems, and there is no obvious reason to deviate. (Implementation: see step \"Define artificial neural network architecture\".)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) _Optimized Parameters_:\n",
    "<br>\n",
    "We will optimize the following parameters. These are also called \"hyper\"parameters to distinguish them from model-specific parameters such as coefficients. To be able to optimite these parameters, we will have to include them in the ANN's architecture (see step \"Define artificial neural network architecture\").\n",
    "\n",
    "| param name | explanation |\n",
    "| --- | --- |\n",
    "| batch_size | Controls how many observations are propagated through the network before coefficients are updated |\n",
    "| deep | Controls whether network is 'shallow' (1 hidden layer) or 'deep' (> 1 hidden layer) |\n",
    "| neurons | Controls number of nodes on network layers |\n",
    "| learning_rate | Controls how strongly coefficients are updated  |\n",
    "| dropout_rate | Controls fraction of layer inputs which will be randomly ignored in updating |\n",
    "| kernel_initializer | Controls the distribution from which the ANN's initial random coefficients are drawn |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) _Optimization procedure_: Staged grid search with class-weighted, k-fold cross-validation.\n",
    "<br>\n",
    "- \"Staged grid search\": Grid search means we define some values for each parameter we want to optimize, and exhaustively search through the resulting parameter \"grid\". For each grid node, a model using this grid node's parameter value combination will be trained on k training subsets (using the optimization metric) and evaluated on k validation subsets of the training data (using the evaluation metric). We run two grid search, optimizing the ANN architecture in the first and the hyperparameters in the second.\n",
    "- \"Class weighted\": To account for class imbalance, weights will be assigned to classes during training to penalize misclassification of the two different classes to different degrees (we unsuccessfully tried SMOTETomek resampling as an alternative).\n",
    "- \"cross-validation\": The purpose of validation sets is to have an indication how well a trained model would generalize to unseen data after deployment (similar to the train/test-split logic described above).\n",
    "- \"k-fold\": The k folds allow to compute the evaluation metric as a mean, and thus make model evaluation more robust against bias resulting from random validation set sampling. (Implementation: see step \"Optimization\".)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define artificial neural network architecture\n",
    "For the \"deep learning\" proof-of-concept requested by Orange, we choose a simple \"feedforward\" (as opposed to, e.g., recurrent or convolutional) neural net architecture. In essence, feedforward here means that the outputs of neurons on one network layer are sent (\"fed forward\") only to neurons of subsequent layers (instead of the same or previous layers). Further, to probe into the potential of \"deep\" versus \"shallow\" learning, we variabilize the number of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adam\n",
    "from keras.constraints import maxnorm\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import GaussianNoise\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import keras.metrics\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def create_model(learning_rate=0.001, dropout_rate=0.0, deep='n', neurons=X_train.shape[1], kernel_initializer='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    if deep == \"y\":\n",
    "        model.add(Dense(round(neurons**(1/1.2), 0), activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(round(neurons**(1/1.5), 0), activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.AUC(name='ROC_AUC'),\n",
    "            \"accuracy\",\n",
    "            keras.metrics.Precision(name=\"precision\"),\n",
    "            keras.metrics.Recall(name=\"recall\")\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=2) #this wrapper allows us to feed the Keras model into Sklearn's GridSearchCV class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "We now turn to the actual model optimization. Hereto, we first compile the optimization procedure we had defined as part of our optimization strategy. (The following code will run only grid search 2 and here use the optimal parameters that have been found during grid search 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "grid_name = \"arch_grid_\"\n",
    "\n",
    "if grid_name == \"arch_grid_\": #we first tune the ANN architecture...\n",
    "    param_grid = dict(\n",
    "        epochs=[5],\n",
    "        batch_size=[128],\n",
    "        deep=['n', 'y'],\n",
    "        neurons=[\n",
    "            round(X_train.shape[1]**(1/1.5), 0),\n",
    "            round(X_train.shape[1]/2, 0),\n",
    "            X_train.shape[1],\n",
    "            round(X_train.shape[1]*2, 0),\n",
    "            round(X_train.shape[1]**(1.5), 0),\n",
    "            round(X_train.shape[1]**(1.6), 0),\n",
    "            round(X_train.shape[1]**(1.7), 0),\n",
    "            round(X_train.shape[1]**(1.8), 0),\n",
    "            round(X_train.shape[1]**(1.9), 0),\n",
    "            round(X_train.shape[1]**(2), 0),\n",
    "            round(X_train.shape[1]**(2.1), 0),\n",
    "            round(X_train.shape[1]**(2.2), 0)]\n",
    "    )\n",
    "elif grid_name == \"hyparam_grid_\": #...and then the hyperparameters\n",
    "    param_grid = dict(\n",
    "        epochs=[5],\n",
    "        batch_size=[64, 128, 256, 512, 1024],\n",
    "        deep=['y'],\n",
    "        neurons=[round(X_train.shape[1]**(2.2), 0)],\n",
    "        learning_rate=[0.0001, 0.001, 0.01],\n",
    "        dropout_rate=[0.0, 0.45, 0.9],\n",
    "        kernel_initializer=['glorot_uniform', 'he_uniform', 'he_normal']\n",
    "    )\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=2) #this wrapper allows us to feed the Keras model into Sklearn's GridSearchCV class\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    verbose=3,\n",
    "    refit='F1',\n",
    "    n_jobs=2,\n",
    "    scoring=scoring,\n",
    "    return_train_score=True,\n",
    "    cv=StratifiedKFold(n_splits=3, shuffle=True),\n",
    ")\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train.values),\n",
    "    y=y_train.values.reshape(-1),\n",
    ")\n",
    "class_weights = dict(zip(np.unique(y_train.values), class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All now left to do in optimization is fit the grid search model to our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection\n",
    "As a basis for model selection, we store the grid search's results to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_result.cv_results_[\"params\"])\n",
    "results[\"means_val_F1\"] = grid_result.cv_results_[\"mean_test_F1\"]\n",
    "results['means_val_ROC_AUC'] = grid_result.cv_results_['mean_test_ROC_AUC']\n",
    "results[\"means_val_Accuracy\"] = grid_result.cv_results_[\"mean_test_Accuracy\"]\n",
    "results[\"means_val_Recall\"] = grid_result.cv_results_[\"mean_test_Recall\"]\n",
    "results[\"means_val_Precision\"] = grid_result.cv_results_[\"mean_test_Precision\"]\n",
    "results[\"means_train_F1\"] = grid_result.cv_results_[\"mean_train_F1\"]\n",
    "results['means_train_ROC_AUC'] = grid_result.cv_results_['mean_train_ROC_AUC']\n",
    "results[\"means_train_Accuracy\"] = grid_result.cv_results_[\"mean_train_Accuracy\"]\n",
    "results[\"means_train_Recall\"] = grid_result.cv_results_[\"mean_train_Recall\"]\n",
    "results[\"means_train_Precision\"] = grid_result.cv_results_[\"mean_train_Precision\"]\n",
    "\n",
    "from datetime import datetime\n",
    "import openpyxl\n",
    "path = \"C:\\\\Users\\\\marc.feldmann\\\\Documents\\\\data_science_local\\\\CustomerChurnPrediction\\\\results\\\\hyparam_opt\\\\\"\n",
    "filename = (path + \"FNN_clf_GSresults_\" + grid_name + datetime.now().strftime(\"%d_%m_%Y__%H_%M_%S\") + \".xlsx\")\n",
    "results.to_excel(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tabular form, the grid search results look like the following. Each row represents one of the models trained during grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.read_excel(\"C:\\\\Users\\\\marc.feldmann\\\\Documents\\\\data_science_local\\\\CustomerChurnPrediction\\\\results\\\\hyparam_opt\\\\FNN_clf_GSresults_param_bundle3_grid_09_08_2022__21_38_02.xlsx\")\n",
    "results.sort_values('means_val_Recall', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(add later here: visualization of some interesting grid search results, including deep learning y/n)\n",
    "nod to: model diagnosis: learning curves (train [how well model learns], val [how well model generalizes to new data]) > show that I can interpret and draw conclusions from that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final model training\n",
    "Wenow train the final model, using the identified optimal parameters (i.e., those that optimize F1 on the validation data). We do this time on the entire training data instead of subsets such as in CV to leverage all available data for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimum params:\n",
    "epochs=5\n",
    "batch_size=1024\n",
    "neurons=round(X_train.shape[1]**(1.9), 0)\n",
    "learning_rate=0.001\n",
    "dropout_rate=0.0\n",
    "kernel_initializer='glorot_uniform'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(neurons, activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(round(neurons**(1/1.2), 0), activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(round(neurons**(1/1.5), 0), activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        keras.metrics.AUC(name='ROC_AUC'),\n",
    "        \"accuracy\",\n",
    "        keras.metrics.Precision(name=\"precision\"),\n",
    "        keras.metrics.Recall(name=\"recall\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=2,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.5. Technical Solution: Final Model Evaluation*\n",
    "Model Evaluation on 'Unseen' Data (simulate by priorly held out 'Test Data')\n",
    "- Do the results make sense?\n",
    "\n",
    "\n",
    "Result interpretation\n",
    "- in write-up: reflect on fact neural networks / deep learning seem to be overhyped\n",
    "- see e.g.: Peter Roßbach: \"Neural Networks vs. Random Forests – Does it always have to be Deep Learning?\n",
    "- - make that explicit point of the write-up! \"test\" that!\n",
    "- show here that I know how to work with learning curves\n",
    "\n",
    "when looking at results, come back to earlier point, explain via clas imbalance\n",
    "come back to earlier point: \n",
    "- make it one main technical point in the article that high accuracy can be misleading (when? why?) - have to also check other measures\n",
    "- - includein write-up my reflections for using precision/recall instead of AUC (argue by importance to detect minority class relative to importance of TPs and FPs) (expl in simple language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['churn'])\n",
    "print(\"ROC-AUC score is {}\".format(sklearn.metrics.roc_auc_score(y_test, y_pred)))\n",
    "\n",
    "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(y_test, y_pred)\n",
    "auc = sklearn.metrics.auc(recall, precision)\n",
    "from matplotlib import pyplot\n",
    "# no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "plt.plot(0.0778, 0.0823, marker=\"o\", markersize=20, markeredgecolor=\"red\", markerfacecolor=\"green\", label='Churn Informed Guessing')\n",
    "# plt.plot([0.4286, 0.4286], [0.375, 0.375], linestyle='--', label='Churn Random Guessing')\n",
    "plt.plot(recall, precision, marker='.', label='ANN Churn Predictor')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.6. Technical Solution: Future Optimization Potentials*\n",
    "(hier sammeln alles ich zeitlich nicht geschafft hab, aber für wichtig halte - um Kritik zu preempten)\n",
    "\n",
    "Schema: Potential - Umsetzungsaufwand - erwarteter Umsetzungseffekt auf Business Metric\n",
    "\n",
    "- version 2: optimization potentials (versus v1) to explore ceteris paribus:\n",
    "- perform infrequent category replacement for test and train sets separately \n",
    "- not explored/limitations: only individually optimized, due to constraints in processing power and time, optimization dependencies between variables neglected\n",
    "- only narrow ranges in gridsearch covered, so sound change that only found local optima per parameter\n",
    "- potential: NaN imputation with means on subsets of rows: one could search powerful clustering criteria first and than impute cluster means\n",
    "- also: was using smaller dataset, large dataset with many more variables may allow to increase a classifier's precision/recall\n",
    "- Make sure to also compare to others' results - I seem to be already working at the upper boundary of what's possible on this dataset with ANNs!\n",
    "- optimization potential: in practice, one would normally traing many different models and select/stack the best; show somehow that I'm aware of that\n",
    "- (optimization potential: add and compare AUC: simple logistic regression, random forest, 'flat' neural network, XGBoost)\n",
    "- optimization potential: put data into an AWS instance and run there\n",
    "- multicollinearity - check whether an issue - we want to have model as simple as possible! will decrease risk of overfitting that ANNs are especially prone to\n",
    "- outliers: Scaler Min Max or Robust made no big difference (see model_comparision Excel), suggesting it is not problematic that we have not removed outliers in data preprocessing; still might contain some potential to increase model performance\n",
    "- feature selection: have touched (selectKBest), but not exhausted feature selection \n",
    "- feature engineering: dimensionality reduction to reduce dimensionality, create new and more 'powerful' features; kurz auf curse of dimensionality eingehen und auf ANNs overviffting tendency; feature selection would have benefit to be explanable however, features anonymized anyways\n",
    "- optimization: \"Optimization of only thought about in terms of tuning hyperparamters; but also preprocessing includes many steps that can be done in different ways - meaning also has potential to optimize: read following in conjunction with \"diary\" to see what I have optimzied here and with which success: <br>\n",
    "- code cleaning: to increase code reusability and readability, repetitive parts could be wrapped into functions and moved into separate script (e.g., preprocessing steps applied both to training and test data)\n",
    "- clarify optimization approach: first optimized preprocessing (experiemented eg. with X instead of Y) - resulted in above described procedure; now: hyperparameter tuning:\n",
    "- optimize network architecture, e.g. LSTM, see \"neural network zoo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Business Recommendations**\n",
    "\"What do the generated insights/model urge us/allow us to do different next Monday, and which value (business metric!) will that generate?\"\n",
    "\n",
    "direkt aus auftrag (1.) ableiten. incl.:\n",
    "- (after implementing comparative models:) \"turns out, deep learning (might) not be best for this kind of problem; best practice computer vision, very large datasets; here: tree model such as XGboost or simple logistic regression better \n",
    "- based on a feature importance chart for final ANN, identify potential churn drivers: discuss how can be made visible and influenced by which staff groups XY (account managers? service managers?); measure, enable and encourage these staff groups to act on identified drivers  \n",
    "\n",
    "Good Example: https://www.kaggle.com/code/hamzaben/employee-churn-model-w-strategic-retention-plan/notebook"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e858973d4b0767378b2a391e82dfc297a0e494a6ac1816312417e02d17e8ebb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
