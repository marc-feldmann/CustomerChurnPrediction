{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Keep in mind while writing up:*\n",
    "\n",
    "- *Be concise! Less is more - the fully story is in the source code for those interested.*\n",
    "- *Be deliberate about: What to highlight in which section (e.g., “this dataset was special due to its high number of variables”…)*\n",
    "- *Work with visuals and only exceptionally with code. Refer to GitHub, dump code there, the technical people will go there. And (hiring) managers will only read the write-up.*\n",
    "- *Optimize business value, not model performance! Time/Resource constraints, ….*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Report\n",
    "# **Preventing Customer Churn with Feedforward Neural Networks**\n",
    "*Disclaimer: This mock project report serves educational purposes only. The company data is public (https://www.kdd.org/kdd-cup/view/kdd-cup-2009/Data). The author has no commercial relationship with mentioned parties.* \n",
    "***\n",
    "### **Executive Summary (max. 7 sentences)**\n",
    "Situation (1 sentence based on 1.)\n",
    "<br>\n",
    "Complication (1 sentence based on 1.)\n",
    "<br>\n",
    "Solution (1 sentence based on 2.)\n",
    "<br>\n",
    "Recommendations including Solutions' Business Value Add (1-3 sentences based on 3.)\n",
    "\"much buzz around ANN, let's test that here\"\n",
    "***\n",
    "### **Report Structure**\n",
    "[Include nice + simple process visualization!]\n",
    "1. Business Problem Statement\n",
    "2. Technical Solution\n",
    "<br>    *2.1. Technical Problem Statement*\n",
    "<br>    *2.2. Exploratory Data Analysis*\n",
    "<br>    *2.3. Data Preprocessing*\n",
    "<br>    *2.4. Feature Selection*\n",
    "<br>    *2.5. Model Selection (incl. Optimization)*\n",
    "<br>    *2.6. Final Model Evaluation*\n",
    "<br>    *2.7. Future Optimization Potentials*\n",
    "3. Business Recommendations\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Business Problem Statement**\n",
    "For firms like French telecommunication provider Orange, customer retention is critical. This is because retaining customers is much cheaper than the alternative: losing a customer and their revenues plus replacement costs. However, Orange lacks an automated, scalable, and data-driven method for predicting customer churn that would allow Orange to initiate retention measures before customers leave. That is, predicting customer churn currently more or less relies on sporadic guesses. Thus, Orange requested a proof-of-concept for a predictive model that can help identify customers who will likely churn so that retention measures can be initiated. Specifically, encouraged by the enthusiasm surrounding \"deep learning\", Orange wants the proof-of-concept to explore the potential of this model class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Technical Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.1. Technical Solution: Technical Problem Statement*\n",
    "The business problem, as put by Orange, is \"to predict customer churn\". This problem requires translation into a better specified, technical problem before it is solvable using mathematical-statistical methods. Technically put, the problem we solve is to\n",
    "\n",
    "*maximize the F1-score over the churn predictions of Orange's customers by implementing an artificial neural network with more than one hidden layer and an output layer containing a single neuron with an activation function*.\n",
    "\n",
    "Each component of this technical problem statement follows from considering the following three issues in light of the business problem we solve: \n",
    "\n",
    "#### Specifying the business problem\n",
    "It is first important to understand that predicting customer churn is, technically, a binary classification problem: given the data available for any particular customer (e.g., age, gender, purchased services, average call duration), we want our model to assign this customer to one of the two classes \"churn\"/\"no churn\". Understanding that we solve a classification problem has important implications for two main elements of the technical problem statement:\n",
    "\n",
    "#### Choosing an adequate model class\n",
    "In a typical data science project, we would train models from many different model classes (e.g., logistic regression classifiers, trees, support vector machines) and select the best performing models (or combine them in an *ensemble*) for deployment. In this project, however, the client Orange has specified upfront that they want a \"deep learning\" model, which in more precise technical terms is widely understood as an artificial neural network (ANN) with more than one hidden layer. Further, since we want the ANN's output to always be either \"churn\" or \"no churn\", its output layer must contain a single neuron with an activation function (e.g., ReLU, sigmoid) that translates continuous into binary values (1/0).\n",
    "\n",
    "#### Choosing adequate evaluation metrics\n",
    "An evaluation metric enables us to assess how \"good\" a developed model is and optimize it. The perhaps most intuitive metric for a classification model is the *accuracy* of its predictions. Accuracy tells us in which percentage of cases a classification model's predictions (\"churn\"/\"no churn\") are true (that is, correctly predict what customers will actually do). However, we can infer from the business context that the classes \"churn\"/\"no churn\" we are interested in are *imbalanced*: only a minority of all customers will churn in any given time period. We can thus expect many more customers to be in the \"no churn\" rather than the \"churn\" class. Accuracy will thus be a bad metric to optimize: the model could 'cheat' and simply predict \"no churn\" in 100% of the cases, and never detect a single churning customer, and still have awesome accuracy. In presence of class imbalance, a metric more adequate to optimize is the *F1-score*. A high F1-score indicates not only that the model is able to detect many of those customers who will indeed churn (high *recall*), but also that the model's \"churn\"-predictions are typically correct (high *precision*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following resources available to solve the problem:\n",
    "- Data: Orange has provided historical customer data (50,000 observations/customers; 230 features).\n",
    "- Software: Python 3.8.5., main packages:\n",
    "    - Pandas, Numpy (for data wrangling)\n",
    "    - Keras/TensorFlow (for neural network modelling)\n",
    "    - Scikit-learn (for feature selection and optimization/gridsearch automation)\n",
    "    - Matplotlib, Seaborn (for visualization)\n",
    "- Hardware: a standard office notebook with an i7-8550U (4 cores @1.80 GHz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.2. Technical Solution: Exploratory Data Analysis (EDA)*\n",
    "Now that we have specified the technical problem this project should solve, we first familiarize ourselves with the historical customer data Orange has provided. Exploratory data analysis helps us identify how we need to preprocess this data so that the ANN can better learn from it to predict churn. This typically involves some basic overall checks (overall dataset structure, feature types, missing values), but also analyses focused on our target variable, that is, the class label vector \"churn\"/\"no churn\" (= what we want to predict)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data\n",
    "We first load the data from a local drive. X is a matrix containing features and observations, y is a vector containing the class labels we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "### some settings to increase reproducibility and report readability \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "seed(3992)\n",
    "tf.random.set_seed(3992)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"float_format\", \"{:f}\".format)\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "### Loading the data\n",
    "X = pd.read_table('data/orange_small_train.data')\n",
    "y = pd.read_table('data/orange_small_train_churn.labels', header = None,sep='\\t').loc[:, 0].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 230)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the overall dataset structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our very first analytical step is to take a broad look at the overall dataset structure, including the number of features (columns) and observations (rows = customers), feature names, features' data types, categorical features' cardinality, missing value formatting, and some basic descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>Var11</th>\n",
       "      <th>Var12</th>\n",
       "      <th>Var13</th>\n",
       "      <th>Var14</th>\n",
       "      <th>Var15</th>\n",
       "      <th>Var16</th>\n",
       "      <th>Var17</th>\n",
       "      <th>Var18</th>\n",
       "      <th>Var19</th>\n",
       "      <th>Var20</th>\n",
       "      <th>Var21</th>\n",
       "      <th>Var22</th>\n",
       "      <th>Var23</th>\n",
       "      <th>Var24</th>\n",
       "      <th>Var25</th>\n",
       "      <th>Var26</th>\n",
       "      <th>Var27</th>\n",
       "      <th>Var28</th>\n",
       "      <th>Var29</th>\n",
       "      <th>Var30</th>\n",
       "      <th>Var31</th>\n",
       "      <th>Var32</th>\n",
       "      <th>Var33</th>\n",
       "      <th>Var34</th>\n",
       "      <th>Var35</th>\n",
       "      <th>Var36</th>\n",
       "      <th>Var37</th>\n",
       "      <th>Var38</th>\n",
       "      <th>Var39</th>\n",
       "      <th>Var40</th>\n",
       "      <th>Var41</th>\n",
       "      <th>Var42</th>\n",
       "      <th>Var43</th>\n",
       "      <th>Var44</th>\n",
       "      <th>Var45</th>\n",
       "      <th>Var46</th>\n",
       "      <th>Var47</th>\n",
       "      <th>Var48</th>\n",
       "      <th>Var49</th>\n",
       "      <th>Var50</th>\n",
       "      <th>Var51</th>\n",
       "      <th>Var52</th>\n",
       "      <th>Var53</th>\n",
       "      <th>Var54</th>\n",
       "      <th>Var55</th>\n",
       "      <th>Var56</th>\n",
       "      <th>Var57</th>\n",
       "      <th>Var58</th>\n",
       "      <th>Var59</th>\n",
       "      <th>Var60</th>\n",
       "      <th>Var61</th>\n",
       "      <th>Var62</th>\n",
       "      <th>Var63</th>\n",
       "      <th>Var64</th>\n",
       "      <th>Var65</th>\n",
       "      <th>Var66</th>\n",
       "      <th>Var67</th>\n",
       "      <th>Var68</th>\n",
       "      <th>Var69</th>\n",
       "      <th>Var70</th>\n",
       "      <th>Var71</th>\n",
       "      <th>Var72</th>\n",
       "      <th>Var73</th>\n",
       "      <th>Var74</th>\n",
       "      <th>Var75</th>\n",
       "      <th>Var76</th>\n",
       "      <th>Var77</th>\n",
       "      <th>Var78</th>\n",
       "      <th>Var79</th>\n",
       "      <th>Var80</th>\n",
       "      <th>Var81</th>\n",
       "      <th>Var82</th>\n",
       "      <th>Var83</th>\n",
       "      <th>Var84</th>\n",
       "      <th>Var85</th>\n",
       "      <th>Var86</th>\n",
       "      <th>Var87</th>\n",
       "      <th>Var88</th>\n",
       "      <th>Var89</th>\n",
       "      <th>Var90</th>\n",
       "      <th>Var91</th>\n",
       "      <th>Var92</th>\n",
       "      <th>Var93</th>\n",
       "      <th>Var94</th>\n",
       "      <th>Var95</th>\n",
       "      <th>Var96</th>\n",
       "      <th>Var97</th>\n",
       "      <th>Var98</th>\n",
       "      <th>Var99</th>\n",
       "      <th>Var100</th>\n",
       "      <th>Var101</th>\n",
       "      <th>Var102</th>\n",
       "      <th>Var103</th>\n",
       "      <th>Var104</th>\n",
       "      <th>Var105</th>\n",
       "      <th>Var106</th>\n",
       "      <th>Var107</th>\n",
       "      <th>Var108</th>\n",
       "      <th>Var109</th>\n",
       "      <th>Var110</th>\n",
       "      <th>Var111</th>\n",
       "      <th>Var112</th>\n",
       "      <th>Var113</th>\n",
       "      <th>Var114</th>\n",
       "      <th>Var115</th>\n",
       "      <th>Var116</th>\n",
       "      <th>Var117</th>\n",
       "      <th>Var118</th>\n",
       "      <th>Var119</th>\n",
       "      <th>Var120</th>\n",
       "      <th>Var121</th>\n",
       "      <th>Var122</th>\n",
       "      <th>Var123</th>\n",
       "      <th>Var124</th>\n",
       "      <th>Var125</th>\n",
       "      <th>Var126</th>\n",
       "      <th>Var127</th>\n",
       "      <th>Var128</th>\n",
       "      <th>Var129</th>\n",
       "      <th>Var130</th>\n",
       "      <th>Var131</th>\n",
       "      <th>Var132</th>\n",
       "      <th>Var133</th>\n",
       "      <th>Var134</th>\n",
       "      <th>Var135</th>\n",
       "      <th>Var136</th>\n",
       "      <th>Var137</th>\n",
       "      <th>Var138</th>\n",
       "      <th>Var139</th>\n",
       "      <th>Var140</th>\n",
       "      <th>Var141</th>\n",
       "      <th>Var142</th>\n",
       "      <th>Var143</th>\n",
       "      <th>Var144</th>\n",
       "      <th>Var145</th>\n",
       "      <th>Var146</th>\n",
       "      <th>Var147</th>\n",
       "      <th>Var148</th>\n",
       "      <th>Var149</th>\n",
       "      <th>Var150</th>\n",
       "      <th>Var151</th>\n",
       "      <th>Var152</th>\n",
       "      <th>Var153</th>\n",
       "      <th>Var154</th>\n",
       "      <th>Var155</th>\n",
       "      <th>Var156</th>\n",
       "      <th>Var157</th>\n",
       "      <th>Var158</th>\n",
       "      <th>Var159</th>\n",
       "      <th>Var160</th>\n",
       "      <th>Var161</th>\n",
       "      <th>Var162</th>\n",
       "      <th>Var163</th>\n",
       "      <th>Var164</th>\n",
       "      <th>Var165</th>\n",
       "      <th>Var166</th>\n",
       "      <th>Var167</th>\n",
       "      <th>Var168</th>\n",
       "      <th>Var169</th>\n",
       "      <th>Var170</th>\n",
       "      <th>Var171</th>\n",
       "      <th>Var172</th>\n",
       "      <th>Var173</th>\n",
       "      <th>Var174</th>\n",
       "      <th>Var175</th>\n",
       "      <th>Var176</th>\n",
       "      <th>Var177</th>\n",
       "      <th>Var178</th>\n",
       "      <th>Var179</th>\n",
       "      <th>Var180</th>\n",
       "      <th>Var181</th>\n",
       "      <th>Var182</th>\n",
       "      <th>Var183</th>\n",
       "      <th>Var184</th>\n",
       "      <th>Var185</th>\n",
       "      <th>Var186</th>\n",
       "      <th>Var187</th>\n",
       "      <th>Var188</th>\n",
       "      <th>Var189</th>\n",
       "      <th>Var190</th>\n",
       "      <th>Var191</th>\n",
       "      <th>Var192</th>\n",
       "      <th>Var193</th>\n",
       "      <th>Var194</th>\n",
       "      <th>Var195</th>\n",
       "      <th>Var196</th>\n",
       "      <th>Var197</th>\n",
       "      <th>Var198</th>\n",
       "      <th>Var199</th>\n",
       "      <th>Var200</th>\n",
       "      <th>Var201</th>\n",
       "      <th>Var202</th>\n",
       "      <th>Var203</th>\n",
       "      <th>Var204</th>\n",
       "      <th>Var205</th>\n",
       "      <th>Var206</th>\n",
       "      <th>Var207</th>\n",
       "      <th>Var208</th>\n",
       "      <th>Var209</th>\n",
       "      <th>Var210</th>\n",
       "      <th>Var211</th>\n",
       "      <th>Var212</th>\n",
       "      <th>Var213</th>\n",
       "      <th>Var214</th>\n",
       "      <th>Var215</th>\n",
       "      <th>Var216</th>\n",
       "      <th>Var217</th>\n",
       "      <th>Var218</th>\n",
       "      <th>Var219</th>\n",
       "      <th>Var220</th>\n",
       "      <th>Var221</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var224</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "      <th>Var230</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1526.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>580.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166.560000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3570.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.076907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1350864.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7333.110000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>117625.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1175.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1212385.000000</td>\n",
       "      <td>69134.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>397579.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1812252.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38418.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bZkvyxLkBI</td>\n",
       "      <td>RO12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>taul</td>\n",
       "      <td>1K8T</td>\n",
       "      <td>lK27</td>\n",
       "      <td>ka_ns41</td>\n",
       "      <td>nQUveAzAF7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dXGu</td>\n",
       "      <td>9_Y1</td>\n",
       "      <td>FbIm</td>\n",
       "      <td>VpdQ</td>\n",
       "      <td>haYg</td>\n",
       "      <td>me75fM6ugJ</td>\n",
       "      <td>kIsH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>L84s</td>\n",
       "      <td>XfqtO3UdzaXh_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XTbPUYD</td>\n",
       "      <td>sH5Z</td>\n",
       "      <td>cJvF</td>\n",
       "      <td>FzaX</td>\n",
       "      <td>1YVfGrO</td>\n",
       "      <td>oslk</td>\n",
       "      <td>fXVEsaq</td>\n",
       "      <td>jySVZNlOJy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xb3V</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>353.520000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4764966.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.408032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2872928.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>151098.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58158.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>-356411.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>590.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4136430.000000</td>\n",
       "      <td>357038.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>278334.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10439160.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>238572.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CEat0G8rTN</td>\n",
       "      <td>RO12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>taul</td>\n",
       "      <td>1K8T</td>\n",
       "      <td>2Ix5</td>\n",
       "      <td>qEdASpP</td>\n",
       "      <td>y2LIM01bE1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lg1t</td>\n",
       "      <td>9_Y1</td>\n",
       "      <td>k13i</td>\n",
       "      <td>sJzTlal</td>\n",
       "      <td>zm5i</td>\n",
       "      <td>me75fM6ugJ</td>\n",
       "      <td>kIsH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>L84s</td>\n",
       "      <td>NhsEn4L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kZJyVg2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FzaX</td>\n",
       "      <td>0AJo2f2</td>\n",
       "      <td>oslk</td>\n",
       "      <td>2Kb5FSF</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fKCe</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5236.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1515.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>816.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>220.080000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5883894.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.599658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130</td>\n",
       "      <td>518.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1675776.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16211.580000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>405104.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3230.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5967.000000</td>\n",
       "      <td>-28.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3478905.000000</td>\n",
       "      <td>248932.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320565.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9826360.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>434946.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eOQt0GoOh3</td>\n",
       "      <td>AERks4l</td>\n",
       "      <td>SEuy</td>\n",
       "      <td>taul</td>\n",
       "      <td>1K8T</td>\n",
       "      <td>ffXs</td>\n",
       "      <td>NldASpP</td>\n",
       "      <td>y4g9XoZ</td>\n",
       "      <td>vynJTq9</td>\n",
       "      <td>smXZ</td>\n",
       "      <td>4bTR</td>\n",
       "      <td>9_Y1</td>\n",
       "      <td>MGOA</td>\n",
       "      <td>VpdQ</td>\n",
       "      <td>haYg</td>\n",
       "      <td>DHn_WUyBhW_whjA88g9bvA64_</td>\n",
       "      <td>kIsH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>L84s</td>\n",
       "      <td>UbxQ8lZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTGHfSv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pMWAe2U</td>\n",
       "      <td>bHR7</td>\n",
       "      <td>UYBR</td>\n",
       "      <td>FzaX</td>\n",
       "      <td>JFM1BiF</td>\n",
       "      <td>Al6ZaUT</td>\n",
       "      <td>NKv4yOc</td>\n",
       "      <td>jySVZNlOJy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>Qu4f</td>\n",
       "      <td>02N6s8f</td>\n",
       "      <td>ib5G6X1eUxUn6</td>\n",
       "      <td>am7c</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.080000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.988250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-275703.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-14.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jg69tYsGvO</td>\n",
       "      <td>RO12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>taul</td>\n",
       "      <td>1K8T</td>\n",
       "      <td>ssAy</td>\n",
       "      <td>_ybO0dd</td>\n",
       "      <td>4hMlgkf58mhwh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W8mQ</td>\n",
       "      <td>9_Y1</td>\n",
       "      <td>YULl</td>\n",
       "      <td>VpdQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>me75fM6ugJ</td>\n",
       "      <td>kIsH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>Mtgm</td>\n",
       "      <td>NhsEn4L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kq0dQfu</td>\n",
       "      <td>eKej</td>\n",
       "      <td>UYBR</td>\n",
       "      <td>FzaX</td>\n",
       "      <td>L91KIiz</td>\n",
       "      <td>oslk</td>\n",
       "      <td>CE7uk3u</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3216.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.552446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>82</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>784448.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37423.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89754.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>10714.840000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15111.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>150650.000000</td>\n",
       "      <td>66046.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3255.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>267162.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>644836.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IXSgUHShse</td>\n",
       "      <td>RO12</td>\n",
       "      <td>SEuy</td>\n",
       "      <td>taul</td>\n",
       "      <td>1K8T</td>\n",
       "      <td>uNkU</td>\n",
       "      <td>EKR938I</td>\n",
       "      <td>ThrHXVS</td>\n",
       "      <td>0v21jmy</td>\n",
       "      <td>smXZ</td>\n",
       "      <td>xklU</td>\n",
       "      <td>9_Y1</td>\n",
       "      <td>RVjC</td>\n",
       "      <td>sJzTlal</td>\n",
       "      <td>6JmL</td>\n",
       "      <td>me75fM6ugJ</td>\n",
       "      <td>kIsH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>L84s</td>\n",
       "      <td>XfqtO3UdzaXh_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SJs3duv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11p4mKe</td>\n",
       "      <td>H3p7</td>\n",
       "      <td>UYBR</td>\n",
       "      <td>FzaX</td>\n",
       "      <td>OrnLfvc</td>\n",
       "      <td>oslk</td>\n",
       "      <td>1J2cvxe</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>mj86</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Var1  Var2  Var3  Var4  Var5        Var6     Var7  Var8  Var9  Var10  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN 1526.000000 7.000000   NaN   NaN    NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN  525.000000 0.000000   NaN   NaN    NaN   \n",
       "2   NaN   NaN   NaN   NaN   NaN 5236.000000 7.000000   NaN   NaN    NaN   \n",
       "3   NaN   NaN   NaN   NaN   NaN         NaN 0.000000   NaN   NaN    NaN   \n",
       "4   NaN   NaN   NaN   NaN   NaN 1029.000000 7.000000   NaN   NaN    NaN   \n",
       "\n",
       "   Var11  Var12       Var13  Var14  Var15  Var16  Var17  Var18  Var19  Var20  \\\n",
       "0    NaN    NaN  184.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1    NaN    NaN    0.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2    NaN    NaN  904.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "3    NaN    NaN    0.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "4    NaN    NaN 3216.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "        Var21       Var22  Var23     Var24      Var25  Var26  Var27  \\\n",
       "0  464.000000  580.000000    NaN 14.000000 128.000000    NaN    NaN   \n",
       "1  168.000000  210.000000    NaN  2.000000  24.000000    NaN    NaN   \n",
       "2 1212.000000 1515.000000    NaN 26.000000 816.000000    NaN    NaN   \n",
       "3         NaN    0.000000    NaN       NaN   0.000000    NaN    NaN   \n",
       "4   64.000000   80.000000    NaN  4.000000  64.000000    NaN    NaN   \n",
       "\n",
       "       Var28  Var29  Var30  Var31  Var32  Var33  Var34    Var35  Var36  Var37  \\\n",
       "0 166.560000    NaN    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "1 353.520000    NaN    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "2 220.080000    NaN    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "3  22.080000    NaN    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "4 200.000000    NaN    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "\n",
       "           Var38  Var39  Var40  Var41  Var42  Var43    Var44  Var45  Var46  \\\n",
       "0    3570.000000    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "1 4764966.000000    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "2 5883894.000000    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "3       0.000000    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "4       0.000000    NaN    NaN    NaN    NaN    NaN 0.000000    NaN    NaN   \n",
       "\n",
       "   Var47  Var48  Var49  Var50  Var51  Var52  Var53  Var54  Var55  Var56  \\\n",
       "0    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "3    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "4    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "     Var57  Var58  Var59  Var60  Var61  Var62  Var63  Var64     Var65  Var66  \\\n",
       "0 4.076907    NaN    NaN    NaN    NaN    NaN    NaN    NaN  9.000000    NaN   \n",
       "1 5.408032    NaN    NaN    NaN    NaN    NaN    NaN    NaN  9.000000    NaN   \n",
       "2 6.599658    NaN    NaN    NaN    NaN    NaN    NaN    NaN  9.000000    NaN   \n",
       "3 1.988250    NaN    NaN    NaN    NaN    NaN    NaN    NaN  9.000000    NaN   \n",
       "4 4.552446    NaN    NaN    NaN    NaN    NaN    NaN    NaN 18.000000    NaN   \n",
       "\n",
       "   Var67  Var68  Var69  Var70  Var71    Var72  Var73      Var74  Var75  \\\n",
       "0    NaN    NaN    NaN    NaN    NaN      NaN     36  35.000000    NaN   \n",
       "1    NaN    NaN    NaN    NaN    NaN 3.000000     26   0.000000    NaN   \n",
       "2    NaN    NaN    NaN    NaN    NaN      NaN    130 518.000000    NaN   \n",
       "3    NaN    NaN    NaN    NaN    NaN      NaN     12   0.000000    NaN   \n",
       "4    NaN    NaN    NaN    NaN    NaN 3.000000     82 224.000000    NaN   \n",
       "\n",
       "           Var76  Var77    Var78  Var79  Var80         Var81  Var82     Var83  \\\n",
       "0 1350864.000000    NaN 0.000000    NaN    NaN   7333.110000    NaN  5.000000   \n",
       "1 2872928.000000    NaN 3.000000    NaN    NaN 151098.900000    NaN 25.000000   \n",
       "2 1675776.000000    NaN 0.000000    NaN    NaN  16211.580000    NaN 40.000000   \n",
       "3       0.000000    NaN 0.000000    NaN    NaN           NaN    NaN  0.000000   \n",
       "4  784448.000000    NaN 0.000000    NaN    NaN  37423.500000    NaN  0.000000   \n",
       "\n",
       "   Var84     Var85  Var86  Var87  Var88  Var89  Var90  Var91  Var92  Var93  \\\n",
       "0    NaN 12.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1    NaN  2.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2    NaN 58.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "3    NaN  0.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "4    NaN  0.000000    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "         Var94  Var95  Var96  Var97  Var98  Var99  Var100  Var101  Var102  \\\n",
       "0          NaN    NaN    NaN    NaN    NaN    NaN     NaN     NaN     NaN   \n",
       "1 58158.000000    NaN    NaN    NaN    NaN    NaN     NaN     NaN     NaN   \n",
       "2          NaN    NaN    NaN    NaN    NaN    NaN     NaN     NaN     NaN   \n",
       "3          NaN    NaN    NaN    NaN    NaN    NaN     NaN     NaN     NaN   \n",
       "4 89754.000000    NaN    NaN    NaN    NaN    NaN     NaN     NaN     NaN   \n",
       "\n",
       "   Var103  Var104  Var105  Var106  Var107  Var108     Var109  Var110  Var111  \\\n",
       "0     NaN     NaN     NaN     NaN     NaN     NaN 104.000000     NaN     NaN   \n",
       "1     NaN     NaN     NaN     NaN     NaN     NaN  40.000000     NaN     NaN   \n",
       "2     NaN     NaN     NaN     NaN     NaN     NaN 312.000000     NaN     NaN   \n",
       "3     NaN     NaN     NaN     NaN     NaN     NaN        NaN     NaN     NaN   \n",
       "4     NaN     NaN     NaN     NaN     NaN     NaN  32.000000     NaN     NaN   \n",
       "\n",
       "      Var112         Var113  Var114  Var115  Var116  Var117  Var118  \\\n",
       "0 168.000000  117625.600000     NaN     NaN     NaN     NaN     NaN   \n",
       "1  40.000000 -356411.600000     NaN     NaN     NaN     NaN     NaN   \n",
       "2 336.000000  405104.000000     NaN     NaN     NaN     NaN     NaN   \n",
       "3   0.000000 -275703.600000     NaN     NaN     NaN     NaN     NaN   \n",
       "4  56.000000   10714.840000     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "       Var119  Var120  Var121  Var122     Var123  Var124       Var125  \\\n",
       "0 1175.000000     NaN     NaN     NaN   6.000000     NaN   720.000000   \n",
       "1  590.000000     NaN     NaN     NaN  72.000000     NaN     0.000000   \n",
       "2 3230.000000     NaN     NaN     NaN 114.000000     NaN  5967.000000   \n",
       "3         NaN     NaN     NaN     NaN   0.000000     NaN     0.000000   \n",
       "4  215.000000     NaN     NaN     NaN   0.000000     NaN 15111.000000   \n",
       "\n",
       "      Var126  Var127  Var128  Var129  Var130  Var131   Var132         Var133  \\\n",
       "0   8.000000     NaN     NaN     NaN     NaN     NaN 0.000000 1212385.000000   \n",
       "1        NaN     NaN     NaN     NaN     NaN     NaN 8.000000 4136430.000000   \n",
       "2 -28.000000     NaN     NaN     NaN     NaN     NaN 0.000000 3478905.000000   \n",
       "3 -14.000000     NaN     NaN     NaN     NaN     NaN 0.000000       0.000000   \n",
       "4  58.000000     NaN     NaN     NaN     NaN     NaN 0.000000  150650.000000   \n",
       "\n",
       "         Var134  Var135  Var136  Var137  Var138  Var139      Var140  Var141  \\\n",
       "0  69134.000000     NaN     NaN     NaN     NaN     NaN  185.000000     NaN   \n",
       "1 357038.000000     NaN     NaN     NaN     NaN     NaN    0.000000     NaN   \n",
       "2 248932.000000     NaN     NaN     NaN     NaN     NaN  800.000000     NaN   \n",
       "3      0.000000     NaN     NaN     NaN     NaN     NaN    0.000000     NaN   \n",
       "4  66046.000000     NaN     NaN     NaN     NaN     NaN 3255.000000     NaN   \n",
       "\n",
       "   Var142   Var143    Var144  Var145  Var146  Var147  Var148        Var149  \\\n",
       "0     NaN 0.000000  9.000000     NaN     NaN     NaN     NaN 397579.000000   \n",
       "1     NaN 0.000000  9.000000     NaN     NaN     NaN     NaN 278334.000000   \n",
       "2     NaN 0.000000 36.000000     NaN     NaN     NaN     NaN 320565.000000   \n",
       "3     NaN 0.000000       NaN     NaN     NaN     NaN     NaN           NaN   \n",
       "4     NaN 0.000000  9.000000     NaN     NaN     NaN     NaN 267162.000000   \n",
       "\n",
       "   Var150  Var151  Var152          Var153  Var154  Var155  Var156  Var157  \\\n",
       "0     NaN     NaN     NaN  1812252.000000     NaN     NaN     NaN     NaN   \n",
       "1     NaN     NaN     NaN 10439160.000000     NaN     NaN     NaN     NaN   \n",
       "2     NaN     NaN     NaN  9826360.000000     NaN     NaN     NaN     NaN   \n",
       "3     NaN     NaN     NaN        0.000000     NaN     NaN     NaN     NaN   \n",
       "4     NaN     NaN     NaN   644836.000000     NaN     NaN     NaN     NaN   \n",
       "\n",
       "   Var158  Var159     Var160  Var161  Var162        Var163  Var164  Var165  \\\n",
       "0     NaN     NaN 142.000000     NaN     NaN  38418.000000     NaN     NaN   \n",
       "1     NaN     NaN  32.000000     NaN     NaN 238572.000000     NaN     NaN   \n",
       "2     NaN     NaN 206.000000     NaN     NaN 434946.000000     NaN     NaN   \n",
       "3     NaN     NaN   0.000000     NaN     NaN      0.000000     NaN     NaN   \n",
       "4     NaN     NaN   2.000000     NaN     NaN      0.000000     NaN     NaN   \n",
       "\n",
       "   Var166  Var167  Var168  Var169  Var170  Var171  Var172   Var173  Var174  \\\n",
       "0     NaN     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN   \n",
       "1     NaN     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN   \n",
       "2     NaN     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN   \n",
       "3     NaN     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN   \n",
       "4     NaN     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN   \n",
       "\n",
       "   Var175  Var176  Var177  Var178  Var179  Var180   Var181  Var182  Var183  \\\n",
       "0     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN     NaN   \n",
       "1     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN     NaN   \n",
       "2     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN     NaN   \n",
       "3     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN     NaN   \n",
       "4     NaN     NaN     NaN     NaN     NaN     NaN 0.000000     NaN     NaN   \n",
       "\n",
       "   Var184  Var185  Var186  Var187  Var188     Var189  Var190 Var191  \\\n",
       "0     NaN     NaN     NaN     NaN     NaN 462.000000     NaN    NaN   \n",
       "1     NaN     NaN     NaN     NaN     NaN        NaN     NaN    NaN   \n",
       "2     NaN     NaN     NaN     NaN     NaN        NaN     NaN    NaN   \n",
       "3     NaN     NaN     NaN     NaN     NaN        NaN     NaN    NaN   \n",
       "4     NaN     NaN     NaN     NaN     NaN        NaN     NaN    NaN   \n",
       "\n",
       "       Var192   Var193 Var194 Var195 Var196 Var197   Var198         Var199  \\\n",
       "0  bZkvyxLkBI     RO12    NaN   taul   1K8T   lK27  ka_ns41     nQUveAzAF7   \n",
       "1  CEat0G8rTN     RO12    NaN   taul   1K8T   2Ix5  qEdASpP     y2LIM01bE1   \n",
       "2  eOQt0GoOh3  AERks4l   SEuy   taul   1K8T   ffXs  NldASpP        y4g9XoZ   \n",
       "3  jg69tYsGvO     RO12    NaN   taul   1K8T   ssAy  _ybO0dd  4hMlgkf58mhwh   \n",
       "4  IXSgUHShse     RO12   SEuy   taul   1K8T   uNkU  EKR938I        ThrHXVS   \n",
       "\n",
       "    Var200 Var201 Var202 Var203 Var204   Var205 Var206  \\\n",
       "0      NaN    NaN   dXGu   9_Y1   FbIm     VpdQ   haYg   \n",
       "1      NaN    NaN   lg1t   9_Y1   k13i  sJzTlal   zm5i   \n",
       "2  vynJTq9   smXZ   4bTR   9_Y1   MGOA     VpdQ   haYg   \n",
       "3      NaN    NaN   W8mQ   9_Y1   YULl     VpdQ    NaN   \n",
       "4  0v21jmy   smXZ   xklU   9_Y1   RVjC  sJzTlal   6JmL   \n",
       "\n",
       "                      Var207 Var208  Var209 Var210 Var211         Var212  \\\n",
       "0                 me75fM6ugJ   kIsH     NaN   uKAI   L84s  XfqtO3UdzaXh_   \n",
       "1                 me75fM6ugJ   kIsH     NaN   uKAI   L84s        NhsEn4L   \n",
       "2  DHn_WUyBhW_whjA88g9bvA64_   kIsH     NaN   uKAI   L84s        UbxQ8lZ   \n",
       "3                 me75fM6ugJ   kIsH     NaN   uKAI   Mtgm        NhsEn4L   \n",
       "4                 me75fM6ugJ   kIsH     NaN   uKAI   L84s  XfqtO3UdzaXh_   \n",
       "\n",
       "  Var213   Var214 Var215   Var216 Var217 Var218 Var219   Var220   Var221  \\\n",
       "0    NaN      NaN    NaN  XTbPUYD   sH5Z   cJvF   FzaX  1YVfGrO     oslk   \n",
       "1    NaN      NaN    NaN  kZJyVg2    NaN    NaN   FzaX  0AJo2f2     oslk   \n",
       "2    NaN  TTGHfSv    NaN  pMWAe2U   bHR7   UYBR   FzaX  JFM1BiF  Al6ZaUT   \n",
       "3    NaN      NaN    NaN  kq0dQfu   eKej   UYBR   FzaX  L91KIiz     oslk   \n",
       "4    NaN  SJs3duv    NaN  11p4mKe   H3p7   UYBR   FzaX  OrnLfvc     oslk   \n",
       "\n",
       "    Var222      Var223 Var224 Var225 Var226   Var227         Var228 Var229  \\\n",
       "0  fXVEsaq  jySVZNlOJy    NaN    NaN   xb3V     RAYp  F2FyR07IdsN7I    NaN   \n",
       "1  2Kb5FSF  LM8l689qOp    NaN    NaN   fKCe     RAYp  F2FyR07IdsN7I    NaN   \n",
       "2  NKv4yOc  jySVZNlOJy    NaN   kG3k   Qu4f  02N6s8f  ib5G6X1eUxUn6   am7c   \n",
       "3  CE7uk3u  LM8l689qOp    NaN    NaN   FSa2     RAYp  F2FyR07IdsN7I    NaN   \n",
       "4  1J2cvxe  LM8l689qOp    NaN   kG3k   FSa2     RAYp  F2FyR07IdsN7I   mj86   \n",
       "\n",
       "   Var230  \n",
       "0     NaN  \n",
       "1     NaN  \n",
       "2     NaN  \n",
       "3     NaN  \n",
       "4     NaN  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Columns: 230 entries, Var1 to Var230\n",
      "dtypes: float64(191), int64(1), object(38)\n",
      "memory usage: 87.7+ MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>Var11</th>\n",
       "      <th>Var12</th>\n",
       "      <th>Var13</th>\n",
       "      <th>Var14</th>\n",
       "      <th>Var15</th>\n",
       "      <th>Var16</th>\n",
       "      <th>Var17</th>\n",
       "      <th>Var18</th>\n",
       "      <th>Var19</th>\n",
       "      <th>Var20</th>\n",
       "      <th>Var21</th>\n",
       "      <th>Var22</th>\n",
       "      <th>Var23</th>\n",
       "      <th>Var24</th>\n",
       "      <th>Var25</th>\n",
       "      <th>Var26</th>\n",
       "      <th>Var27</th>\n",
       "      <th>Var28</th>\n",
       "      <th>Var29</th>\n",
       "      <th>Var30</th>\n",
       "      <th>Var31</th>\n",
       "      <th>Var32</th>\n",
       "      <th>Var33</th>\n",
       "      <th>Var34</th>\n",
       "      <th>Var35</th>\n",
       "      <th>Var36</th>\n",
       "      <th>Var37</th>\n",
       "      <th>Var38</th>\n",
       "      <th>Var39</th>\n",
       "      <th>Var40</th>\n",
       "      <th>Var41</th>\n",
       "      <th>Var42</th>\n",
       "      <th>Var43</th>\n",
       "      <th>Var44</th>\n",
       "      <th>Var45</th>\n",
       "      <th>Var46</th>\n",
       "      <th>Var47</th>\n",
       "      <th>Var48</th>\n",
       "      <th>Var49</th>\n",
       "      <th>Var50</th>\n",
       "      <th>Var51</th>\n",
       "      <th>Var52</th>\n",
       "      <th>Var53</th>\n",
       "      <th>Var54</th>\n",
       "      <th>Var55</th>\n",
       "      <th>Var56</th>\n",
       "      <th>Var57</th>\n",
       "      <th>Var58</th>\n",
       "      <th>Var59</th>\n",
       "      <th>Var60</th>\n",
       "      <th>Var61</th>\n",
       "      <th>Var62</th>\n",
       "      <th>Var63</th>\n",
       "      <th>Var64</th>\n",
       "      <th>Var65</th>\n",
       "      <th>Var66</th>\n",
       "      <th>Var67</th>\n",
       "      <th>Var68</th>\n",
       "      <th>Var69</th>\n",
       "      <th>Var70</th>\n",
       "      <th>Var71</th>\n",
       "      <th>Var72</th>\n",
       "      <th>Var73</th>\n",
       "      <th>Var74</th>\n",
       "      <th>Var75</th>\n",
       "      <th>Var76</th>\n",
       "      <th>Var77</th>\n",
       "      <th>Var78</th>\n",
       "      <th>Var79</th>\n",
       "      <th>Var80</th>\n",
       "      <th>Var81</th>\n",
       "      <th>Var82</th>\n",
       "      <th>Var83</th>\n",
       "      <th>Var84</th>\n",
       "      <th>Var85</th>\n",
       "      <th>Var86</th>\n",
       "      <th>Var87</th>\n",
       "      <th>Var88</th>\n",
       "      <th>Var89</th>\n",
       "      <th>Var90</th>\n",
       "      <th>Var91</th>\n",
       "      <th>Var92</th>\n",
       "      <th>Var93</th>\n",
       "      <th>Var94</th>\n",
       "      <th>Var95</th>\n",
       "      <th>Var96</th>\n",
       "      <th>Var97</th>\n",
       "      <th>Var98</th>\n",
       "      <th>Var99</th>\n",
       "      <th>Var100</th>\n",
       "      <th>Var101</th>\n",
       "      <th>Var102</th>\n",
       "      <th>Var103</th>\n",
       "      <th>Var104</th>\n",
       "      <th>Var105</th>\n",
       "      <th>Var106</th>\n",
       "      <th>Var107</th>\n",
       "      <th>Var108</th>\n",
       "      <th>Var109</th>\n",
       "      <th>Var110</th>\n",
       "      <th>Var111</th>\n",
       "      <th>Var112</th>\n",
       "      <th>Var113</th>\n",
       "      <th>Var114</th>\n",
       "      <th>Var115</th>\n",
       "      <th>Var116</th>\n",
       "      <th>Var117</th>\n",
       "      <th>Var118</th>\n",
       "      <th>Var119</th>\n",
       "      <th>Var120</th>\n",
       "      <th>Var121</th>\n",
       "      <th>Var122</th>\n",
       "      <th>Var123</th>\n",
       "      <th>Var124</th>\n",
       "      <th>Var125</th>\n",
       "      <th>Var126</th>\n",
       "      <th>Var127</th>\n",
       "      <th>Var128</th>\n",
       "      <th>Var129</th>\n",
       "      <th>Var130</th>\n",
       "      <th>Var131</th>\n",
       "      <th>Var132</th>\n",
       "      <th>Var133</th>\n",
       "      <th>Var134</th>\n",
       "      <th>Var135</th>\n",
       "      <th>Var136</th>\n",
       "      <th>Var137</th>\n",
       "      <th>Var138</th>\n",
       "      <th>Var139</th>\n",
       "      <th>Var140</th>\n",
       "      <th>Var141</th>\n",
       "      <th>Var142</th>\n",
       "      <th>Var143</th>\n",
       "      <th>Var144</th>\n",
       "      <th>Var145</th>\n",
       "      <th>Var146</th>\n",
       "      <th>Var147</th>\n",
       "      <th>Var148</th>\n",
       "      <th>Var149</th>\n",
       "      <th>Var150</th>\n",
       "      <th>Var151</th>\n",
       "      <th>Var152</th>\n",
       "      <th>Var153</th>\n",
       "      <th>Var154</th>\n",
       "      <th>Var155</th>\n",
       "      <th>Var156</th>\n",
       "      <th>Var157</th>\n",
       "      <th>Var158</th>\n",
       "      <th>Var159</th>\n",
       "      <th>Var160</th>\n",
       "      <th>Var161</th>\n",
       "      <th>Var162</th>\n",
       "      <th>Var163</th>\n",
       "      <th>Var164</th>\n",
       "      <th>Var165</th>\n",
       "      <th>Var166</th>\n",
       "      <th>Var167</th>\n",
       "      <th>Var168</th>\n",
       "      <th>Var169</th>\n",
       "      <th>Var170</th>\n",
       "      <th>Var171</th>\n",
       "      <th>Var172</th>\n",
       "      <th>Var173</th>\n",
       "      <th>Var174</th>\n",
       "      <th>Var175</th>\n",
       "      <th>Var176</th>\n",
       "      <th>Var177</th>\n",
       "      <th>Var178</th>\n",
       "      <th>Var179</th>\n",
       "      <th>Var180</th>\n",
       "      <th>Var181</th>\n",
       "      <th>Var182</th>\n",
       "      <th>Var183</th>\n",
       "      <th>Var184</th>\n",
       "      <th>Var185</th>\n",
       "      <th>Var186</th>\n",
       "      <th>Var187</th>\n",
       "      <th>Var188</th>\n",
       "      <th>Var189</th>\n",
       "      <th>Var190</th>\n",
       "      <th>Var191</th>\n",
       "      <th>Var192</th>\n",
       "      <th>Var193</th>\n",
       "      <th>Var194</th>\n",
       "      <th>Var195</th>\n",
       "      <th>Var196</th>\n",
       "      <th>Var197</th>\n",
       "      <th>Var198</th>\n",
       "      <th>Var199</th>\n",
       "      <th>Var200</th>\n",
       "      <th>Var201</th>\n",
       "      <th>Var202</th>\n",
       "      <th>Var203</th>\n",
       "      <th>Var204</th>\n",
       "      <th>Var205</th>\n",
       "      <th>Var206</th>\n",
       "      <th>Var207</th>\n",
       "      <th>Var208</th>\n",
       "      <th>Var209</th>\n",
       "      <th>Var210</th>\n",
       "      <th>Var211</th>\n",
       "      <th>Var212</th>\n",
       "      <th>Var213</th>\n",
       "      <th>Var214</th>\n",
       "      <th>Var215</th>\n",
       "      <th>Var216</th>\n",
       "      <th>Var217</th>\n",
       "      <th>Var218</th>\n",
       "      <th>Var219</th>\n",
       "      <th>Var220</th>\n",
       "      <th>Var221</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var224</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "      <th>Var230</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>702.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1240.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>44471.000000</td>\n",
       "      <td>44461.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>1240.000000</td>\n",
       "      <td>558.000000</td>\n",
       "      <td>44461.000000</td>\n",
       "      <td>1240.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44471.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>42770.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>44989.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>847.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>3747.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>646.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>820.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>847.000000</td>\n",
       "      <td>558.000000</td>\n",
       "      <td>694.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>44461.000000</td>\n",
       "      <td>694.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>1129.000000</td>\n",
       "      <td>27620.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>44461.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>44471.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>1240.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>1083.000000</td>\n",
       "      <td>646.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>1129.000000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>27620.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>558.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>873.000000</td>\n",
       "      <td>451.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>820.000000</td>\n",
       "      <td>820.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>42770.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>1129.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>820.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>44471.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>44461.000000</td>\n",
       "      <td>36080.000000</td>\n",
       "      <td>1083.000000</td>\n",
       "      <td>1083.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>1240.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>694.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>44461.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>44471.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>42770.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>847.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>694.000000</td>\n",
       "      <td>1129.000000</td>\n",
       "      <td>873.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>873.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1083.000000</td>\n",
       "      <td>1487.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1240.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>646.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>44991.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>21022.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>1083</td>\n",
       "      <td>49631</td>\n",
       "      <td>50000</td>\n",
       "      <td>12784</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>49857</td>\n",
       "      <td>50000</td>\n",
       "      <td>49996</td>\n",
       "      <td>24592</td>\n",
       "      <td>12783</td>\n",
       "      <td>49999</td>\n",
       "      <td>49857</td>\n",
       "      <td>50000</td>\n",
       "      <td>48066</td>\n",
       "      <td>44471</td>\n",
       "      <td>50000</td>\n",
       "      <td>49857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>1129</td>\n",
       "      <td>24592</td>\n",
       "      <td>694</td>\n",
       "      <td>50000</td>\n",
       "      <td>49297</td>\n",
       "      <td>49297</td>\n",
       "      <td>44789</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>44789</td>\n",
       "      <td>820</td>\n",
       "      <td>23856</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>21568</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>361</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>225</td>\n",
       "      <td>4291</td>\n",
       "      <td>5073</td>\n",
       "      <td>15415</td>\n",
       "      <td>2</td>\n",
       "      <td>5713</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>15415</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>13990</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>4291</td>\n",
       "      <td>7</td>\n",
       "      <td>4291</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>r__I</td>\n",
       "      <td>qFpmfo8zhV</td>\n",
       "      <td>RO12</td>\n",
       "      <td>SEuy</td>\n",
       "      <td>taul</td>\n",
       "      <td>1K8T</td>\n",
       "      <td>0Xwj</td>\n",
       "      <td>fhk21Ss</td>\n",
       "      <td>r83_sZi</td>\n",
       "      <td>yP09M03</td>\n",
       "      <td>smXZ</td>\n",
       "      <td>nyZz</td>\n",
       "      <td>9_Y1</td>\n",
       "      <td>RVjC</td>\n",
       "      <td>VpdQ</td>\n",
       "      <td>IYzP</td>\n",
       "      <td>me75fM6ugJ</td>\n",
       "      <td>kIsH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>L84s</td>\n",
       "      <td>NhsEn4L</td>\n",
       "      <td>KdSa</td>\n",
       "      <td>5zARyjR</td>\n",
       "      <td>eGzu</td>\n",
       "      <td>mAjbk_S</td>\n",
       "      <td>gvA6</td>\n",
       "      <td>cJvF</td>\n",
       "      <td>FzaX</td>\n",
       "      <td>4UxGlow</td>\n",
       "      <td>oslk</td>\n",
       "      <td>catzS2D</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>4n2X</td>\n",
       "      <td>ELof</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>am7c</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1083</td>\n",
       "      <td>385</td>\n",
       "      <td>35964</td>\n",
       "      <td>12567</td>\n",
       "      <td>47958</td>\n",
       "      <td>49550</td>\n",
       "      <td>4629</td>\n",
       "      <td>4441</td>\n",
       "      <td>955</td>\n",
       "      <td>73</td>\n",
       "      <td>12777</td>\n",
       "      <td>198</td>\n",
       "      <td>45233</td>\n",
       "      <td>1819</td>\n",
       "      <td>31962</td>\n",
       "      <td>17274</td>\n",
       "      <td>35079</td>\n",
       "      <td>46022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47570</td>\n",
       "      <td>40299</td>\n",
       "      <td>29303</td>\n",
       "      <td>1129</td>\n",
       "      <td>73</td>\n",
       "      <td>694</td>\n",
       "      <td>4937</td>\n",
       "      <td>283</td>\n",
       "      <td>25319</td>\n",
       "      <td>40304</td>\n",
       "      <td>4441</td>\n",
       "      <td>37009</td>\n",
       "      <td>4441</td>\n",
       "      <td>36608</td>\n",
       "      <td>820</td>\n",
       "      <td>11072</td>\n",
       "      <td>8031</td>\n",
       "      <td>35156</td>\n",
       "      <td>32703</td>\n",
       "      <td>11689</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.487179</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>425.298387</td>\n",
       "      <td>0.125396</td>\n",
       "      <td>238793.328850</td>\n",
       "      <td>1326.437116</td>\n",
       "      <td>6.809496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.145299</td>\n",
       "      <td>392605.656355</td>\n",
       "      <td>8.625806</td>\n",
       "      <td>16.071685</td>\n",
       "      <td>1249.688401</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.239275</td>\n",
       "      <td>11.393287</td>\n",
       "      <td>7.215959</td>\n",
       "      <td>0.245092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>234.518225</td>\n",
       "      <td>290.245382</td>\n",
       "      <td>7.535306</td>\n",
       "      <td>4.507926</td>\n",
       "      <td>96.827010</td>\n",
       "      <td>0.070612</td>\n",
       "      <td>0.028245</td>\n",
       "      <td>224.507669</td>\n",
       "      <td>0.022792</td>\n",
       "      <td>7.435897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127004.950413</td>\n",
       "      <td>1.173247</td>\n",
       "      <td>0.716810</td>\n",
       "      <td>159553.853344</td>\n",
       "      <td>648522.148195</td>\n",
       "      <td>2579106.927519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.962933</td>\n",
       "      <td>26.653846</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.157937</td>\n",
       "      <td>0.166833</td>\n",
       "      <td>7256.127907</td>\n",
       "      <td>16.796132</td>\n",
       "      <td>2.188034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.091861</td>\n",
       "      <td>35.690883</td>\n",
       "      <td>43652.006725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>654326.500000</td>\n",
       "      <td>4.789686</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87964.667183</td>\n",
       "      <td>3.512311</td>\n",
       "      <td>164061.333333</td>\n",
       "      <td>414596.520976</td>\n",
       "      <td>9.538668</td>\n",
       "      <td>40.462810</td>\n",
       "      <td>5.096774</td>\n",
       "      <td>40.201729</td>\n",
       "      <td>26526.471067</td>\n",
       "      <td>14.868896</td>\n",
       "      <td>96.841499</td>\n",
       "      <td>0.016812</td>\n",
       "      <td>85.596293</td>\n",
       "      <td>3531944.965703</td>\n",
       "      <td>400340.558171</td>\n",
       "      <td>137.059345</td>\n",
       "      <td>4.190659</td>\n",
       "      <td>66.641080</td>\n",
       "      <td>103.658127</td>\n",
       "      <td>6.498791</td>\n",
       "      <td>1490153.836634</td>\n",
       "      <td>10.410256</td>\n",
       "      <td>0.534707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54421.039005</td>\n",
       "      <td>103084.052693</td>\n",
       "      <td>2.426219</td>\n",
       "      <td>20.023560</td>\n",
       "      <td>42.527419</td>\n",
       "      <td>8.461026</td>\n",
       "      <td>286892.974359</td>\n",
       "      <td>5.424501</td>\n",
       "      <td>68.947368</td>\n",
       "      <td>5.517028</td>\n",
       "      <td>0.019943</td>\n",
       "      <td>91.372896</td>\n",
       "      <td>170679.444444</td>\n",
       "      <td>2.127774</td>\n",
       "      <td>98671.065858</td>\n",
       "      <td>109771.411765</td>\n",
       "      <td>4.646253</td>\n",
       "      <td>0.915938</td>\n",
       "      <td>21295.060932</td>\n",
       "      <td>26.279924</td>\n",
       "      <td>0.887464</td>\n",
       "      <td>19.804124</td>\n",
       "      <td>28765.797421</td>\n",
       "      <td>18.782784</td>\n",
       "      <td>100.789024</td>\n",
       "      <td>67.192683</td>\n",
       "      <td>38164.132362</td>\n",
       "      <td>5.086079</td>\n",
       "      <td>193779.515670</td>\n",
       "      <td>60.888660</td>\n",
       "      <td>6.683761</td>\n",
       "      <td>284824.232416</td>\n",
       "      <td>66.221066</td>\n",
       "      <td>-153278.608487</td>\n",
       "      <td>608132.609186</td>\n",
       "      <td>37.745122</td>\n",
       "      <td>0.064103</td>\n",
       "      <td>129837.294490</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>916.112185</td>\n",
       "      <td>31.642233</td>\n",
       "      <td>6.951567</td>\n",
       "      <td>0.043513</td>\n",
       "      <td>60.188038</td>\n",
       "      <td>223079.148828</td>\n",
       "      <td>27887.625177</td>\n",
       "      <td>-0.553880</td>\n",
       "      <td>28.092336</td>\n",
       "      <td>96.526316</td>\n",
       "      <td>10.692308</td>\n",
       "      <td>0.495968</td>\n",
       "      <td>4064876.535613</td>\n",
       "      <td>3.524616</td>\n",
       "      <td>2273571.919495</td>\n",
       "      <td>437340.384877</td>\n",
       "      <td>199.920443</td>\n",
       "      <td>110690.905588</td>\n",
       "      <td>3.829060</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>181541.371890</td>\n",
       "      <td>1381.259643</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.128205</td>\n",
       "      <td>0.058012</td>\n",
       "      <td>11.727665</td>\n",
       "      <td>58.438252</td>\n",
       "      <td>2.781439</td>\n",
       "      <td>1.701412</td>\n",
       "      <td>140.486886</td>\n",
       "      <td>294920.804255</td>\n",
       "      <td>157687.615579</td>\n",
       "      <td>11.060213</td>\n",
       "      <td>8.314123</td>\n",
       "      <td>6181967.170056</td>\n",
       "      <td>1538220.900285</td>\n",
       "      <td>0.801140</td>\n",
       "      <td>169.472622</td>\n",
       "      <td>33.073516</td>\n",
       "      <td>1.896907</td>\n",
       "      <td>4.713940</td>\n",
       "      <td>38.803005</td>\n",
       "      <td>3.374288</td>\n",
       "      <td>336016.762288</td>\n",
       "      <td>486077.956658</td>\n",
       "      <td>1.753642</td>\n",
       "      <td>28842.355097</td>\n",
       "      <td>22.553463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>332.938575</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.041096</td>\n",
       "      <td>367451.723236</td>\n",
       "      <td>9.744452</td>\n",
       "      <td>0.006846</td>\n",
       "      <td>7.044965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.716129</td>\n",
       "      <td>618888.394037</td>\n",
       "      <td>16.687307</td>\n",
       "      <td>3.138062</td>\n",
       "      <td>3776754.552707</td>\n",
       "      <td>0.611456</td>\n",
       "      <td>1416638.101330</td>\n",
       "      <td>77773.795326</td>\n",
       "      <td>8.460919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.299145</td>\n",
       "      <td>16.544160</td>\n",
       "      <td>167.368477</td>\n",
       "      <td>270.142137</td>\n",
       "      <td>22007.045192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>40.709951</td>\n",
       "      <td>0.141933</td>\n",
       "      <td>4270.193518</td>\n",
       "      <td>1.275481</td>\n",
       "      <td>644125.905205</td>\n",
       "      <td>2685.693668</td>\n",
       "      <td>6.326053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154.777855</td>\n",
       "      <td>928089.603543</td>\n",
       "      <td>2.869558</td>\n",
       "      <td>64.185508</td>\n",
       "      <td>2794.954874</td>\n",
       "      <td>3.714316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.386254</td>\n",
       "      <td>49.493856</td>\n",
       "      <td>34.415427</td>\n",
       "      <td>1.781497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>565.560147</td>\n",
       "      <td>704.489990</td>\n",
       "      <td>49.449472</td>\n",
       "      <td>9.928819</td>\n",
       "      <td>214.318283</td>\n",
       "      <td>0.528836</td>\n",
       "      <td>0.247210</td>\n",
       "      <td>98.520240</td>\n",
       "      <td>0.212436</td>\n",
       "      <td>8.852461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>495792.065200</td>\n",
       "      <td>4.377707</td>\n",
       "      <td>2.996007</td>\n",
       "      <td>327715.060218</td>\n",
       "      <td>1382224.798108</td>\n",
       "      <td>3010076.456426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.410005</td>\n",
       "      <td>61.845522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.570395</td>\n",
       "      <td>1.629344</td>\n",
       "      <td>8798.176988</td>\n",
       "      <td>70.817333</td>\n",
       "      <td>10.190705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.880583</td>\n",
       "      <td>127.034597</td>\n",
       "      <td>71229.488704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1379407.949366</td>\n",
       "      <td>5.828331</td>\n",
       "      <td>NaN</td>\n",
       "      <td>273885.562784</td>\n",
       "      <td>2.025882</td>\n",
       "      <td>645780.537918</td>\n",
       "      <td>767371.190351</td>\n",
       "      <td>49.177745</td>\n",
       "      <td>190.745739</td>\n",
       "      <td>24.414978</td>\n",
       "      <td>155.736984</td>\n",
       "      <td>32764.473242</td>\n",
       "      <td>10.134441</td>\n",
       "      <td>311.249450</td>\n",
       "      <td>0.289544</td>\n",
       "      <td>397.062793</td>\n",
       "      <td>4937707.012055</td>\n",
       "      <td>809681.501273</td>\n",
       "      <td>541.152530</td>\n",
       "      <td>2.304777</td>\n",
       "      <td>52.859170</td>\n",
       "      <td>766.711978</td>\n",
       "      <td>8.625836</td>\n",
       "      <td>1853693.469274</td>\n",
       "      <td>41.686861</td>\n",
       "      <td>2.135359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>188329.196998</td>\n",
       "      <td>106272.133545</td>\n",
       "      <td>2.298420</td>\n",
       "      <td>88.129462</td>\n",
       "      <td>243.244963</td>\n",
       "      <td>20.620485</td>\n",
       "      <td>458778.436847</td>\n",
       "      <td>5.250840</td>\n",
       "      <td>226.468406</td>\n",
       "      <td>17.208674</td>\n",
       "      <td>0.373366</td>\n",
       "      <td>360.768354</td>\n",
       "      <td>222820.568594</td>\n",
       "      <td>0.624598</td>\n",
       "      <td>180633.296795</td>\n",
       "      <td>360087.858228</td>\n",
       "      <td>21.232008</td>\n",
       "      <td>3.404187</td>\n",
       "      <td>83413.164796</td>\n",
       "      <td>94.595076</td>\n",
       "      <td>3.147381</td>\n",
       "      <td>83.987299</td>\n",
       "      <td>33844.793479</td>\n",
       "      <td>85.563090</td>\n",
       "      <td>379.766750</td>\n",
       "      <td>253.177833</td>\n",
       "      <td>160059.598218</td>\n",
       "      <td>52.121719</td>\n",
       "      <td>517817.756374</td>\n",
       "      <td>141.249202</td>\n",
       "      <td>2.708837</td>\n",
       "      <td>497265.556147</td>\n",
       "      <td>157.637671</td>\n",
       "      <td>761372.983362</td>\n",
       "      <td>932057.047664</td>\n",
       "      <td>194.745524</td>\n",
       "      <td>0.434128</td>\n",
       "      <td>288764.749505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2165.433155</td>\n",
       "      <td>122.796382</td>\n",
       "      <td>30.766999</td>\n",
       "      <td>0.378505</td>\n",
       "      <td>221.551302</td>\n",
       "      <td>744424.057512</td>\n",
       "      <td>90128.381424</td>\n",
       "      <td>22.532095</td>\n",
       "      <td>124.352514</td>\n",
       "      <td>317.055769</td>\n",
       "      <td>36.852088</td>\n",
       "      <td>1.114864</td>\n",
       "      <td>25311869.221418</td>\n",
       "      <td>9.994878</td>\n",
       "      <td>2438599.586633</td>\n",
       "      <td>604341.470155</td>\n",
       "      <td>132.535538</td>\n",
       "      <td>189493.923541</td>\n",
       "      <td>12.431351</td>\n",
       "      <td>0.050331</td>\n",
       "      <td>393358.342330</td>\n",
       "      <td>3990.510522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.970706</td>\n",
       "      <td>0.643061</td>\n",
       "      <td>11.720288</td>\n",
       "      <td>228.095238</td>\n",
       "      <td>7.884085</td>\n",
       "      <td>1.484652</td>\n",
       "      <td>633.367253</td>\n",
       "      <td>656894.626034</td>\n",
       "      <td>413611.163030</td>\n",
       "      <td>53.043226</td>\n",
       "      <td>9.095334</td>\n",
       "      <td>4348926.094701</td>\n",
       "      <td>2280223.822721</td>\n",
       "      <td>3.042950</td>\n",
       "      <td>544.686538</td>\n",
       "      <td>187.525494</td>\n",
       "      <td>6.325709</td>\n",
       "      <td>11.027292</td>\n",
       "      <td>99.497149</td>\n",
       "      <td>8.579672</td>\n",
       "      <td>973198.710662</td>\n",
       "      <td>848863.809050</td>\n",
       "      <td>8.104872</td>\n",
       "      <td>115786.908570</td>\n",
       "      <td>96.211261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102.879214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.921735</td>\n",
       "      <td>604237.735365</td>\n",
       "      <td>10.397319</td>\n",
       "      <td>0.132503</td>\n",
       "      <td>36.186111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.002253</td>\n",
       "      <td>1306030.280181</td>\n",
       "      <td>59.016629</td>\n",
       "      <td>28.206035</td>\n",
       "      <td>3785695.814743</td>\n",
       "      <td>2.495681</td>\n",
       "      <td>2279786.367380</td>\n",
       "      <td>201618.768217</td>\n",
       "      <td>46.973777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.781967</td>\n",
       "      <td>60.223030</td>\n",
       "      <td>113.980072</td>\n",
       "      <td>86.707692</td>\n",
       "      <td>29085.146490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-66.880000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9803600.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-32.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.420000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>518.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.920000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>166.560000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7794.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1253.705000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2982.960000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.743164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3916.867500</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89360.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16356.960000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31297.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8630.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4342.140000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>-182714.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>425.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>-20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>216807.500000</td>\n",
       "      <td>29808.000000</td>\n",
       "      <td>23.205000</td>\n",
       "      <td>3792.910000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1232346.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>283.360000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>191735.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.380000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>2732.670000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>861.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1290246.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4201.540000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10269.840000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74884.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.514741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>157253.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>14754.600000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>480636.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>882000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73523.410000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>56817.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>84938.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>41091.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17534.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8338.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>97552.800000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>29615.420000</td>\n",
       "      <td>6748.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>560.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6471.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1479810.000000</td>\n",
       "      <td>208928.000000</td>\n",
       "      <td>225.680000</td>\n",
       "      <td>46254.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>51898.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8131560.000000</td>\n",
       "      <td>341524.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>134622.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>331.120000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>165488.400000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2431310.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>116778.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>197.640000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>12668.940000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118742.500000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>262863.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1604.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>170.080000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>285.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>266.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72972.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>158794.000000</td>\n",
       "      <td>683226.000000</td>\n",
       "      <td>4570944.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10593.925000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>48779.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>718796.250000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45491.250000</td>\n",
       "      <td>5.264741</td>\n",
       "      <td>70228.000000</td>\n",
       "      <td>531993.150000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>39063.015000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>6241545.000000</td>\n",
       "      <td>367392.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2309884.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12055.500000</td>\n",
       "      <td>181977.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>377991.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>246662.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>117353.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>39792.690000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>181947.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>384594.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>147487.500000</td>\n",
       "      <td>962158.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>109729.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>895.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31617.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3604725.000000</td>\n",
       "      <td>614884.000000</td>\n",
       "      <td>292.530000</td>\n",
       "      <td>130226.650000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147336.000000</td>\n",
       "      <td>1350.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>412112.750000</td>\n",
       "      <td>112197.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10373380.000000</td>\n",
       "      <td>2267270.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>182556.000000</td>\n",
       "      <td>615900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>276.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>383.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>527046.750000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>554414.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6471827.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1844952.000000</td>\n",
       "      <td>48810.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>252.960000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>29396.340000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>680.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>130668.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>6048550.000000</td>\n",
       "      <td>131761.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2300.000000</td>\n",
       "      <td>12325590.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>197872.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>434.920000</td>\n",
       "      <td>1220.000000</td>\n",
       "      <td>948.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36272.000000</td>\n",
       "      <td>45340.000000</td>\n",
       "      <td>1555.000000</td>\n",
       "      <td>494.000000</td>\n",
       "      <td>13168.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5158.560000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10886400.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>2419600.000000</td>\n",
       "      <td>11635020.000000</td>\n",
       "      <td>18846900.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2648.000000</td>\n",
       "      <td>728.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>60553.400000</td>\n",
       "      <td>1668.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1660.000000</td>\n",
       "      <td>347806.400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12513150.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3012970.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9676800.000000</td>\n",
       "      <td>5443200.000000</td>\n",
       "      <td>1215.000000</td>\n",
       "      <td>4624.000000</td>\n",
       "      <td>441.000000</td>\n",
       "      <td>2952.000000</td>\n",
       "      <td>223728.300000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11921.000000</td>\n",
       "      <td>18741870.000000</td>\n",
       "      <td>6189690.000000</td>\n",
       "      <td>14106.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>264.000000</td>\n",
       "      <td>142156.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>19353600.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3637080.000000</td>\n",
       "      <td>1814403.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6335.000000</td>\n",
       "      <td>4160.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>2425840.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>5105.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9404.000000</td>\n",
       "      <td>1865367.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5640330.000000</td>\n",
       "      <td>3629172.000000</td>\n",
       "      <td>498.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1245440.000000</td>\n",
       "      <td>2848.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1755.000000</td>\n",
       "      <td>279458.100000</td>\n",
       "      <td>2002.000000</td>\n",
       "      <td>10152.000000</td>\n",
       "      <td>6768.000000</td>\n",
       "      <td>2274345.000000</td>\n",
       "      <td>1932.000000</td>\n",
       "      <td>7257600.000000</td>\n",
       "      <td>7456.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>3628806.000000</td>\n",
       "      <td>10352.000000</td>\n",
       "      <td>9932480.000000</td>\n",
       "      <td>4493460.000000</td>\n",
       "      <td>5337.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2073600.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>105060.000000</td>\n",
       "      <td>2502.000000</td>\n",
       "      <td>672.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>13086.000000</td>\n",
       "      <td>9676800.000000</td>\n",
       "      <td>5436045.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>2480.000000</td>\n",
       "      <td>7147.000000</td>\n",
       "      <td>636.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>442233600.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>15009900.000000</td>\n",
       "      <td>5735340.000000</td>\n",
       "      <td>710.430000</td>\n",
       "      <td>1209602.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2700560.000000</td>\n",
       "      <td>520545.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>6126.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>18808.000000</td>\n",
       "      <td>16934400.000000</td>\n",
       "      <td>6048000.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>13907800.000000</td>\n",
       "      <td>15048560.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>8050.000000</td>\n",
       "      <td>5440.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>4862.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>10886400.000000</td>\n",
       "      <td>14515200.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>1209600.000000</td>\n",
       "      <td>2261.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1270.480000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>957.000000</td>\n",
       "      <td>5443200.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1156.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>892.000000</td>\n",
       "      <td>8554350.000000</td>\n",
       "      <td>1345.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>14284830.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>11994780.000000</td>\n",
       "      <td>3048400.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>910.000000</td>\n",
       "      <td>628.620000</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>230427.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Var1        Var2          Var3        Var4           Var5  \\\n",
       "count  702.000000 1241.000000   1240.000000 1579.000000    1487.000000   \n",
       "unique        NaN         NaN           NaN         NaN            NaN   \n",
       "top           NaN         NaN           NaN         NaN            NaN   \n",
       "freq          NaN         NaN           NaN         NaN            NaN   \n",
       "mean    11.487179    0.004029    425.298387    0.125396  238793.328850   \n",
       "std     40.709951    0.141933   4270.193518    1.275481  644125.905205   \n",
       "min      0.000000    0.000000      0.000000    0.000000       0.000000   \n",
       "25%      0.000000    0.000000      0.000000    0.000000       0.000000   \n",
       "50%      0.000000    0.000000      0.000000    0.000000       0.000000   \n",
       "75%     16.000000    0.000000      0.000000    0.000000  118742.500000   \n",
       "max    680.000000    5.000000 130668.000000   27.000000 6048550.000000   \n",
       "\n",
       "                Var6         Var7     Var8        Var9           Var10  \\\n",
       "count   44471.000000 44461.000000 0.000000  702.000000     1487.000000   \n",
       "unique           NaN          NaN      NaN         NaN             NaN   \n",
       "top              NaN          NaN      NaN         NaN             NaN   \n",
       "freq             NaN          NaN      NaN         NaN             NaN   \n",
       "mean     1326.437116     6.809496      NaN   48.145299   392605.656355   \n",
       "std      2685.693668     6.326053      NaN  154.777855   928089.603543   \n",
       "min         0.000000     0.000000      NaN    0.000000        0.000000   \n",
       "25%       518.000000     0.000000      NaN    4.000000        0.000000   \n",
       "50%       861.000000     7.000000      NaN   20.000000        0.000000   \n",
       "75%      1428.000000     7.000000      NaN   46.000000   262863.000000   \n",
       "max    131761.000000   140.000000      NaN 2300.000000 12325590.000000   \n",
       "\n",
       "             Var11       Var12         Var13       Var14    Var15       Var16  \\\n",
       "count  1240.000000  558.000000  44461.000000 1240.000000 0.000000 1487.000000   \n",
       "unique         NaN         NaN           NaN         NaN      NaN         NaN   \n",
       "top            NaN         NaN           NaN         NaN      NaN         NaN   \n",
       "freq           NaN         NaN           NaN         NaN      NaN         NaN   \n",
       "mean      8.625806   16.071685   1249.688401    0.741935      NaN  120.239275   \n",
       "std       2.869558   64.185508   2794.954874    3.714316      NaN   72.386254   \n",
       "min       8.000000    0.000000      0.000000    0.000000      NaN    0.000000   \n",
       "25%       8.000000    0.000000      0.000000    0.000000      NaN   51.920000   \n",
       "50%       8.000000    0.000000    232.000000    0.000000      NaN  131.080000   \n",
       "75%       8.000000   16.000000   1604.000000    0.000000      NaN  170.080000   \n",
       "max      40.000000 1184.000000 197872.000000   48.000000      NaN  434.920000   \n",
       "\n",
       "             Var17       Var18       Var19    Var20        Var21        Var22  \\\n",
       "count  1579.000000 1579.000000 1579.000000 0.000000 44471.000000 44991.000000   \n",
       "unique         NaN         NaN         NaN      NaN          NaN          NaN   \n",
       "top            NaN         NaN         NaN      NaN          NaN          NaN   \n",
       "freq           NaN         NaN         NaN      NaN          NaN          NaN   \n",
       "mean     11.393287    7.215959    0.245092      NaN   234.518225   290.245382   \n",
       "std      49.493856   34.415427    1.781497      NaN   565.560147   704.489990   \n",
       "min       0.000000    0.000000    0.000000      NaN     0.000000     0.000000   \n",
       "25%       0.000000    0.000000    0.000000      NaN   112.000000   135.000000   \n",
       "50%       0.000000    0.000000    0.000000      NaN   144.000000   180.000000   \n",
       "75%      10.000000    0.000000    0.000000      NaN   228.000000   285.000000   \n",
       "max    1220.000000  948.000000   27.000000      NaN 36272.000000 45340.000000   \n",
       "\n",
       "             Var23        Var24        Var25       Var26       Var27  \\\n",
       "count  1487.000000 42770.000000 44991.000000 1487.000000 1487.000000   \n",
       "unique         NaN          NaN          NaN         NaN         NaN   \n",
       "top            NaN          NaN          NaN         NaN         NaN   \n",
       "freq           NaN          NaN          NaN         NaN         NaN   \n",
       "mean      7.535306     4.507926    96.827010    0.070612    0.028245   \n",
       "std      49.449472     9.928819   214.318283    0.528836    0.247210   \n",
       "min       0.000000     0.000000     0.000000    0.000000    0.000000   \n",
       "25%       0.000000     0.000000    16.000000    0.000000    0.000000   \n",
       "50%       0.000000     2.000000    48.000000    0.000000    0.000000   \n",
       "75%       5.000000     6.000000   112.000000    0.000000    0.000000   \n",
       "max    1555.000000   494.000000 13168.000000    9.000000    4.000000   \n",
       "\n",
       "              Var28      Var29      Var30    Var31    Var32           Var33  \\\n",
       "count  44989.000000 702.000000 702.000000 0.000000 0.000000      847.000000   \n",
       "unique          NaN        NaN        NaN      NaN      NaN             NaN   \n",
       "top             NaN        NaN        NaN      NaN      NaN             NaN   \n",
       "freq            NaN        NaN        NaN      NaN      NaN             NaN   \n",
       "mean     224.507669   0.022792   7.435897      NaN      NaN   127004.950413   \n",
       "std       98.520240   0.212436   8.852461      NaN      NaN   495792.065200   \n",
       "min      -66.880000   0.000000   0.000000      NaN      NaN        0.000000   \n",
       "25%      166.560000   0.000000   0.000000      NaN      NaN        0.000000   \n",
       "50%      220.080000   0.000000   5.000000      NaN      NaN        0.000000   \n",
       "75%      266.400000   0.000000  10.000000      NaN      NaN    72972.000000   \n",
       "max     5158.560000   2.000000  95.000000      NaN      NaN 10886400.000000   \n",
       "\n",
       "             Var34        Var35          Var36           Var37  \\\n",
       "count  1241.000000 44991.000000    1241.000000     1579.000000   \n",
       "unique         NaN          NaN            NaN             NaN   \n",
       "top            NaN          NaN            NaN             NaN   \n",
       "freq           NaN          NaN            NaN             NaN   \n",
       "mean      1.173247     0.716810  159553.853344   648522.148195   \n",
       "std       4.377707     2.996007  327715.060218  1382224.798108   \n",
       "min       0.000000     0.000000       0.000000        0.000000   \n",
       "25%       0.000000     0.000000       0.000000        0.000000   \n",
       "50%       0.000000     0.000000       0.000000        0.000000   \n",
       "75%       0.000000     0.000000  158794.000000   683226.000000   \n",
       "max      56.000000   110.000000 2419600.000000 11635020.000000   \n",
       "\n",
       "                 Var38    Var39       Var40      Var41    Var42       Var43  \\\n",
       "count     44991.000000 0.000000 1241.000000 702.000000 0.000000 1241.000000   \n",
       "unique             NaN      NaN         NaN        NaN      NaN         NaN   \n",
       "top                NaN      NaN         NaN        NaN      NaN         NaN   \n",
       "freq               NaN      NaN         NaN        NaN      NaN         NaN   \n",
       "mean    2579106.927519      NaN   13.962933  26.653846      NaN    4.157937   \n",
       "std     3010076.456426      NaN   81.410005  61.845522      NaN   20.570395   \n",
       "min           0.000000      NaN    0.000000   0.000000      NaN    0.000000   \n",
       "25%        7794.000000      NaN    0.000000   0.000000      NaN    0.000000   \n",
       "50%     1290246.000000      NaN    0.000000   7.000000      NaN    0.000000   \n",
       "75%     4570944.000000      NaN    8.000000  28.000000      NaN    0.000000   \n",
       "max    18846900.000000      NaN 2648.000000 728.000000      NaN  625.000000   \n",
       "\n",
       "              Var44        Var45       Var46      Var47    Var48       Var49  \\\n",
       "count  44991.000000   344.000000 1241.000000 702.000000 0.000000 1241.000000   \n",
       "unique          NaN          NaN         NaN        NaN      NaN         NaN   \n",
       "top             NaN          NaN         NaN        NaN      NaN         NaN   \n",
       "freq            NaN          NaN         NaN        NaN      NaN         NaN   \n",
       "mean       0.166833  7256.127907   16.796132   2.188034      NaN    0.091861   \n",
       "std        1.629344  8798.176988   70.817333  10.190705      NaN    0.880583   \n",
       "min        0.000000     0.000000    0.000000   0.000000      NaN    0.000000   \n",
       "25%        0.000000  1253.705000    0.000000   0.000000      NaN    0.000000   \n",
       "50%        0.000000  4201.540000    0.000000   0.000000      NaN    0.000000   \n",
       "75%        0.000000 10593.925000   16.000000   0.000000      NaN    0.000000   \n",
       "max      135.000000 60553.400000 1668.000000 168.000000      NaN   18.000000   \n",
       "\n",
       "             Var50         Var51    Var52           Var53       Var54  \\\n",
       "count   702.000000   3747.000000 0.000000      702.000000 1241.000000   \n",
       "unique         NaN           NaN      NaN             NaN         NaN   \n",
       "top            NaN           NaN      NaN             NaN         NaN   \n",
       "freq           NaN           NaN      NaN             NaN         NaN   \n",
       "mean     35.690883  43652.006725      NaN   654326.500000    4.789686   \n",
       "std     127.034597  71229.488704      NaN  1379407.949366    5.828331   \n",
       "min       0.000000      0.000000      NaN        0.000000    0.000000   \n",
       "25%       0.000000   2982.960000      NaN        0.000000    0.000000   \n",
       "50%       0.000000  10269.840000      NaN    74884.500000    0.000000   \n",
       "75%       5.000000  48779.800000      NaN   718796.250000    8.000000   \n",
       "max    1660.000000 347806.400000      NaN 12513150.000000   32.000000   \n",
       "\n",
       "          Var55          Var56        Var57          Var58          Var59  \\\n",
       "count  0.000000     646.000000 50000.000000     702.000000     820.000000   \n",
       "unique      NaN            NaN          NaN            NaN            NaN   \n",
       "top         NaN            NaN          NaN            NaN            NaN   \n",
       "freq        NaN            NaN          NaN            NaN            NaN   \n",
       "mean        NaN   87964.667183     3.512311  164061.333333  414596.520976   \n",
       "std         NaN  273885.562784     2.025882  645780.537918  767371.190351   \n",
       "min         NaN       0.000000     0.000214       0.000000       0.000000   \n",
       "25%         NaN       0.000000     1.743164       0.000000       0.000000   \n",
       "50%         NaN       0.000000     3.514741       0.000000  157253.850000   \n",
       "75%         NaN   45491.250000     5.264741   70228.000000  531993.150000   \n",
       "max         NaN 3012970.000000     7.000000 9676800.000000 5443200.000000   \n",
       "\n",
       "             Var60       Var61      Var62       Var63         Var64  \\\n",
       "count  1487.000000  847.000000 558.000000  694.000000    238.000000   \n",
       "unique         NaN         NaN        NaN         NaN           NaN   \n",
       "top            NaN         NaN        NaN         NaN           NaN   \n",
       "freq           NaN         NaN        NaN         NaN           NaN   \n",
       "mean      9.538668   40.462810   5.096774   40.201729  26526.471067   \n",
       "std      49.177745  190.745739  24.414978  155.736984  32764.473242   \n",
       "min       0.000000    0.000000   0.000000    0.000000      0.000000   \n",
       "25%       0.000000    0.000000   0.000000    0.000000   3916.867500   \n",
       "50%       0.000000   16.000000   0.000000   12.000000  14754.600000   \n",
       "75%       6.000000   32.000000   0.000000   36.000000  39063.015000   \n",
       "max    1215.000000 4624.000000 441.000000 2952.000000 223728.300000   \n",
       "\n",
       "              Var65       Var66       Var67        Var68           Var69  \\\n",
       "count  44461.000000  694.000000 1487.000000  1241.000000     1487.000000   \n",
       "unique          NaN         NaN         NaN          NaN             NaN   \n",
       "top             NaN         NaN         NaN          NaN             NaN   \n",
       "freq            NaN         NaN         NaN          NaN             NaN   \n",
       "mean      14.868896   96.841499    0.016812    85.596293  3531944.965703   \n",
       "std       10.134441  311.249450    0.289544   397.062793  4937707.012055   \n",
       "min        9.000000    0.000000    0.000000     0.000000        0.000000   \n",
       "25%        9.000000    8.000000    0.000000     0.000000        0.000000   \n",
       "50%        9.000000   40.000000    0.000000     7.000000   480636.000000   \n",
       "75%       18.000000   92.000000    0.000000    91.000000  6241545.000000   \n",
       "max      180.000000 4600.000000    5.000000 11921.000000 18741870.000000   \n",
       "\n",
       "                Var70        Var71        Var72        Var73         Var74  \\\n",
       "count     1487.000000  1129.000000 27620.000000 50000.000000  44461.000000   \n",
       "unique            NaN          NaN          NaN          NaN           NaN   \n",
       "top               NaN          NaN          NaN          NaN           NaN   \n",
       "freq              NaN          NaN          NaN          NaN           NaN   \n",
       "mean    400340.558171   137.059345     4.190659    66.641080    103.658127   \n",
       "std     809681.501273   541.152530     2.304777    52.859170    766.711978   \n",
       "min          0.000000     0.000000     3.000000     4.000000      0.000000   \n",
       "25%          0.000000     0.000000     3.000000    24.000000      0.000000   \n",
       "50%          0.000000    42.000000     3.000000    52.000000      7.000000   \n",
       "75%     367392.000000   126.000000     6.000000   102.000000     84.000000   \n",
       "max    6189690.000000 14106.000000    24.000000   264.000000 142156.000000   \n",
       "\n",
       "             Var75           Var76      Var77        Var78    Var79  \\\n",
       "count  1241.000000    44991.000000 702.000000 44991.000000 0.000000   \n",
       "unique         NaN             NaN        NaN          NaN      NaN   \n",
       "top            NaN             NaN        NaN          NaN      NaN   \n",
       "freq           NaN             NaN        NaN          NaN      NaN   \n",
       "mean      6.498791  1490153.836634  10.410256     0.534707      NaN   \n",
       "std       8.625836  1853693.469274  41.686861     2.135359      NaN   \n",
       "min       0.000000        0.000000   0.000000     0.000000      NaN   \n",
       "25%       0.000000    89360.000000   0.000000     0.000000      NaN   \n",
       "50%       5.000000   882000.000000   0.000000     0.000000      NaN   \n",
       "75%      10.000000  2309884.000000   6.000000     0.000000      NaN   \n",
       "max      70.000000 19353600.000000 666.000000    39.000000      NaN   \n",
       "\n",
       "                Var80          Var81       Var82        Var83       Var84  \\\n",
       "count     1487.000000   44471.000000 1579.000000 44991.000000 1240.000000   \n",
       "unique            NaN            NaN         NaN          NaN         NaN   \n",
       "top               NaN            NaN         NaN          NaN         NaN   \n",
       "freq              NaN            NaN         NaN          NaN         NaN   \n",
       "mean     54421.039005  103084.052693    2.426219    20.023560   42.527419   \n",
       "std     188329.196998  106272.133545    2.298420    88.129462  243.244963   \n",
       "min          0.000000       0.000000    0.000000     0.000000    0.000000   \n",
       "25%          0.000000   16356.960000    0.000000     0.000000    0.000000   \n",
       "50%          0.000000   73523.410000    3.000000    10.000000    0.000000   \n",
       "75%      12055.500000  181977.000000    3.000000    25.000000    0.000000   \n",
       "max    3637080.000000 1814403.000000    9.000000  6335.000000 4160.000000   \n",
       "\n",
       "              Var85          Var86      Var87       Var88      Var89  \\\n",
       "count  44991.000000     702.000000 702.000000 1083.000000 646.000000   \n",
       "unique          NaN            NaN        NaN         NaN        NaN   \n",
       "top             NaN            NaN        NaN         NaN        NaN   \n",
       "freq            NaN            NaN        NaN         NaN        NaN   \n",
       "mean       8.461026  286892.974359   5.424501   68.947368   5.517028   \n",
       "std       20.620485  458778.436847   5.250840  226.468406  17.208674   \n",
       "min        0.000000       0.000000   0.000000    0.000000   0.000000   \n",
       "25%        0.000000       0.000000   0.000000    0.000000   0.000000   \n",
       "50%        4.000000   56817.000000   7.000000   25.000000   0.000000   \n",
       "75%       10.000000  377991.500000   7.000000   75.000000   6.000000   \n",
       "max     1178.000000 2425840.000000  28.000000 5105.000000 300.000000   \n",
       "\n",
       "            Var90       Var91          Var92       Var93          Var94  \\\n",
       "count  702.000000 1129.000000     171.000000 1487.000000   27620.000000   \n",
       "unique        NaN         NaN            NaN         NaN            NaN   \n",
       "top           NaN         NaN            NaN         NaN            NaN   \n",
       "freq          NaN         NaN            NaN         NaN            NaN   \n",
       "mean     0.019943   91.372896  170679.444444    2.127774   98671.065858   \n",
       "std      0.373366  360.768354  222820.568594    0.624598  180633.296795   \n",
       "min      0.000000    0.000000       0.000000    2.000000       0.000000   \n",
       "25%      0.000000    0.000000   31297.000000    2.000000    8630.250000   \n",
       "50%      0.000000   28.000000   84938.000000    2.000000   41091.000000   \n",
       "75%      0.000000   84.000000  246662.500000    2.000000  117353.250000   \n",
       "max      7.000000 9404.000000 1865367.000000    8.000000 5640330.000000   \n",
       "\n",
       "                Var95       Var96       Var97          Var98       Var99  \\\n",
       "count     1241.000000 1241.000000 1487.000000     558.000000 1579.000000   \n",
       "unique            NaN         NaN         NaN            NaN         NaN   \n",
       "top               NaN         NaN         NaN            NaN         NaN   \n",
       "freq              NaN         NaN         NaN            NaN         NaN   \n",
       "mean    109771.411765    4.646253    0.915938   21295.060932   26.279924   \n",
       "std     360087.858228   21.232008    3.404187   83413.164796   94.595076   \n",
       "min          0.000000    0.000000    0.000000       0.000000    0.000000   \n",
       "25%          0.000000    0.000000    0.000000       0.000000    0.000000   \n",
       "50%          0.000000    0.000000    0.000000       0.000000    0.000000   \n",
       "75%          0.000000    4.000000    0.000000       0.000000   24.000000   \n",
       "max    3629172.000000  498.000000   36.000000 1245440.000000 2848.000000   \n",
       "\n",
       "           Var100      Var101        Var102      Var103       Var104  \\\n",
       "count  702.000000  873.000000    451.000000 1487.000000   820.000000   \n",
       "unique        NaN         NaN           NaN         NaN          NaN   \n",
       "top           NaN         NaN           NaN         NaN          NaN   \n",
       "freq          NaN         NaN           NaN         NaN          NaN   \n",
       "mean     0.887464   19.804124  28765.797421   18.782784   100.789024   \n",
       "std      3.147381   83.987299  33844.793479   85.563090   379.766750   \n",
       "min      0.000000    0.000000      0.000000    0.000000     0.000000   \n",
       "25%      0.000000    0.000000   4342.140000    0.000000     0.000000   \n",
       "50%      0.000000    0.000000  17534.700000    0.000000    45.000000   \n",
       "75%      0.000000   18.000000  39792.690000   14.000000   117.000000   \n",
       "max     28.000000 1755.000000 279458.100000 2002.000000 10152.000000   \n",
       "\n",
       "            Var105         Var106      Var107         Var108       Var109  \\\n",
       "count   820.000000    1579.000000 1487.000000     702.000000 42770.000000   \n",
       "unique         NaN            NaN         NaN            NaN          NaN   \n",
       "top            NaN            NaN         NaN            NaN          NaN   \n",
       "freq           NaN            NaN         NaN            NaN          NaN   \n",
       "mean     67.192683   38164.132362    5.086079  193779.515670    60.888660   \n",
       "std     253.177833  160059.598218   52.121719  517817.756374   141.249202   \n",
       "min       0.000000       0.000000    0.000000       0.000000     0.000000   \n",
       "25%       0.000000       0.000000    0.000000       0.000000    24.000000   \n",
       "50%      30.000000       0.000000    0.000000    8338.000000    32.000000   \n",
       "75%      78.000000       0.000000    3.000000  181947.000000    56.000000   \n",
       "max    6768.000000 2274345.000000 1932.000000 7257600.000000  7456.000000   \n",
       "\n",
       "           Var110         Var111       Var112          Var113         Var114  \\\n",
       "count  702.000000    1129.000000 44991.000000    50000.000000    1241.000000   \n",
       "unique        NaN            NaN          NaN             NaN            NaN   \n",
       "top           NaN            NaN          NaN             NaN            NaN   \n",
       "freq          NaN            NaN          NaN             NaN            NaN   \n",
       "mean     6.683761  284824.232416    66.221066  -153278.608487  608132.609186   \n",
       "std      2.708837  497265.556147   157.637671   761372.983362  932057.047664   \n",
       "min      6.000000       0.000000     0.000000 -9803600.000000       0.000000   \n",
       "25%      6.000000       0.000000    16.000000  -182714.800000       0.000000   \n",
       "50%      6.000000   97552.800000    40.000000    29615.420000    6748.000000   \n",
       "75%      6.000000  384594.000000    72.000000   147487.500000  962158.000000   \n",
       "max     30.000000 3628806.000000 10352.000000  9932480.000000 4493460.000000   \n",
       "\n",
       "            Var115     Var116         Var117     Var118        Var119  \\\n",
       "count   820.000000 702.000000    1579.000000 171.000000  44471.000000   \n",
       "unique         NaN        NaN            NaN        NaN           NaN   \n",
       "top            NaN        NaN            NaN        NaN           NaN   \n",
       "freq           NaN        NaN            NaN        NaN           NaN   \n",
       "mean     37.745122   0.064103  129837.294490   3.000000    916.112185   \n",
       "std     194.745524   0.434128  288764.749505   0.000000   2165.433155   \n",
       "min       0.000000   0.000000       0.000000   3.000000      0.000000   \n",
       "25%       0.000000   0.000000       0.000000   3.000000    425.000000   \n",
       "50%       9.000000   0.000000       0.000000   3.000000    560.000000   \n",
       "75%      36.000000   0.000000  109729.000000   3.000000    895.000000   \n",
       "max    5337.000000   3.000000 2073600.000000   3.000000 105060.000000   \n",
       "\n",
       "            Var120     Var121      Var122       Var123         Var124  \\\n",
       "count  1487.000000 702.000000 1241.000000 44991.000000    1579.000000   \n",
       "unique         NaN        NaN         NaN          NaN            NaN   \n",
       "top            NaN        NaN         NaN          NaN            NaN   \n",
       "freq           NaN        NaN         NaN          NaN            NaN   \n",
       "mean     31.642233   6.951567    0.043513    60.188038  223079.148828   \n",
       "std     122.796382  30.766999    0.378505   221.551302  744424.057512   \n",
       "min       0.000000   0.000000    0.000000     0.000000       0.000000   \n",
       "25%       0.000000   0.000000    0.000000     6.000000       0.000000   \n",
       "50%       0.000000   2.000000    0.000000    30.000000       0.000000   \n",
       "75%      24.000000   6.000000    0.000000    72.000000       0.000000   \n",
       "max    2502.000000 672.000000    6.000000 13086.000000 9676800.000000   \n",
       "\n",
       "               Var125       Var126      Var127      Var128     Var129  \\\n",
       "count    44461.000000 36080.000000 1083.000000 1083.000000 702.000000   \n",
       "unique            NaN          NaN         NaN         NaN        NaN   \n",
       "top               NaN          NaN         NaN         NaN        NaN   \n",
       "freq              NaN          NaN         NaN         NaN        NaN   \n",
       "mean     27887.625177    -0.553880   28.092336   96.526316  10.692308   \n",
       "std      90128.381424    22.532095  124.352514  317.055769  36.852088   \n",
       "min          0.000000   -32.000000    0.000000    0.000000   0.000000   \n",
       "25%        234.000000   -20.000000    0.000000    0.000000   0.000000   \n",
       "50%       6471.000000     4.000000    0.000000   35.000000   2.000000   \n",
       "75%      31617.000000    10.000000   32.000000  105.000000  10.000000   \n",
       "max    5436045.000000    68.000000 2480.000000 7147.000000 636.000000   \n",
       "\n",
       "            Var130           Var131       Var132          Var133  \\\n",
       "count  1240.000000       702.000000 44991.000000    44991.000000   \n",
       "unique         NaN              NaN          NaN             NaN   \n",
       "top            NaN              NaN          NaN             NaN   \n",
       "freq           NaN              NaN          NaN             NaN   \n",
       "mean      0.495968   4064876.535613     3.524616  2273571.919495   \n",
       "std       1.114864  25311869.221418     9.994878  2438599.586633   \n",
       "min       0.000000         0.000000     0.000000        0.000000   \n",
       "25%       0.000000         0.000000     0.000000   216807.500000   \n",
       "50%       0.000000         0.000000     0.000000  1479810.000000   \n",
       "75%       0.000000         0.000000     0.000000  3604725.000000   \n",
       "max       3.000000 442233600.000000   184.000000 15009900.000000   \n",
       "\n",
       "               Var134      Var135         Var136     Var137      Var138  \\\n",
       "count    44991.000000 1579.000000     694.000000 702.000000 1579.000000   \n",
       "unique            NaN         NaN            NaN        NaN         NaN   \n",
       "top               NaN         NaN            NaN        NaN         NaN   \n",
       "freq              NaN         NaN            NaN        NaN         NaN   \n",
       "mean    437340.384877  199.920443  110690.905588   3.829060    0.001267   \n",
       "std     604341.470155  132.535538  189493.923541  12.431351    0.050331   \n",
       "min          0.000000    0.000000       0.000000   0.000000    0.000000   \n",
       "25%      29808.000000   23.205000    3792.910000   0.000000    0.000000   \n",
       "50%     208928.000000  225.680000   46254.600000   0.000000    0.000000   \n",
       "75%     614884.000000  292.530000  130226.650000   4.000000    0.000000   \n",
       "max    5735340.000000  710.430000 1209602.000000 204.000000    2.000000   \n",
       "\n",
       "               Var139        Var140   Var141     Var142       Var143  \\\n",
       "count     1487.000000  44461.000000 0.000000 702.000000 44991.000000   \n",
       "unique            NaN           NaN      NaN        NaN          NaN   \n",
       "top               NaN           NaN      NaN        NaN          NaN   \n",
       "freq              NaN           NaN      NaN        NaN          NaN   \n",
       "mean    181541.371890   1381.259643      NaN   1.128205     0.058012   \n",
       "std     393358.342330   3990.510522      NaN   1.970706     0.643061   \n",
       "min          0.000000      0.000000      NaN   0.000000     0.000000   \n",
       "25%          0.000000      0.000000      NaN   0.000000     0.000000   \n",
       "50%          0.000000    220.000000      NaN   0.000000     0.000000   \n",
       "75%     147336.000000   1350.000000      NaN   4.000000     0.000000   \n",
       "max    2700560.000000 520545.000000      NaN  12.000000    18.000000   \n",
       "\n",
       "             Var144      Var145      Var146      Var147       Var148  \\\n",
       "count  44471.000000 1579.000000 1487.000000 1487.000000  1487.000000   \n",
       "unique          NaN         NaN         NaN         NaN          NaN   \n",
       "top             NaN         NaN         NaN         NaN          NaN   \n",
       "freq            NaN         NaN         NaN         NaN          NaN   \n",
       "mean      11.727665   58.438252    2.781439    1.701412   140.486886   \n",
       "std       11.720288  228.095238    7.884085    1.484652   633.367253   \n",
       "min        0.000000    0.000000    0.000000    0.000000     0.000000   \n",
       "25%        0.000000    0.000000    0.000000    0.000000     0.000000   \n",
       "50%        9.000000   12.000000    0.000000    2.000000    24.000000   \n",
       "75%       18.000000   54.000000    0.000000    2.000000   128.000000   \n",
       "max       81.000000 6126.000000   96.000000    8.000000 18808.000000   \n",
       "\n",
       "                Var149         Var150      Var151      Var152          Var153  \\\n",
       "count     42770.000000    1579.000000  847.000000 1579.000000    44991.000000   \n",
       "unique             NaN            NaN         NaN         NaN             NaN   \n",
       "top                NaN            NaN         NaN         NaN             NaN   \n",
       "freq               NaN            NaN         NaN         NaN             NaN   \n",
       "mean     294920.804255  157687.615579   11.060213    8.314123  6181967.170056   \n",
       "std      656894.626034  413611.163030   53.043226    9.095334  4348926.094701   \n",
       "min           0.000000       0.000000    0.000000    0.000000        0.000000   \n",
       "25%           0.000000       0.000000    0.000000    0.000000  1232346.000000   \n",
       "50%       51898.000000       0.000000    0.000000    6.000000  8131560.000000   \n",
       "75%      412112.750000  112197.500000    8.000000   12.000000 10373380.000000   \n",
       "max    16934400.000000 6048000.000000 1200.000000   66.000000 13907800.000000   \n",
       "\n",
       "                Var154      Var155      Var156      Var157     Var158  \\\n",
       "count       702.000000 1579.000000  694.000000 1129.000000 873.000000   \n",
       "unique             NaN         NaN         NaN         NaN        NaN   \n",
       "top                NaN         NaN         NaN         NaN        NaN   \n",
       "freq               NaN         NaN         NaN         NaN        NaN   \n",
       "mean    1538220.900285    0.801140  169.472622   33.073516   1.896907   \n",
       "std     2280223.822721    3.042950  544.686538  187.525494   6.325709   \n",
       "min           0.000000    0.000000    0.000000    0.000000   0.000000   \n",
       "25%           0.000000    0.000000   14.000000    0.000000   0.000000   \n",
       "50%      341524.000000    0.000000   70.000000    8.000000   0.000000   \n",
       "75%     2267270.000000    0.000000  161.000000   28.000000   3.000000   \n",
       "max    15048560.000000   35.000000 8050.000000 5440.000000  87.000000   \n",
       "\n",
       "            Var159       Var160      Var161          Var162          Var163  \\\n",
       "count  1241.000000 44991.000000 1579.000000     1241.000000    44991.000000   \n",
       "unique         NaN          NaN         NaN             NaN             NaN   \n",
       "top            NaN          NaN         NaN             NaN             NaN   \n",
       "freq           NaN          NaN         NaN             NaN             NaN   \n",
       "mean      4.713940    38.803005    3.374288   336016.762288   486077.956658   \n",
       "std      11.027292    99.497149    8.579672   973198.710662   848863.809050   \n",
       "min       0.000000     0.000000    0.000000        0.000000        0.000000   \n",
       "25%       0.000000    10.000000    0.000000        0.000000        0.000000   \n",
       "50%       0.000000    22.000000    0.000000        0.000000   134622.000000   \n",
       "75%       0.000000    42.000000    0.000000   182556.000000   615900.000000   \n",
       "max      99.000000  4862.000000   81.000000 10886400.000000 14515200.000000   \n",
       "\n",
       "            Var164         Var165      Var166   Var167      Var168   Var169  \\\n",
       "count  1579.000000     873.000000 1487.000000 0.000000  702.000000 0.000000   \n",
       "unique         NaN            NaN         NaN      NaN         NaN      NaN   \n",
       "top            NaN            NaN         NaN      NaN         NaN      NaN   \n",
       "freq           NaN            NaN         NaN      NaN         NaN      NaN   \n",
       "mean      1.753642   28842.355097   22.553463      NaN  332.938575      NaN   \n",
       "std       8.104872  115786.908570   96.211261      NaN  102.879214      NaN   \n",
       "min       0.000000       0.000000    0.000000      NaN    0.000000      NaN   \n",
       "25%       0.000000       0.000000    0.000000      NaN  283.360000      NaN   \n",
       "50%       0.000000       0.000000    0.000000      NaN  331.120000      NaN   \n",
       "75%       0.000000     276.000000   14.000000      NaN  383.800000      NaN   \n",
       "max     138.000000 1209600.000000 2261.000000      NaN 1270.480000      NaN   \n",
       "\n",
       "            Var170         Var171      Var172       Var173      Var174  \\\n",
       "count  1241.000000    1083.000000 1487.000000 44991.000000 1579.000000   \n",
       "unique         NaN            NaN         NaN          NaN         NaN   \n",
       "top            NaN            NaN         NaN          NaN         NaN   \n",
       "freq           NaN            NaN         NaN          NaN         NaN   \n",
       "mean      3.041096  367451.723236    9.744452     0.006846    7.044965   \n",
       "std      27.921735  604237.735365   10.397319     0.132503   36.186111   \n",
       "min       0.000000       0.000000    0.000000     0.000000    0.000000   \n",
       "25%       0.000000       0.000000    0.000000     0.000000    0.000000   \n",
       "50%       0.000000  165488.400000    7.000000     0.000000    0.000000   \n",
       "75%       3.000000  527046.750000   14.000000     0.000000    8.000000   \n",
       "max     957.000000 5443200.000000  119.000000     6.000000 1156.000000   \n",
       "\n",
       "         Var175      Var176         Var177      Var178      Var179  \\\n",
       "count  0.000000 1240.000000    1241.000000  646.000000 1579.000000   \n",
       "unique      NaN         NaN            NaN         NaN         NaN   \n",
       "top         NaN         NaN            NaN         NaN         NaN   \n",
       "freq        NaN         NaN            NaN         NaN         NaN   \n",
       "mean        NaN    4.716129  618888.394037   16.687307    3.138062   \n",
       "std         NaN   31.002253 1306030.280181   59.016629   28.206035   \n",
       "min         NaN    0.000000       0.000000    0.000000    0.000000   \n",
       "25%         NaN    0.000000       0.000000    0.000000    0.000000   \n",
       "50%         NaN    0.000000       0.000000    5.000000    0.000000   \n",
       "75%         NaN    0.000000  554414.000000   15.000000    0.000000   \n",
       "max         NaN  892.000000 8554350.000000 1345.000000  890.000000   \n",
       "\n",
       "                Var180       Var181          Var182         Var183  \\\n",
       "count       702.000000 44991.000000     1579.000000    1241.000000   \n",
       "unique             NaN          NaN             NaN            NaN   \n",
       "top                NaN          NaN             NaN            NaN   \n",
       "freq               NaN          NaN             NaN            NaN   \n",
       "mean    3776754.552707     0.611456  1416638.101330   77773.795326   \n",
       "std     3785695.814743     2.495681  2279786.367380  201618.768217   \n",
       "min           0.000000     0.000000        0.000000       0.000000   \n",
       "25%      191735.250000     0.000000        0.000000       0.000000   \n",
       "50%     2431310.000000     0.000000   116778.000000       0.000000   \n",
       "75%     6471827.250000     0.000000  1844952.000000   48810.000000   \n",
       "max    14284830.000000    49.000000 11994780.000000 3048400.000000   \n",
       "\n",
       "            Var184   Var185     Var186     Var187      Var188       Var189  \\\n",
       "count  1241.000000 0.000000 702.000000 702.000000 1241.000000 21022.000000   \n",
       "unique         NaN      NaN        NaN        NaN         NaN          NaN   \n",
       "top            NaN      NaN        NaN        NaN         NaN          NaN   \n",
       "freq           NaN      NaN        NaN        NaN         NaN          NaN   \n",
       "mean      8.460919      NaN   3.299145  16.544160  167.368477   270.142137   \n",
       "std      46.973777      NaN   8.781967  60.223030  113.980072    86.707692   \n",
       "min       0.000000      NaN   0.000000   0.000000   -6.420000     6.000000   \n",
       "25%       0.000000      NaN   0.000000   0.000000   19.380000   204.000000   \n",
       "50%       0.000000      NaN   0.000000   4.000000  197.640000   270.000000   \n",
       "75%       8.000000      NaN   6.000000  14.000000  252.960000   330.000000   \n",
       "max    1200.000000      NaN 102.000000 910.000000  628.620000   642.000000   \n",
       "\n",
       "              Var190 Var191      Var192 Var193 Var194 Var195 Var196 Var197  \\\n",
       "count     333.000000   1083       49631  50000  12784  50000  50000  49857   \n",
       "unique           NaN      1         361     51      3     23      4    225   \n",
       "top              NaN   r__I  qFpmfo8zhV   RO12   SEuy   taul   1K8T   0Xwj   \n",
       "freq             NaN   1083         385  35964  12567  47958  49550   4629   \n",
       "mean    22007.045192    NaN         NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "std     29085.146490    NaN         NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "min         0.000000    NaN         NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "25%      2732.670000    NaN         NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "50%     12668.940000    NaN         NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "75%     29396.340000    NaN         NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "max    230427.000000    NaN         NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "         Var198   Var199   Var200 Var201 Var202 Var203 Var204 Var205 Var206  \\\n",
       "count     50000    49996    24592  12783  49999  49857  50000  48066  44471   \n",
       "unique     4291     5073    15415      2   5713      5    100      3     21   \n",
       "top     fhk21Ss  r83_sZi  yP09M03   smXZ   nyZz   9_Y1   RVjC   VpdQ   IYzP   \n",
       "freq       4441      955       73  12777    198  45233   1819  31962  17274   \n",
       "mean        NaN      NaN      NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "std         NaN      NaN      NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "min         NaN      NaN      NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "25%         NaN      NaN      NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "50%         NaN      NaN      NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "75%         NaN      NaN      NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "max         NaN      NaN      NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "            Var207 Var208   Var209 Var210 Var211   Var212 Var213   Var214  \\\n",
       "count        50000  49857 0.000000  50000  50000    50000   1129    24592   \n",
       "unique          14      2      NaN      6      2       81      1    15415   \n",
       "top     me75fM6ugJ   kIsH      NaN   uKAI   L84s  NhsEn4L   KdSa  5zARyjR   \n",
       "freq         35079  46022      NaN  47570  40299    29303   1129       73   \n",
       "mean           NaN    NaN      NaN    NaN    NaN      NaN    NaN      NaN   \n",
       "std            NaN    NaN      NaN    NaN    NaN      NaN    NaN      NaN   \n",
       "min            NaN    NaN      NaN    NaN    NaN      NaN    NaN      NaN   \n",
       "25%            NaN    NaN      NaN    NaN    NaN      NaN    NaN      NaN   \n",
       "50%            NaN    NaN      NaN    NaN    NaN      NaN    NaN      NaN   \n",
       "75%            NaN    NaN      NaN    NaN    NaN      NaN    NaN      NaN   \n",
       "max            NaN    NaN      NaN    NaN    NaN      NaN    NaN      NaN   \n",
       "\n",
       "       Var215   Var216 Var217 Var218 Var219   Var220 Var221   Var222  \\\n",
       "count     694    50000  49297  49297  44789    50000  50000    50000   \n",
       "unique      1     2016  13990      2     22     4291      7     4291   \n",
       "top      eGzu  mAjbk_S   gvA6   cJvF   FzaX  4UxGlow   oslk  catzS2D   \n",
       "freq      694     4937    283  25319  40304     4441  37009     4441   \n",
       "mean      NaN      NaN    NaN    NaN    NaN      NaN    NaN      NaN   \n",
       "std       NaN      NaN    NaN    NaN    NaN      NaN    NaN      NaN   \n",
       "min       NaN      NaN    NaN    NaN    NaN      NaN    NaN      NaN   \n",
       "25%       NaN      NaN    NaN    NaN    NaN      NaN    NaN      NaN   \n",
       "50%       NaN      NaN    NaN    NaN    NaN      NaN    NaN      NaN   \n",
       "75%       NaN      NaN    NaN    NaN    NaN      NaN    NaN      NaN   \n",
       "max       NaN      NaN    NaN    NaN    NaN      NaN    NaN      NaN   \n",
       "\n",
       "            Var223 Var224 Var225 Var226 Var227         Var228 Var229   Var230  \n",
       "count        44789    820  23856  50000  50000          50000  21568 0.000000  \n",
       "unique           4      1      3     23      7             30      4      NaN  \n",
       "top     LM8l689qOp   4n2X   ELof   FSa2   RAYp  F2FyR07IdsN7I   am7c      NaN  \n",
       "freq         36608    820  11072   8031  35156          32703  11689      NaN  \n",
       "mean           NaN    NaN    NaN    NaN    NaN            NaN    NaN      NaN  \n",
       "std            NaN    NaN    NaN    NaN    NaN            NaN    NaN      NaN  \n",
       "min            NaN    NaN    NaN    NaN    NaN            NaN    NaN      NaN  \n",
       "25%            NaN    NaN    NaN    NaN    NaN            NaN    NaN      NaN  \n",
       "50%            NaN    NaN    NaN    NaN    NaN            NaN    NaN      NaN  \n",
       "75%            NaN    NaN    NaN    NaN    NaN            NaN    NaN      NaN  \n",
       "max            NaN    NaN    NaN    NaN    NaN            NaN    NaN      NaN  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features' mean cardinality: 1882.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_cat = list(X.select_dtypes(include=['object']).columns)\n",
    "temp = X[features_cat]\n",
    "temp = temp.nunique().sort_values(ascending=False).reset_index()\n",
    "temp.columns = ['categorical features', 'number of different categories']\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "plt.figure(figsize=(20, 5))\n",
    "ax = sns.barplot(x='categorical features', y='number of different categories', data=temp, color='black', alpha=0.7)\n",
    "plt.xticks(np.arange(0, len(temp['categorical features'])+1, 5))\n",
    "plt.xlabel('Categorical Features')\n",
    "plt.xticks([])\n",
    "print(\"Categorical features' mean cardinality:\", round(temp['number of different categories'].sum()/temp.shape[0],0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we noted that the data contains missing values (\"NaN\"), we want to know precisely which percentage of values is missing in the data we have been given, and also how these missing values are distributed across features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.698"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(X.isna().sum().sum()/(X.shape[0]*X.shape[1]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 230 artists>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp = X.isna().sum()/(X.shape[0])*100\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.xticks(np.arange(0, len(temp)+1, 5))\n",
    "plt.xlim(0,len(temp))\n",
    "plt.xlabel('Features')\n",
    "plt.xticks([])\n",
    "plt.ylabel('% missing values')\n",
    "plt.bar(range(len(temp)), sorted(temp, reverse=True), color='black', alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking class balance \"churn\"/\"no churn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having acquired an overall impression of the data, we now take a more focused look on our target variable 'Churn', that is, the column containing the class labels our model will need to predict.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ8AAAEvCAYAAADrf2CjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWf0lEQVR4nO3df6xf9Xkf8PeDnQBaCoPgMGSzmg5PDUFbMxxEm2hryyboVkbaQecuLWyyailja9KsrSCT1uQPpERrlyxLoUJNxI9lAY+lC6TJOgYha1QENWkGBYLiJk2wQOCELKHboDN59sc9d/n6cn19Df742ve+XtJX33Oecz7Hz/nH/urtz/mc6u4AAAAAwAjHrXQDAAAAAKxewicAAAAAhhE+AQAAADCM8AkAAACAYYRPAAAAAAwjfAIAAABgmPUr3cCRdtppp/XmzZtXug0AAACAVePBBx/8RndvWOzYmgufNm/enF27dq10GwAAAACrRlV97UDHPHYHAAAAwDDCJwAAAACGET4BAAAAMIzwCQAAAIBhhE8AAAAADCN8AgAAAGAY4RMAAAAAwwifAAAAABhG+AQAAADAMMInAAAAAIYRPgEAAAAwzPqVbgCOZpdccslKtwCsQnfeeedKtwAAAEeMmU8AAAAADCN8AgAAAGAY4RMAAAAAwwifAAAAABhG+AQAAADAMMInAAAAAIYRPgEAAAAwjPAJAAAAgGGETwAAAAAMI3wCAAAAYBjhEwAAAADDCJ8AAAAAGEb4BAAAAMAwwicAAAAAhhE+AQAAADCM8AkAAACAYYRPAAAAAAwjfAIAAABgGOETAAAAAMMInwAAAAAYRvgEAAAAwDDCJwAAAACGET4BAAAAMIzwCQAAAIBhhE8AAAAADCN8AgAAAGAY4RMAAAAAwwifAAAAABhG+AQAAADAMMInAAAAAIYRPgEAAAAwzPDwqarWVdUfVdWnpv1Tq+quqvry9H3KzLnXVNXuqnq8qi6aqZ9XVQ9Pxz5UVTXVj6+q26b6/VW1efT9AAAAALB8R2Lm0zuSPDazf3WSu7t7S5K7p/1U1TlJtiV5Q5KLk1xXVeumMdcn2ZFky/S5eKpvT/Kt7j47yQeSvH/srQAAAABwKIaGT1W1KcnfS/LbM+VLk9w0bd+U5K0z9Vu7+4Xu/mqS3UnOr6ozkpzU3fd1dye5ecGY+WvdnuTC+VlRAAAAAKy80TOfPpjkV5N8d6Z2enc/lSTT9+um+sYkT8yct2eqbZy2F9b3G9Pd+5J8O8lrFzZRVTuqaldV7dq7d+8rvCUAAAAAlmtY+FRVP5nkme5+cLlDFqn1EvWlxuxf6L6hu7d299YNGzYssx0AAAAAXqn1A6/95iR/v6r+bpITkpxUVf8+ydNVdUZ3PzU9UvfMdP6eJGfOjN+U5MmpvmmR+uyYPVW1PsnJSZ4ddUMAAAAAHJphM5+6+5ru3tTdmzO3kPg93f1zSe5IcuV02pVJPjlt35Fk2/QGu7Myt7D4A9Ojec9V1QXTek5XLBgzf63Lpj/jJTOfAAAAAFgZI2c+Hcj7kuysqu1Jvp7k8iTp7keqameSR5PsS3JVd784jXl7khuTnJjkM9MnST6S5Jaq2p25GU/bjtRNAAAAAHBwRyR86u57k9w7bX8zyYUHOO/aJNcuUt+V5NxF6s9nCq8AAAAAOPqMftsdAAAAAGuY8AkAAACAYYRPAAAAAAwjfAIAAABgGOETAAAAAMMInwAAAAAYRvgEAAAAwDDCJwAAAACGET4BAAAAMIzwCQAAAIBhhE8AAAAADCN8AgAAAGAY4RMAAAAAwwifAAAAABhG+AQAAADAMMInAAAAAIYRPgEAAAAwjPAJAAAAgGGETwAAAAAMI3wCAAAAYBjhEwAAAADDCJ8AAAAAGEb4BAAAAMAwwicAAAAAhhE+AQAAADCM8AkAAACAYYRPAAAAAAwjfAIAAABgGOETAAAAAMMInwAAAAAYRvgEAAAAwDDCJwAAAACGET4BAAAAMIzwCQAAAIBhhE8AAAAADCN8AgAAAGAY4RMAAAAAwwifAAAAABhG+AQAAADAMMInAAAAAIYRPgEAAAAwjPAJAAAAgGGETwAAAAAMI3wCAAAAYBjhEwAAAADDCJ8AAAAAGGZY+FRVJ1TVA1X1P6rqkap671Q/taruqqovT9+nzIy5pqp2V9XjVXXRTP28qnp4OvahqqqpfnxV3TbV76+qzaPuBwAAAIBDN3Lm0wtJfry7/3qSH0pycVVdkOTqJHd395Ykd0/7qapzkmxL8oYkFye5rqrWTde6PsmOJFumz8VTfXuSb3X32Uk+kOT9A+8HAAAAgEM0LHzqOX827b5q+nSSS5PcNNVvSvLWafvSJLd29wvd/dUku5OcX1VnJDmpu+/r7k5y84Ix89e6PcmF87OiAAAAAFh5Q9d8qqp1VfXFJM8kuau7709yenc/lSTT9+um0zcmeWJm+J6ptnHaXljfb0x370vy7SSvHXIzAAAAAByyoeFTd7/Y3T+UZFPmZjGdu8Tpi81Y6iXqS43Z/8JVO6pqV1Xt2rt370G6BgAAAOBwOSJvu+vu/5nk3syt1fT09Chdpu9nptP2JDlzZtimJE9O9U2L1PcbU1Xrk5yc5NlF/vwbuntrd2/dsGHD4bkpAAAAAA5q5NvuNlTVX5y2T0zyt5N8KckdSa6cTrsyySen7TuSbJveYHdW5hYWf2B6NO+5qrpgWs/pigVj5q91WZJ7pnWhAAAAADgKrB947TOS3DS9se64JDu7+1NVdV+SnVW1PcnXk1yeJN39SFXtTPJokn1JruruF6drvT3JjUlOTPKZ6ZMkH0lyS1XtztyMp20D7wcAAACAQzQsfOruh5K8cZH6N5NceIAx1ya5dpH6riQvWS+qu5/PFF4BAAAAcPQ5Ims+AQAAALA2CZ8AAAAAGEb4BAAAAMAwwicAAAAAhhE+AQAAADCM8AkAAACAYYRPAAAAAAwjfAIAAABgGOETAAAAAMMInwAAAAAYRvgEAAAAwDDCJwAAAACGET4BAAAAMIzwCQAAAIBhlhU+VdWbl1MDAAAAgFnLnfn075ZZAwAAAID/b/1SB6vqh5P8SJINVfWumUMnJVk3sjEAAAAAjn1Lhk9JXp3kNdN53zdT/06Sy0Y1BQAAAMDqsGT41N2fS/K5qrqxu792hHoCAAAAYJU42MynecdX1Q1JNs+O6e4fH9EUAAAAAKvDcsOn/5jkt5L8dpIXx7UDAAAAwGqy3PBpX3dfP7QTAAAAAFad45Z53p1V9U+r6oyqOnX+M7QzAAAAAI55y535dOX0/SsztU7yA4e3HQAAAABWk2WFT9191uhGAAAAAFh9lhU+VdUVi9W7++bD2w4AAAAAq8lyH7t708z2CUkuTPKFJMInAAAAAA5ouY/d/fPZ/ao6OcktQzoCAAAAYNVY7tvuFvrfSbYczkYAAAAAWH2Wu+bTnZl7u12SrEvy+iQ7RzUFAAAAwOqw3DWffn1me1+Sr3X3ngH9AAAAALCKLOuxu+7+XJIvJfm+JKck+fORTQEAAACwOiwrfKqqn0nyQJLLk/xMkvur6rKRjQEAAABw7FvuY3f/MsmbuvuZJKmqDUn+W5LbRzUGAAAAwLFvuW+7O24+eJp88xDGAgAAALBGLXfm03+pqt9L8vFp/x8m+fSYlgAAAABYLZYMn6rq7CSnd/evVNVPJ3lLkkpyX5KPHYH+AAAAADiGHezRuQ8meS5JuvsT3f2u7v6lzM16+uDY1gAAAAA41h0sfNrc3Q8tLHb3riSbh3QEAAAAwKpxsPDphCWOnXg4GwEAAABg9TlY+PSHVfULC4tVtT3Jg2NaAgAAAGC1ONjb7t6Z5Heq6m35Xti0Ncmrk/zUwL4AAAAAWAWWDJ+6++kkP1JVP5bk3Kn8u919z/DOAAAAADjmHWzmU5Kkuz+b5LODewEAAABglTnYmk8AAAAA8LIJnwAAAAAYRvgEAAAAwDDDwqeqOrOqPltVj1XVI1X1jql+alXdVVVfnr5PmRlzTVXtrqrHq+qimfp5VfXwdOxDVVVT/fiqum2q319Vm0fdDwAAAACHbuTMp31J/kV3vz7JBUmuqqpzklyd5O7u3pLk7mk/07FtSd6Q5OIk11XVuula1yfZkWTL9Ll4qm9P8q3uPjvJB5K8f+D9AAAAAHCIhoVP3f1Ud39h2n4uyWNJNia5NMlN02k3JXnrtH1pklu7+4Xu/mqS3UnOr6ozkpzU3fd1dye5ecGY+WvdnuTC+VlRAAAAAKy8I7Lm0/Q43BuT3J/k9O5+KpkLqJK8bjptY5InZobtmWobp+2F9f3GdPe+JN9O8tohNwEAAADAIRsePlXVa5L8pyTv7O7vLHXqIrVeor7UmIU97KiqXVW1a+/evQdrGQAAAIDDZGj4VFWvylzw9LHu/sRUfnp6lC7T9zNTfU+SM2eGb0ry5FTftEh9vzFVtT7JyUmeXdhHd9/Q3Vu7e+uGDRsOx60BAAAAsAwj33ZXST6S5LHu/jczh+5IcuW0fWWST87Ut01vsDsrcwuLPzA9mvdcVV0wXfOKBWPmr3VZknumdaEAAAAAOAqsH3jtNyf5+SQPV9UXp9q7k7wvyc6q2p7k60kuT5LufqSqdiZ5NHNvyruqu1+cxr09yY1JTkzymemTzIVbt1TV7szNeNo28H4AAAAAOETDwqfu/nwWX5MpSS48wJhrk1y7SH1XknMXqT+fKbwCAAAA4OhzRN52BwAAAMDaJHwCAAAAYBjhEwAAAADDCJ8AAAAAGEb4BAAAAMAwwicAAAAAhhE+AQAAADCM8AkAAACAYYRPAAAAAAwjfAIAAABgGOETAAAAAMMInwAAAAAYRvgEAAAAwDDCJwAAAACGET4BAAAAMIzwCQAAAIBhhE8AAAAADCN8AgAAAGAY4RMAAAAAwwifAAAAABhG+AQAAADAMMInAAAAAIYRPgEAAAAwjPAJAAAAgGGETwAAAAAMI3wCAAAAYBjhEwAAAADDCJ8AAAAAGEb4BAAAAMAwwicAAAAAhhE+AQAAADCM8AkAAACAYYRPAAAAAAwjfAIAAABgGOETAAAAAMMInwAAAAAYRvgEAAAAwDDCJwAAAACGET4BAAAAMIzwCQAAAIBhhE8AAAAADCN8AgAAAGAY4RMAAAAAwwifAAAAABhG+AQAAADAMMInAAAAAIYRPgEAAAAwzLDwqao+WlXPVNUfz9ROraq7qurL0/cpM8euqardVfV4VV00Uz+vqh6ejn2oqmqqH19Vt031+6tq86h7AQAAAODlGTnz6cYkFy+oXZ3k7u7ekuTuaT9VdU6SbUneMI25rqrWTWOuT7IjyZbpM3/N7Um+1d1nJ/lAkvcPuxMAAAAAXpZh4VN3//ckzy4oX5rkpmn7piRvnanf2t0vdPdXk+xOcn5VnZHkpO6+r7s7yc0Lxsxf6/YkF87PigIAAADg6HCk13w6vbufSpLp+3VTfWOSJ2bO2zPVNk7bC+v7jenufUm+neS1wzoHAAAA4JAdLQuOLzZjqZeoLzXmpRev2lFVu6pq1969e19miwAAAAAcqiMdPj09PUqX6fuZqb4nyZkz521K8uRU37RIfb8xVbU+ycl56WN+SZLuvqG7t3b31g0bNhymWwEAAADgYI50+HRHkiun7SuTfHKmvm16g91ZmVtY/IHp0bznquqCaT2nKxaMmb/WZUnumdaFAgAAAOAosX7Uhavq40l+NMlpVbUnya8leV+SnVW1PcnXk1yeJN39SFXtTPJokn1JruruF6dLvT1zb847Mclnpk+SfCTJLVW1O3MznraNuhcAAAAAXp5h4VN3/+wBDl14gPOvTXLtIvVdSc5dpP58pvAKAAAAgKPT0bLgOAAAAACrkPAJAAAAgGGETwAAAAAMI3wCAAAAYBjhEwAAAADDCJ8AAAAAGEb4BAAAAMAwwicAAAAAhhE+AQAAADCM8AkAAACAYYRPAAAAAAwjfAIAAABgGOETAAAAAMMInwAAAAAYRvgEAAAAwDDCJwAAAACGET4BAAAAMIzwCQAAAIBhhE8AAAAADCN8AgAAAGAY4RMAAAAAwwifAAAAABhG+AQAAADAMMInAAAAAIYRPgEAAAAwjPAJAAAAgGGETwAAAAAMI3wCAAAAYBjhEwAAAADDCJ8AAAAAGGb9SjcAAAC8cpdccslKtwCsQnfeeedKt8AqYOYTAAAAAMMInwAAAAAYRvgEAAAAwDDCJwAAAACGET4BAAAAMIzwCQAAAIBhhE8AAAAADCN8AgAAAGAY4RMAAAAAwwifAAAAABhG+AQAAADAMMInAAAAAIYRPgEAAAAwjPAJAAAAgGGETwAAAAAMI3wCAAAAYJhjPnyqqour6vGq2l1VV690PwAAAAB8zzEdPlXVuiS/meQnkpyT5Ger6pyV7QoAAACAecd0+JTk/CS7u/sr3f3nSW5NcukK9wQAAADA5FgPnzYmeWJmf89UAwAAAOAosH6lG3iFapFav+Skqh1Jdky7f1ZVjw/tCliLTkvyjZVugmND1WL/fAHAEeN3C8vmdwuH4PsPdOBYD5/2JDlzZn9TkicXntTdNyS54Ug1Baw9VbWru7eudB8AAAfjdwtwpB3rj939YZItVXVWVb06ybYkd6xwTwAAAABMjumZT929r6r+WZLfS7IuyUe7+5EVbgsAAACAyTEdPiVJd386yadXug9gzfNoLwBwrPC7BTiiqvsl63MDAAAAwGFxrK/5BAAAAMBRTPgErClV1VX1GzP7v1xV7znEa/xEVe2qqseq6ktV9etT/caquuwwtwwAcEBV9Zeq6taq+pOqerSqPl1VO6rqUyvdG8A84ROw1ryQ5Ker6rSXM7iqzk3y4SQ/192vT3Jukq8cjsaqat3huA4AsDZUVSX5nST3dvdf6e5zkrw7yemv8LrH/NrAwNFF+ASsNfsyt8jmLy08UFXfX1V3V9VD0/dfXmT8rya5tru/lMy9dbO7r5s5/jer6g+q6ivzs6Cq6kdn//exqj5cVf942v7TqvpXVfX5JJdP+++tqi9U1cNV9YOH7c4BgNXmx5L83+7+rflCd38xye8neU1V3T7N0v7YFFTN//Y4bdreWlX3Ttvvqaobquq/Jrl52v9oVd07/a75xSN9c8DqIXwC1qLfTPK2qjp5Qf3DSW7u7r+W5GNJPrTI2HOTPLjEtc9I8pYkP5nkfcvs5/nufkt33zrtf6O7/0aS65P88jKvAQCsPUv9LnljkncmOSfJDyR58zKud16SS7v7H037P5jkoiTnJ/m1qnrVK+oWWLOET8Ca093fSXJzkoX/g/fDSf7DtH1L5kKkQ/Wfu/u73f1olj/l/bYF+5+Yvh9Msvll9AAA8EB37+nu7yb5Ypb3m+KO7v4/M/u/290vdPc3kjyTV/g4H7B2CZ+AteqDSbYn+QtLnNOL1B7J3P8KHsgLM9s1fe/L/n/fnrBgzP86wDVeTGLNBQDgQJb6XTL7m2T2N8Xs75Ll/iZZeA2AQyJ8Atak7n42yc7MBVDz/iDJtmn7bUk+v8jQf53k3VX1V5Okqo6rqncd5I/7WpJzqur46VG/C19R8wAAc+5JcnxV/cJ8oarelORvLTHmT/O9wOofjGsN4HuET8Ba9htJZt9694tJ/klVPZTk55O8Y+GA7n4oc+snfLyqHkvyx5lb5+mAuvuJzAVdD2VuLak/OhzNAwBrW3d3kp9K8neq6k+q6pEk70ny5BLD3pvk31bV72duNhPAcDX39xUAAAAAHH5mPgEAAAAwjPAJAAAAgGGETwAAAAAMI3wCAAAAYBjhEwAAAADDCJ8AAAAAGEb4BAAAAMAwwicAAAAAhvl/GS1Q93Y2E8gAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "temp = y.copy()\n",
    "temp = temp.replace(-1, 'No Churn')\n",
    "temp = temp.replace(1, 'Churn')\n",
    "temp.value_counts().plot.bar(color='black', alpha=0.7, rot=0)\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No Churn    46328\n",
       "Churn        3672\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### > EDA key insights:\n",
    "To summarize, our brief exploratory analysis helped us gain some important insights about the features, observations, and target variable in our data. Going forward, we will need to keep these insights at the back of our mind as they will instruct us how to properly preprocess the data so that our predictive model can learn well from it. Key insight we have gleaned are:\n",
    "\n",
    "- Features: \n",
    "\t- Most or all features' scales differ.\n",
    "\t- Orange has anonymized data before providing it (likely to protect customers' privacy).\n",
    "\t- The 230 total features include 38 categorical and 192 numerical features.\n",
    "\t- High cardinality is a big issue in this dataset (on average, 1882 unique values per categorical feature)\n",
    "- Observations:\n",
    "\t- We have been given data on 50.000 customers and their churn behaviour that we can use for optimizing and evaluating our model.\n",
    "\t- Missing values are another big issue in this dataset (around 70%).\n",
    "- Target variable:\n",
    "\t- As expected, the two classes in our target variable 'Churn' are heavily imbalanced (around 1 churning customer for 12 non-churning customers).\n",
    "\t- The class labels \"churn\"/\"no churn\" are represented by the numerical values 1/-1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.3. Technical Solution: Data Preprocessing*\n",
    "Before the data can be used for effective model training, it needs to be preprocessed. This is because the ANN model we implement requires data to be in a particular format to effectively learn from the data how to make good predictions. In addition, since preprocessing steps often have interdependencies, being deliberate about their order is important. Otherwise we risk messing up the data used for model training, compromising prediction quality. \n",
    "\n",
    "Main preprocessing steps that the key insights gleaned from EDA suggest are the following:\n",
    "- remove observations and features with all values missing\n",
    "- replace infrequent categories with a single 'catch-all' category\n",
    "- create binary indicator columns for missing values\n",
    "- impute missing values\n",
    "- encode (= 'make numerical') categorical features\n",
    "- normalize the features' scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove observations and features with all values missing\n",
    "We first remove any potential observations and features which might not hold any values. These will only push computation times and risk complicating our model without adding anything to its explanatory power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dropna(axis=0, how='all', inplace=True)\n",
    "X.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see comparing to above, no observations, but 18 features have been removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace infrequent categories with a single catch-all category\n",
    "EDA has shown that a big issue in the data is the high mean cardinality of categorical features. If there are many different categories, it will be hard for our model to recognize patterns that the model can exploit for predicting customer churn. Thus, to help our model more easily infer useful explanatory patterns, we replace all categories whose frequency of occurrence lies below a cetrain treshold with a single category \"RARE_VALUE\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cat = list(X.select_dtypes(include=['object']).columns) # update this since we have dropped some features\n",
    "for feat in features_cat:\n",
    "    X.loc[X[feat].value_counts(dropna=False)[X[feat]].values < X.shape[0] * 0.02, feat] = 'RARE_VALUE' # replace all categories which occurr in less than 2% of observatiosn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated by the following fraction of cells in which categories have been replaced, this preprocessing step will have a significant impact on our later model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X == 'RARE_VALUE'].count().sum()/(X.shape[0]*X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting aside some test data\n",
    "Before we proceed with the next preprocessing steps, we put aside some test data. The idea behing putting aside test data is essentially to pretend that we would possess some of the new/unseen data that will actually be only coming in after our model will have been deployed (= in actual use, or 'production'). This trick allows us to get an idea about how good our model would perform 'out in the wild'.\n",
    "<br>\n",
    "<br>\n",
    "However, the need to put aside test data also introduces additional complexity into preprocessing: For preprocessing steps whose parameters depend on the data they are applied to (e.g., scaling, encoding), we have to act like we would not have the test data available. This means we need to first preprocess the training data, and then the test data separately in the exact same way. Not doing this and having the test data still 'in' during data-dependent preprocessing steps would allow our model to learn from information that the model could, conceptually speaking, impossibly have access to. This is called 'peeking' or 'data leakage'. Data leakage risks leading our evaluation metrics to tell us that we have built a good model, but then after deployment find out that we have actually built a bad model - that is, when time and money have already been wasted.\n",
    "<br>\n",
    "<br>\n",
    "There is yet another issue we need to account for when putting aside test data: the class imbalance in our data that our EDA has confirmed. Usually, we would simply randomly split test and training data. However, since we need the training and test data to reflect the dataset's class distribution (\"churn\"/\"no churn\"), we perform a *stratified* split, that is, a split replicating the dataset's class distribution to both the created training and test subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=3992)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding missing value indicator features\n",
    "We now return once more to the issue of missing values. Since the number of missing values in the data is so high, we want to think some more about how we might exploit that. While a missing value represents an absence of information, the very fact per se that a value is missing might indicate something (e.g., a customer not booking a particular service) and possess predictive power. Thus, for each feature with missing values, we create an additional feature which indicates via the values 1 or 0 for each observation the presence or absence of a missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clm in X_train:\n",
    "    if X_train[clm].isna().sum() > 0:\n",
    "        X_train.insert(X_train.shape[1], f\"{clm}_NaNInd\", 0)\n",
    "        X_train[f\"{clm}_NaNInd\"] = np.where(pd.isnull(X_train[clm]), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, 185 binary variables have beend added to our training data, resulting in 397 variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing missing values\n",
    "To add at least some meaningful information to the data where values are missing, we impute missing values in numerical features with column-wise means, and missing values in categorical features with the category 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "features_num_train = list(X_train.select_dtypes(include=['float']).columns)\n",
    "imputer_nums = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_train[features_num_train] = imputer_nums.fit_transform(X_train[features_num_train])\n",
    "\n",
    "features_cat_train = list(X_train.select_dtypes(include=['object']).columns)\n",
    "imputer_cats = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='unknown')\n",
    "X_train[features_cat_train] = imputer_cats.fit_transform(X_train[features_cat_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our data now does not contain any missing values anymore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(X_train.isna().sum().sum()/(X_train.shape[0]*X.shape[1]), 3) #returns the percentage of missing values in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding categorical features\n",
    "We have also learned during EDA that there are quite some categorical features. Our ANN model (like many other model classes), however, can only handle numerical inputs. We thus *one-hot encode* (OHE) the categorical features. This means that for each unique value in each categorical feature, we create an additional feature which indicates via the values 1 or 0 for each observation the presence or absence of that unqiue value. We drop the original features and only keep the one-hot encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "enc = make_column_transformer((OneHotEncoder(max_categories=20, handle_unknown='ignore'), features_cat_train), remainder='passthrough')\n",
    "transformed = enc.fit_transform(X_train)\n",
    "enc_df = pd.DataFrame(transformed, columns=enc.get_feature_names())\n",
    "cols = enc_df.columns.tolist()\n",
    "cols = cols[168:] + cols[:168]\n",
    "X_train = enc_df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, OHE has added 130 dummy variables to our training data, resulting in 527 variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rescale features\n",
    "Finally, we rescale the features. We have seen in EDA that the features were measured using different scales. To rule out that this could affect our model, we remove from all features (except the indicator columns we added) the dataset mean and scale the feature to unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train[features_num_train] = scaler.fit_transform(X_train[features_num_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that scaling has happened properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming class labels\n",
    "We finally bring the class labels into a more suitable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.replace(-1, 0, inplace=True)\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set preprocessing\n",
    "We now apply the same preprocessing steps, in the same order, with the same parameters, to the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert NaN indicator columns\n",
    "for clm in X_test:\n",
    "    if X_test[clm].isna().sum() > 0:\n",
    "        X_test.insert(X_test.shape[1], f\"{clm}_NaNInd\", 0)\n",
    "        X_test[f\"{clm}_NaNInd\"] = np.where(pd.isnull(X_test[clm]), 1, 0)\n",
    "\n",
    "# missing value imputation: apply imputers that have been fit to training data\n",
    "X_test[features_num_train] = imputer_nums.transform(X_test[features_num_train])\n",
    "X_test[features_cat_train] = imputer_cats.transform(X_test[features_cat_train])\n",
    "X_test.isna().sum().sum()\n",
    "\n",
    "# Encode categorical features\n",
    "transformed = enc.transform(X_test)\n",
    "enc_df = pd.DataFrame(transformed, columns=enc.get_feature_names())\n",
    "cols = enc_df.columns.tolist()\n",
    "cols = cols[168:] + cols[:168]\n",
    "X_test = enc_df[cols]\n",
    "\n",
    "# apply scaler that has been fit to training data\n",
    "X_test[features_num_train] = scaler.transform(X_test[features_num_train])\n",
    "\n",
    "# transform target variable\n",
    "y_test.replace(-1, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We complete preprocessing by ensuring that the preprocessed training and test datasets have the same 527 features, in the same order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of identical columns:', sum(X_train.columns == X_test.columns)/len(X_train.columns)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### *2.5. Technical Solution: Feature Selection*\n",
    "\n",
    "While we have now preprocessed our data, the number of features is very high. In other words, our data is high-dimensional. This is undesirable for a variety of reasons, among the most important ones computational overhead and increased overfitting risk (see *curse of dimensionality*). Generally speaking, we want predictive models to use all the features with the most predictive power, but also as few features as possible. Thus, before we move on to building our ANN model, we train a random forest classifier model from which we can extract those features which have helped the forest model to best split observations into churning and non-churning customers.\n",
    "<br>\n",
    "<br>\n",
    "We first split our training data into a training and a cross-validation dataset for feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fs, X_cv_fs, y_train_fs, y_cv_fs = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2, random_state=3992)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that we do not select features based on an extremely bad random forest model, we optimize the model a little. Specifically, we grow multiple forest models with different numbers of trees, and with different maximum depths: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_classifier = RandomForestClassifier()\n",
    "parameters = {'max_depth':np.arange(3,10),'n_estimators':list(range(25,251,25))}\n",
    "random_grid = GridSearchCV(random_classifier, parameters, cv=3)\n",
    "random_grid.fit(X_cv_fs, y_cv_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing all forest models that we have grown, it turns out that the following parameters create the best random forest classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal Forest Hyperparams:\", random_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile a random forest classifier with the identified optimum parameters and train it on the training data for feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=25,\n",
    "    max_depth=3,\n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None,\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced',\n",
    "    random_state=3992,\n",
    "    verbose=0,\n",
    "    warm_start=False)\n",
    "\n",
    "rf_model.fit(X=X_train_fs, y=y_train_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract and visualize from this optimized forest model the features that most helped the model tell apart churning and non-churning customers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(rf_model.feature_importances_, index = X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "# feature importance plot\n",
    "std = np.std([rf_model.feature_importances_[:101] for rf_model in rf_model.estimators_], axis=0)\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "feature_importances[:101].plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean Decrease in Impurity (MDI)\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, only a very small number of the more than 500 features is actually helpful for predicting customer churn. Using only these features will save A LOT of computation time during ANN training, and also greatly reduce ANN complexity, without losing prediction quality. More simply put, feature selection has helped us make better predictions MUCH less costly. We thus throw all features but the most important ones from the training and test data that we use for ANN training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_imp_feats = feature_importances[:26].index.to_list()\n",
    "X_train = X_train[most_imp_feats]\n",
    "X_test = X_test[most_imp_feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### *2.4. Technical Solution: Model Selection (incl. Optimization)*\n",
    "\n",
    "Now that we have preprocessed our data and identified the most important features, we use the data for building and optimizing an artificial neural network classification model. Here, we will benefit from having spelled out earlier the technical problem we solve (*maximize the F1-score over the churn predictions of Orange's customers by implementing an artificial neural network with more than one hidden layer and an output layer containing a single neuron with an activation function*). This statement will guide us in the following steps: defining an optimization strategy, defining the ANN's architecture, optimization, and final model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model optimization strategy\n",
    "The reason for defining a model optimization strategy is simple: resources are limited, but the number of possible models is infinite (literally). This is due to the typically high number of optimizable parameters, and the infinite ranges of possible values for several of these parameters. This implies that simply jumping into optimization will likely make one end up endlessly tune everything and anything. Generally speaking, while model optimization strategies should be tailored to data science projects case-by-case, formulating them always requires a thorough understanding of the project's stakeholder expectations, the problem to be solved, the model classes used, and the resources available.\n",
    "<br>\n",
    "<br>\n",
    "For the prediction of churn among Orange's customers, we will focus our optimization strategy on four basic elements:\n",
    "- (1) the evaluation metric\n",
    "- (2) the optimization metric\n",
    "- (3) the optimized hyperparameters, and\n",
    "- (4) the optimization procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) _Evaluation metric_: F1-score.\n",
    "<br>\n",
    "We use this metric to identify which model among all trained models best solves our problem. We have explained our choice of the F1-score in 2.1. We also pull some further metrics from Keras' metrics library to enable a more comprehensive assessment of the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "scoring = {\n",
    "    \"F1\": sklearn.metrics.make_scorer(sklearn.metrics.f1_score),\n",
    "    'ROC_AUC': sklearn.metrics.make_scorer(sklearn.metrics.roc_auc_score),\n",
    "    \"Accuracy\": sklearn.metrics.make_scorer(sklearn.metrics.accuracy_score),\n",
    "    \"Recall\": sklearn.metrics.make_scorer(sklearn.metrics.recall_score),\n",
    "    \"Precision\": sklearn.metrics.make_scorer(sklearn.metrics.precision_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) _Optimization metric_: Binary cross-entropy loss.\n",
    "<br>\n",
    "We use this metric to allow the model to \"learn\", that is, adjust its coefficients ('weights') during training. Binary cross-entropy is a default optimization metric for binary classification problems, and there is no obvious reason to deviate. (Implementation: see step \"Define artificial neural network architecture\".)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) _Optimized Parameters_:\n",
    "<br>\n",
    "We will optimize the following parameters. These are also called \"hyper\"parameters to distinguish them from model-specific parameters such as coefficients. To be able to optimite these parameters, we will have to include them in the ANN's architecture (see step \"Define artificial neural network architecture\").\n",
    "\n",
    "| param name | explanation |\n",
    "| --- | --- |\n",
    "| batch_size | Controls how many observations are propagated through the network before coefficients are updated |\n",
    "| deep | Controls whether network is 'shallow' (1 hidden layer) or 'deep' (> 1 hidden layer) |\n",
    "| neurons | Controls number of nodes on network layers |\n",
    "| learning_rate | Controls how strongly coefficients are updated  |\n",
    "| dropout_rate | Controls fraction of layer inputs which will be randomly ignored in updating |\n",
    "| kernel_initializer | Controls the distribution from which the ANN's initial random coefficients are drawn |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) _Optimization procedure_: Staged grid search with class-weighted, k-fold cross-validation.\n",
    "<br>\n",
    "- \"Staged grid search\": Grid search means we define some values for each parameter we want to optimize, and exhaustively search through the resulting parameter \"grid\". For each grid node, a model using this grid node's parameter value combination will be trained on k training subsets (using the optimization metric) and evaluated on k validation subsets of the training data (using the evaluation metric). We run two grid search, optimizing the ANN architecture in the first and the hyperparameters in the second.\n",
    "- \"Class weighted\": To account for class imbalance, weights will be assigned to classes during training to penalize misclassification of the two different classes to different degrees (we unsuccessfully tried SMOTETomek resampling as an alternative).\n",
    "- \"cross-validation\": The purpose of validation sets is to have an indication how well a trained model would generalize to unseen data after deployment (similar to the train/test-split logic described above).\n",
    "- \"k-fold\": The k folds allow to compute the evaluation metric as a mean, and thus make model evaluation more robust against bias resulting from random validation set sampling. (Implementation: see step \"Optimization\".)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define artificial neural network architecture\n",
    "For the \"deep learning\" proof-of-concept requested by Orange, we choose a simple \"feedforward\" (as opposed to, e.g., recurrent or convolutional) neural net architecture. In essence, feedforward here means that the outputs of neurons on one network layer are sent (\"fed forward\") only to neurons of subsequent layers (instead of the same or previous layers). Further, to probe into the potential of \"deep\" versus \"shallow\" learning, we variabilize the number of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adam\n",
    "from keras.constraints import maxnorm\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import GaussianNoise\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import keras.metrics\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def create_model(learning_rate=0.001, dropout_rate=0.0, deep='n', neurons=X_train.shape[1], kernel_initializer='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    if deep == \"y\":\n",
    "        model.add(Dense(round(neurons**(1/1.2), 0), activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(round(neurons**(1/1.5), 0), activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.AUC(name='ROC_AUC'),\n",
    "            \"accuracy\",\n",
    "            keras.metrics.Precision(name=\"precision\"),\n",
    "            keras.metrics.Recall(name=\"recall\")\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=2) #this wrapper allows us to feed the Keras model into Sklearn's GridSearchCV class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "We now turn to the actual model optimization. Hereto, we first compile the optimization procedure we had defined as part of our optimization strategy. (The following code will run only grid search 2 and here use the optimal parameters that have been found during grid search 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "grid_name = \"arch_grid_\"\n",
    "\n",
    "if grid_name == \"arch_grid_\": #we first tune the ANN architecture...\n",
    "    param_grid = dict(\n",
    "        epochs=[5],\n",
    "        batch_size=[128],\n",
    "        deep=['n', 'y'],\n",
    "        neurons=[\n",
    "            round(X_train.shape[1]**(1/1.5), 0),\n",
    "            round(X_train.shape[1]/2, 0),\n",
    "            X_train.shape[1],\n",
    "            round(X_train.shape[1]*2, 0),\n",
    "            round(X_train.shape[1]**(1.5), 0),\n",
    "            round(X_train.shape[1]**(1.6), 0),\n",
    "            round(X_train.shape[1]**(1.7), 0),\n",
    "            round(X_train.shape[1]**(1.8), 0),\n",
    "            round(X_train.shape[1]**(1.9), 0),\n",
    "            round(X_train.shape[1]**(2), 0),\n",
    "            round(X_train.shape[1]**(2.1), 0),\n",
    "            round(X_train.shape[1]**(2.2), 0)]\n",
    "    )\n",
    "elif grid_name == \"hyparam_grid_\": #...and then the hyperparameters\n",
    "    param_grid = dict(\n",
    "        epochs=[5],\n",
    "        batch_size=[64, 128, 256, 512, 1024],\n",
    "        deep=['y'],\n",
    "        neurons=[round(X_train.shape[1]**(2.2), 0)],\n",
    "        learning_rate=[0.0001, 0.001, 0.01],\n",
    "        dropout_rate=[0.0, 0.45, 0.9],\n",
    "        kernel_initializer=['glorot_uniform', 'he_uniform', 'he_normal']\n",
    "    )\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=2) #this wrapper allows us to feed the Keras model into Sklearn's GridSearchCV class\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    verbose=3,\n",
    "    refit='F1',\n",
    "    n_jobs=2,\n",
    "    scoring=scoring,\n",
    "    return_train_score=True,\n",
    "    cv=StratifiedKFold(n_splits=3, shuffle=True),\n",
    ")\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train.values),\n",
    "    y=y_train.values.reshape(-1),\n",
    ")\n",
    "class_weights = dict(zip(np.unique(y_train.values), class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All now left to do in optimization is fit the grid search model to our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection\n",
    "As a basis for model selection, we store the grid search's results to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_result.cv_results_[\"params\"])\n",
    "results[\"means_val_F1\"] = grid_result.cv_results_[\"mean_test_F1\"]\n",
    "results['means_val_ROC_AUC'] = grid_result.cv_results_['mean_test_ROC_AUC']\n",
    "results[\"means_val_Accuracy\"] = grid_result.cv_results_[\"mean_test_Accuracy\"]\n",
    "results[\"means_val_Recall\"] = grid_result.cv_results_[\"mean_test_Recall\"]\n",
    "results[\"means_val_Precision\"] = grid_result.cv_results_[\"mean_test_Precision\"]\n",
    "results[\"means_train_F1\"] = grid_result.cv_results_[\"mean_train_F1\"]\n",
    "results['means_train_ROC_AUC'] = grid_result.cv_results_['mean_train_ROC_AUC']\n",
    "results[\"means_train_Accuracy\"] = grid_result.cv_results_[\"mean_train_Accuracy\"]\n",
    "results[\"means_train_Recall\"] = grid_result.cv_results_[\"mean_train_Recall\"]\n",
    "results[\"means_train_Precision\"] = grid_result.cv_results_[\"mean_train_Precision\"]\n",
    "\n",
    "from datetime import datetime\n",
    "import openpyxl\n",
    "path = \"C:\\\\Users\\\\marc.feldmann\\\\Documents\\\\data_science_local\\\\CustomerChurnPrediction\\\\results\\\\hyparam_opt\\\\\"\n",
    "filename = (path + \"FNN_clf_GSresults_\" + grid_name + datetime.now().strftime(\"%d_%m_%Y__%H_%M_%S\") + \".xlsx\")\n",
    "results.to_excel(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tabular form, the grid search results look like the following. Each row represents one of the models trained during grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.read_excel(\"C:\\\\Users\\\\marc.feldmann\\\\Documents\\\\data_science_local\\\\CustomerChurnPrediction\\\\results\\\\hyparam_opt\\\\FNN_clf_GSresults_param_bundle3_grid_09_08_2022__21_38_02.xlsx\")\n",
    "results.sort_values('means_val_Recall', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(add later here: visualization of some interesting grid search results, including deep learning y/n)\n",
    "nod to: model diagnosis: learning curves (train [how well model learns], val [how well model generalizes to new data]) > show that I can interpret and draw conclusions from that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final model training\n",
    "Wenow train the final model, using the identified optimal parameters (i.e., those that optimize F1 on the validation data). We do this time on the entire training data instead of subsets such as in CV to leverage all available data for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimum params:\n",
    "epochs=5\n",
    "batch_size=1024\n",
    "neurons=round(X_train.shape[1]**(1.9), 0)\n",
    "learning_rate=0.001\n",
    "dropout_rate=0.0\n",
    "kernel_initializer='glorot_uniform'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(neurons, activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(round(neurons**(1/1.2), 0), activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(round(neurons**(1/1.5), 0), activation='relu', input_dim=X_train.shape[1], kernel_initializer=kernel_initializer))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        keras.metrics.AUC(name='ROC_AUC'),\n",
    "        \"accuracy\",\n",
    "        keras.metrics.Precision(name=\"precision\"),\n",
    "        keras.metrics.Recall(name=\"recall\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=2,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.5. Technical Solution: Final Model Evaluation*\n",
    "Model Evaluation on 'Unseen' Data (simulate by priorly held out 'Test Data')\n",
    "- Do the results make sense?\n",
    "\n",
    "\n",
    "Result interpretation\n",
    "- in write-up: reflect on fact neural networks / deep learning seem to be overhyped\n",
    "- see e.g.: Peter Roßbach: \"Neural Networks vs. Random Forests – Does it always have to be Deep Learning?\n",
    "- - make that explicit point of the write-up! \"test\" that!\n",
    "- show here that I know how to work with learning curves\n",
    "\n",
    "when looking at results, come back to earlier point, explain via clas imbalance\n",
    "come back to earlier point: \n",
    "- make it one main technical point in the article that high accuracy can be misleading (when? why?) - have to also check other measures\n",
    "- - includein write-up my reflections for using precision/recall instead of AUC (argue by importance to detect minority class relative to importance of TPs and FPs) (expl in simple language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['churn'])\n",
    "print(\"ROC-AUC score is {}\".format(sklearn.metrics.roc_auc_score(y_test, y_pred)))\n",
    "\n",
    "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(y_test, y_pred)\n",
    "auc = sklearn.metrics.auc(recall, precision)\n",
    "from matplotlib import pyplot\n",
    "# no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "plt.plot(0.0778, 0.0823, marker=\"o\", markersize=20, markeredgecolor=\"red\", markerfacecolor=\"green\", label='Churn Informed Guessing')\n",
    "# plt.plot([0.4286, 0.4286], [0.375, 0.375], linestyle='--', label='Churn Random Guessing')\n",
    "plt.plot(recall, precision, marker='.', label='ANN Churn Predictor')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.6. Technical Solution: Future Optimization Potentials*\n",
    "(hier sammeln alles ich zeitlich nicht geschafft hab, aber für wichtig halte - um Kritik zu preempten)\n",
    "\n",
    "Schema: Potential - Umsetzungsaufwand - erwarteter Umsetzungseffekt auf Business Metric\n",
    "\n",
    "- version 2: optimization potentials (versus v1) to explore ceteris paribus:\n",
    "- perform infrequent category replacement for test and train sets separately \n",
    "- not explored/limitations: only individually optimized, due to constraints in processing power and time, optimization dependencies between variables neglected\n",
    "- only narrow ranges in gridsearch covered, so sound change that only found local optima per parameter\n",
    "- potential: NaN imputation with means on subsets of rows: one could search powerful clustering criteria first and than impute cluster means\n",
    "- also: was using smaller dataset, large dataset with many more variables may allow to increase a classifier's precision/recall\n",
    "- Make sure to also compare to others' results - I seem to be already working at the upper boundary of what's possible on this dataset with ANNs!\n",
    "- optimization potential: in practice, one would normally traing many different models and select/stack the best; show somehow that I'm aware of that\n",
    "- (optimization potential: add and compare AUC: simple logistic regression, random forest, 'flat' neural network, XGBoost)\n",
    "- optimization potential: put data into an AWS instance and run there\n",
    "- multicollinearity - check whether an issue - we want to have model as simple as possible! will decrease risk of overfitting that ANNs are especially prone to\n",
    "- outliers: Scaler Min Max or Robust made no big difference (see model_comparision Excel), suggesting it is not problematic that we have not removed outliers in data preprocessing; still might contain some potential to increase model performance\n",
    "- feature selection: have touched (selectKBest), but not exhausted feature selection \n",
    "- feature engineering: dimensionality reduction to reduce dimensionality, create new and more 'powerful' features; kurz auf curse of dimensionality eingehen und auf ANNs overviffting tendency; feature selection would have benefit to be explanable however, features anonymized anyways\n",
    "- optimization: \"Optimization of only thought about in terms of tuning hyperparamters; but also preprocessing includes many steps that can be done in different ways - meaning also has potential to optimize: read following in conjunction with \"diary\" to see what I have optimzied here and with which success: <br>\n",
    "- code cleaning: to increase code reusability and readability, repetitive parts could be wrapped into functions and moved into separate script (e.g., preprocessing steps applied both to training and test data)\n",
    "- clarify optimization approach: first optimized preprocessing (experiemented eg. with X instead of Y) - resulted in above described procedure; now: hyperparameter tuning:\n",
    "- optimize network architecture, e.g. LSTM, see \"neural network zoo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Business Recommendations**\n",
    "\"What do the generated insights/model urge us/allow us to do different next Monday, and which value (business metric!) will that generate?\"\n",
    "\n",
    "direkt aus auftrag (1.) ableiten. incl.:\n",
    "- (after implementing comparative models:) \"turns out, deep learning (might) not be best for this kind of problem; best practice computer vision, very large datasets; here: tree model such as XGboost or simple logistic regression better \n",
    "- based on a feature importance chart for final ANN, identify potential churn drivers: discuss how can be made visible and influenced by which staff groups XY (account managers? service managers?); measure, enable and encourage these staff groups to act on identified drivers  \n",
    "\n",
    "Good Example: https://www.kaggle.com/code/hamzaben/employee-churn-model-w-strategic-retention-plan/notebook"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e858973d4b0767378b2a391e82dfc297a0e494a6ac1816312417e02d17e8ebb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
